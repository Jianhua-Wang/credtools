{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"credtools","text":"<p>Multi-ancestry fine-mapping pipeline with interactive web visualization.</p> <ul> <li>Documentation: https://Jianhua-Wang.github.io/credtools</li> <li>GitHub: https://github.com/Jianhua-Wang/credtools</li> <li>PyPI: https://pypi.org/project/credtools/</li> <li>Free software: MIT</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-ancestry fine-mapping: Support for multiple fine-mapping tools (SuSiE, FINEMAP, etc.)</li> <li>Meta-analysis capabilities: Combine results across populations and cohorts</li> <li>Quality control: Built-in QC metrics and visualizations</li> <li>Interactive web interface: Explore results through a modern web dashboard</li> <li>Command-line interface: Easy-to-use CLI for all operations</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#basic-installation","title":"Basic Installation","text":"<pre><code>pip install credtools\n</code></pre>"},{"location":"#with-web-visualization","title":"With Web Visualization","text":"<p>To use the interactive web interface, install with web dependencies: <pre><code>pip install credtools[web]\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#command-line-usage","title":"Command Line Usage","text":"<pre><code># Run complete fine-mapping pipeline\ncredtools pipeline input_loci.txt output_dir/\n\n# Launch web visualization interface\ncredtools web /path/to/results --port 8080\n\n# View specific loci files\ncredtools web /path/to/data \\\n  --allmeta-loci data/allmeta_loci.txt \\\n  --popumeta-loci data/popumeta_loci.txt \\\n  --nometa-loci data/nometa_loci.txt\n</code></pre>"},{"location":"#web-interface","title":"Web Interface","text":"<p>The web interface provides: - Home page: Overview of all loci with interactive filtering - Locus pages: Detailed views with LocusZoom-style plots - Quality control: Comprehensive QC metrics and visualizations - Multi-tool comparison: Compare results across different fine-mapping methods</p> <p>Access the web interface at <code>http://localhost:8080</code> after running <code>credtools web</code>.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>For detailed documentation, see https://Jianhua-Wang.github.io/credtools</p>"},{"location":"#web-visualization","title":"Web Visualization","text":"<p>The web module (<code>credtools.web</code>) provides interactive visualization of fine-mapping results. See credtools/web/README.md for detailed usage instructions.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0028-2025-06-13","title":"[0.0.28] (2025-06-13)","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>add api docs</li> </ul>"},{"location":"changelog/#0027-2025-06-12","title":"[0.0.27] (2025-06-12)","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>add set_L_by_cojo to cli:pipeline</li> </ul>"},{"location":"changelog/#0026-2025-06-02","title":"[0.0.26] (2025-06-02)","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>add web visualization</li> </ul>"},{"location":"changelog/#0025-2025-06-02","title":"[0.0.25] (2025-06-02)","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>add tutorial</li> </ul>"},{"location":"changelog/#0023-2025-02-01","title":"[0.0.23] (2025-02-01)","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>fix finemap cred bug</li> </ul>"},{"location":"changelog/#0021-2025-01-20","title":"[0.0.21] (2025-01-20)","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>fix no install error for carma</li> </ul>"},{"location":"changelog/#0020-2025-01-20","title":"[0.0.20] (2025-01-20)","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>fix zero maf in finemap</li> </ul>"},{"location":"changelog/#0019-2025-01-20","title":"[0.0.19] (2025-01-20)","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>qc support for multiprocessing</li> </ul>"},{"location":"changelog/#0018-2025-01-19","title":"[0.0.18] (2025-01-19)","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>fix the bug of no credible set</li> </ul>"},{"location":"changelog/#0017-2025-01-18","title":"[0.0.17] (2025-01-18)","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>support for multiprocessing</li> <li>add progress bar</li> </ul>"},{"location":"changelog/#0016-2025-01-18","title":"[0.0.16] (2025-01-18)","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>support for sumstats.gz and ldmap.gz</li> </ul>"},{"location":"changelog/#0015-2024-12-17","title":"[0.0.15] (2024-12-17)","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>cli args</li> </ul>"},{"location":"changelog/#0014-2024-12-16","title":"[0.0.14] (2024-12-16)","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>cli</li> </ul>"},{"location":"changelog/#0013-2024-12-16","title":"[0.0.13] (2024-12-16)","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>pipeline</li> </ul>"},{"location":"changelog/#0012-2024-12-15","title":"[0.0.12] (2024-12-15)","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>ensemble fine-mapping</li> </ul>"},{"location":"changelog/#0011-2024-12-15","title":"[0.0.11] (2024-12-15)","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>multisusie</li> </ul>"},{"location":"changelog/#0010-2024-12-15","title":"[0.0.10] (2024-12-15)","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>susiex</li> <li>Rsparseld</li> <li>CARMA</li> </ul>"},{"location":"changelog/#009-2024-10-21","title":"[0.0.9] (2024-10-21)","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>abf</li> <li>susie</li> <li>finemap</li> </ul>"},{"location":"changelog/#008-2024-10-10","title":"[0.0.8] (2024-10-10)","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>load ld matrix and ld map</li> <li>munge sumstat</li> <li>example data</li> </ul>"},{"location":"changelog/#007-2024-10-09","title":"[0.0.7] (2024-10-09)","text":""},{"location":"changelog/#added_15","title":"Added","text":"<ul> <li>test for ldmatrix</li> </ul>"},{"location":"changelog/#006-2024-10-09","title":"[0.0.6] (2024-10-09)","text":""},{"location":"changelog/#added_16","title":"Added","text":"<ul> <li>functions for load LD</li> <li>test for ColName</li> </ul>"},{"location":"changelog/#005-2024-10-08","title":"[0.0.5] (2024-10-08)","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Jianhua-Wang/credtools/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>credtools could always use more documentation, whether as part of the official credtools docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Jianhua-Wang/credtools/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>credtools</code> for local development.</p> <ol> <li>Fork the <code>credtools</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>$ git clone git@github.com:your_name_here/credtools.git\n</code></pre> </li> <li> <p>Ensure poetry is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>$ poetry install -E test -E doc -E dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>$ poetry run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.6, 3.7, 3.8 and 3.9. Check    https://github.com/Jianhua-Wang/credtools/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<pre><code>$ poetry run pytest tests/test_credtools.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>$ poetry run bump2version patch # possible: major / minor / patch\n$ git push\n$ git push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"installation/","title":"Installation","text":"<p>CREDTOOLS requires Python 3.9 or higher. The base installation includes all dependencies needed for fine-mapping analysis.</p>"},{"location":"installation/#basic-installation","title":"Basic Installation","text":"<p>To install the base CREDTOOLS package, run this command in your terminal:</p> <pre><code>$ pip install credtools\n</code></pre> <p>This is the preferred method to install CREDTOOLS, as it will always install the most recent stable release.</p>"},{"location":"installation/#web-visualization","title":"Web Visualization","text":"<p>To use the interactive web interface, install with web dependencies:</p> <pre><code>$ pip install credtools[web]\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development, you may want to install CREDTOOLS in \"editable\" mode:</p> <pre><code>$ git clone https://github.com/Jianhua-Wang/credtools.git\n$ cd credtools\n$ pip install -e .\n</code></pre> <p>Check that CREDTOOLS is installed correctly:</p> <pre><code>$ credtools --help\n</code></pre> <p>For web visualization:</p> <pre><code>$ credtools web --help\n</code></pre>"},{"location":"installation/#conda-installation","title":"Conda Installation","text":"<p>You can also install CREDTOOLS using conda:</p> <pre><code>$ conda install -c conda-forge credtools\n</code></pre>"},{"location":"installation/#source","title":"Source","text":"<p>The source for CREDTOOLS can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>$ git clone git://github.com/Jianhua-Wang/credtools\n</code></pre> <p>Or download the tarball:</p> <pre><code>$ curl -OJL https://github.com/Jianhua-Wang/credtools/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>$ cd credtools\n$ pip install -e .\n</code></pre>"},{"location":"installation/#web-dependencies","title":"Web Dependencies","text":"<p>The web visualization requires additional dependencies. You can install them with:</p> <pre><code>$ pip install credtools[web]\n</code></pre> <p>If you don't have root access, you can install for your user only:</p> <pre><code>$ pip install --user credtools[web]\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, please check:</p> <ol> <li>Python version (3.9+ required)</li> <li>pip is up to date</li> <li>You have write permissions for the installation directory</li> </ol> <p>For web visualization issues, ensure all web dependencies are installed:</p> <pre><code>$ pip install credtools[web]\n</code></pre>"},{"location":"installation/#links","title":"Links","text":""},{"location":"tutorial/","title":"Tutorial","text":"<p>Welcome to the comprehensive CREDTOOLS (Credible Set Tools) tutorial! This guide will walk you through using CREDTOOLS for genetic fine-mapping analysis across different ancestries and cohorts.</p>"},{"location":"tutorial/#tutorial-structure","title":"Tutorial Structure","text":"<ol> <li>Getting Started - Learn what CREDTOOLS can do and its overall framework</li> <li>Quick Start - Jump right in with the <code>credtools pipeline</code> command</li> <li>Single Input - Fine-map a single study</li> <li>Multi Input - Combine multiple studies</li> <li>Web Visualization - Interactive results exploration</li> <li>Advanced Topics - Custom workflows and optimization</li> </ol>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>CREDTOOLS installed (see Installation)</li> <li>Basic understanding of fine-mapping concepts</li> <li>Python 3.9 or higher</li> </ul>"},{"location":"tutorial/#example-data","title":"Example Data","text":"<p>Throughout this tutorial, we'll use example datasets included with CREDTOOLS. You can find them in the <code>exampledata/</code> directory after installation.</p>"},{"location":"tutorial/#web-visualization","title":"Web Visualization","text":"<p>For interactive visualization, install with web dependencies:</p> <pre><code>pip install credtools[web]\n</code></pre>"},{"location":"tutorial/#navigation","title":"Navigation","text":"<p>Use the navigation menu on the left to jump between sections, or follow the tutorial sequentially using the \"Next\" links at the bottom of each page.</p> <p>Let's get started! \ud83d\udc49 Getting Started </p>"},{"location":"API/cojo/","title":"cojo","text":"<p>Wrapper for COJO.</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection","title":"<code>conditional_selection(locus, p_cutoff=5e-08, collinear_cutoff=0.9, window_size=10000000, maf_cutoff=0.01, diff_freq_cutoff=0.2)</code>","text":"<p>Perform conditional selection on the locus using COJO method.</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--parameters","title":"Parameters","text":"<p>locus : Locus     The locus to perform conditional selection on. Must contain summary statistics     and LD matrix data. p_cutoff : float, optional     The p-value cutoff for the conditional selection, by default 5e-8.     If no SNPs pass this threshold, it will be relaxed to 1e-5. collinear_cutoff : float, optional     The collinearity cutoff for the conditional selection, by default 0.9.     SNPs with LD correlation above this threshold are considered collinear. window_size : int, optional     The window size in base pairs for the conditional selection, by default 10000000.     SNPs within this window are considered for conditional analysis. maf_cutoff : float, optional     The minor allele frequency cutoff for the conditional selection, by default 0.01.     SNPs with MAF below this threshold are excluded. diff_freq_cutoff : float, optional     The difference in frequency cutoff between summary statistics and reference panel,     by default 0.2. SNPs with frequency differences above this threshold are excluded.</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--returns","title":"Returns","text":"<p>pd.DataFrame     The conditional selection results containing independently associated variants     with columns including SNP identifiers, effect sizes, and conditional p-values.</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--warnings","title":"Warnings","text":"<p>If no SNPs pass the initial p-value cutoff, the threshold is automatically relaxed to 1e-5 and a warning is logged.</p> <p>If AF2 (reference allele frequency) is not available in the LD matrix, a warning is logged and frequency checking is disabled.</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--notes","title":"Notes","text":"<p>COJO (Conditional and Joint analysis) performs stepwise conditional analysis to identify independently associated variants at a locus. The method:</p> <ol> <li>Identifies the most significant SNP</li> <li>Performs conditional analysis on remaining SNPs</li> <li>Iteratively adds independently associated SNPs</li> <li>Continues until no more SNPs meet significance criteria</li> </ol> <p>The algorithm accounts for linkage disequilibrium patterns and helps distinguish truly independent signals from those in LD with lead variants.</p> <p>Reference: Yang, J. et al. Conditional and joint multiple-SNP analysis of GWAS summary statistics identifies additional variants influencing complex traits. Nat Genet 44, 369-375 (2012).</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--examples","title":"Examples","text":"Source code in <code>credtools/cojo.py</code> <pre><code>def conditional_selection(\n    locus: Locus,\n    p_cutoff: float = 5e-8,\n    collinear_cutoff: float = 0.9,\n    window_size: int = 10000000,\n    maf_cutoff: float = 0.01,\n    diff_freq_cutoff: float = 0.2,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform conditional selection on the locus using COJO method.\n\n    Parameters\n    ----------\n    locus : Locus\n        The locus to perform conditional selection on. Must contain summary statistics\n        and LD matrix data.\n    p_cutoff : float, optional\n        The p-value cutoff for the conditional selection, by default 5e-8.\n        If no SNPs pass this threshold, it will be relaxed to 1e-5.\n    collinear_cutoff : float, optional\n        The collinearity cutoff for the conditional selection, by default 0.9.\n        SNPs with LD correlation above this threshold are considered collinear.\n    window_size : int, optional\n        The window size in base pairs for the conditional selection, by default 10000000.\n        SNPs within this window are considered for conditional analysis.\n    maf_cutoff : float, optional\n        The minor allele frequency cutoff for the conditional selection, by default 0.01.\n        SNPs with MAF below this threshold are excluded.\n    diff_freq_cutoff : float, optional\n        The difference in frequency cutoff between summary statistics and reference panel,\n        by default 0.2. SNPs with frequency differences above this threshold are excluded.\n\n    Returns\n    -------\n    pd.DataFrame\n        The conditional selection results containing independently associated variants\n        with columns including SNP identifiers, effect sizes, and conditional p-values.\n\n    Warnings\n    --------\n    If no SNPs pass the initial p-value cutoff, the threshold is automatically\n    relaxed to 1e-5 and a warning is logged.\n\n    If AF2 (reference allele frequency) is not available in the LD matrix,\n    a warning is logged and frequency checking is disabled.\n\n    Notes\n    -----\n    COJO (Conditional and Joint analysis) performs stepwise conditional analysis\n    to identify independently associated variants at a locus. The method:\n\n    1. Identifies the most significant SNP\n    2. Performs conditional analysis on remaining SNPs\n    3. Iteratively adds independently associated SNPs\n    4. Continues until no more SNPs meet significance criteria\n\n    The algorithm accounts for linkage disequilibrium patterns and helps\n    distinguish truly independent signals from those in LD with lead variants.\n\n    Reference: Yang, J. et al. Conditional and joint multiple-SNP analysis of GWAS\n    summary statistics identifies additional variants influencing complex traits.\n    Nat Genet 44, 369-375 (2012).\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Basic conditional selection\n    &gt;&gt;&gt; results = conditional_selection(locus)\n    &gt;&gt;&gt; print(f\"Found {len(results)} independent signals\")\n    Found 3 independent signals\n\n    &gt;&gt;&gt; # With custom thresholds\n    &gt;&gt;&gt; results = conditional_selection(\n    ...     locus,\n    ...     p_cutoff=1e-6,\n    ...     maf_cutoff=0.05\n    ... )\n    &gt;&gt;&gt; print(results[['SNP', 'b', 'se', 'p']])\n        SNP           b        se         p\n    0   rs123456   0.15   0.025   1.2e-08\n    1   rs789012  -0.08   0.020   4.5e-07\n    \"\"\"\n    sumstats = locus.sumstats.copy()\n    sumstats = sumstats[\n        [\n            ColName.SNPID,\n            ColName.EA,\n            ColName.NEA,\n            ColName.BETA,\n            ColName.SE,\n            ColName.P,\n            ColName.EAF,\n        ]\n    ]\n    sumstats.columns = [\"SNP\", \"A1\", \"A2\", \"b\", \"se\", \"p\", \"freq\"]\n    sumstats[\"N\"] = locus.sample_size\n    if p_cutoff &lt; 1e-5 and len(sumstats[sumstats[\"p\"] &lt; p_cutoff]) == 0:\n        logger.warning(\"No SNPs passed the p-value cutoff, using p_cutoff=1e-5\")\n        p_cutoff = 1e-5\n\n    ld_matrix = locus.ld.r.copy()\n    ld_freq: Optional[pd.DataFrame] = locus.ld.map.copy()\n    if ld_freq is not None and \"AF2\" not in ld_freq.columns:\n        logger.warning(\"AF2 is not in the LD matrix.\")\n        ld_freq = None\n    elif ld_freq is not None:\n        ld_freq = ld_freq[[\"SNPID\", \"AF2\"]]\n        ld_freq.columns = [\"SNP\", \"freq\"]\n        ld_freq[\"freq\"] = 1 - ld_freq[\"freq\"]\n    c = COJO(\n        p_cutoff=p_cutoff,\n        collinear_cutoff=collinear_cutoff,\n        window_size=window_size,\n        maf_cutoff=maf_cutoff,\n        diff_freq_cutoff=diff_freq_cutoff,\n    )\n    c.load_sumstats(sumstats=sumstats, ld_matrix=ld_matrix, ld_freq=ld_freq)  # type: ignore\n    cojo_result = c.conditional_selection()\n    return cojo_result\n</code></pre>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--basic-conditional-selection","title":"Basic conditional selection","text":"<p>results = conditional_selection(locus) print(f\"Found {len(results)} independent signals\") Found 3 independent signals</p>"},{"location":"API/cojo/#credtools.cojo.conditional_selection--with-custom-thresholds","title":"With custom thresholds","text":"<p>results = conditional_selection( ...     locus, ...     p_cutoff=1e-6, ...     maf_cutoff=0.05 ... ) print(results[['SNP', 'b', 'se', 'p']])     SNP           b        se         p 0   rs123456   0.15   0.025   1.2e-08 1   rs789012  -0.08   0.020   4.5e-07</p>"},{"location":"API/credibleset/","title":"credibleset","text":"<p>Credible Set functions.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet","title":"<code>CredibleSet</code>","text":"<p>Class representing credible sets from one fine-mapping tool.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet--parameters","title":"Parameters","text":"<p>tool : str     The name of the fine-mapping tool. parameters : Dict[str, Any]     Additional parameters used by the fine-mapping tool. coverage : float     The coverage of the credible sets. n_cs : int     The number of credible sets. cs_sizes : List[int]     Sizes of each credible set. lead_snps : List[str]     List of lead SNPs. snps : List[List[str]]     List of SNPs for each credible set. pips : pd.Series     Posterior inclusion probabilities.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet--attributes","title":"Attributes","text":"<p>tool : str     The name of the fine-mapping tool. n_cs : int     The number of credible sets. coverage : float     The coverage of the credible sets. lead_snps : List[str]     List of lead SNPs. snps : List[List[str]]     List of SNPs for each credible set. cs_sizes : List[int]     Sizes of each credible set. pips : pd.Series     Posterior inclusion probabilities. parameters : Dict[str, Any]     Additional parameters used by the fine-mapping tool.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>class CredibleSet:\n    \"\"\"\n    Class representing credible sets from one fine-mapping tool.\n\n    Parameters\n    ----------\n    tool : str\n        The name of the fine-mapping tool.\n    parameters : Dict[str, Any]\n        Additional parameters used by the fine-mapping tool.\n    coverage : float\n        The coverage of the credible sets.\n    n_cs : int\n        The number of credible sets.\n    cs_sizes : List[int]\n        Sizes of each credible set.\n    lead_snps : List[str]\n        List of lead SNPs.\n    snps : List[List[str]]\n        List of SNPs for each credible set.\n    pips : pd.Series\n        Posterior inclusion probabilities.\n\n    Attributes\n    ----------\n    tool : str\n        The name of the fine-mapping tool.\n    n_cs : int\n        The number of credible sets.\n    coverage : float\n        The coverage of the credible sets.\n    lead_snps : List[str]\n        List of lead SNPs.\n    snps : List[List[str]]\n        List of SNPs for each credible set.\n    cs_sizes : List[int]\n        Sizes of each credible set.\n    pips : pd.Series\n        Posterior inclusion probabilities.\n    parameters : Dict[str, Any]\n        Additional parameters used by the fine-mapping tool.\n    \"\"\"\n\n    def __init__(\n        self,\n        tool: str,\n        parameters: Dict[str, Any],\n        coverage: float,\n        n_cs: int,\n        cs_sizes: List[int],\n        lead_snps: List[str],\n        snps: List[List[str]],\n        pips: pd.Series,\n    ) -&gt; None:\n        \"\"\"\n        Initialize CredibleSet object.\n\n        Parameters\n        ----------\n        tool : str\n            The name of the fine-mapping tool.\n        parameters : Dict[str, Any]\n            Additional parameters used by the fine-mapping tool.\n        coverage : float\n            The coverage of the credible sets.\n        n_cs : int\n            The number of credible sets.\n        cs_sizes : List[int]\n            Sizes of each credible set.\n        lead_snps : List[str]\n            List of lead SNPs.\n        snps : List[List[str]]\n            List of SNPs for each credible set.\n        pips : pd.Series\n            Posterior inclusion probabilities.\n        \"\"\"\n        self._tool = tool\n        self._parameters = parameters\n        self._coverage = coverage\n        self._n_cs = n_cs\n        self._cs_sizes = cs_sizes\n        self._lead_snps = lead_snps\n        self._snps = snps\n        self._pips = pips\n        # TODO: add results data like, if it is converged, etc.\n\n    @property\n    def tool(self) -&gt; str:\n        \"\"\"Get the tool name.\"\"\"\n        return self._tool\n\n    @property\n    def parameters(self) -&gt; Dict[str, Any]:\n        \"\"\"Get the parameters.\"\"\"\n        return self._parameters\n\n    @property\n    def coverage(self) -&gt; float:\n        \"\"\"Get the coverage.\"\"\"\n        # TODO: add actual coverage, as a list of coverage for each credible set\n        return self._coverage\n\n    @property\n    def n_cs(self) -&gt; int:\n        \"\"\"Get the number of credible sets.\"\"\"\n        return self._n_cs\n\n    @property\n    def cs_sizes(self) -&gt; List[int]:\n        \"\"\"Get the sizes of each credible set.\"\"\"\n        return self._cs_sizes\n\n    @property\n    def lead_snps(self) -&gt; List[str]:\n        \"\"\"Get the lead SNPs.\"\"\"\n        return self._lead_snps\n\n    @property\n    def snps(self) -&gt; List[List[str]]:\n        \"\"\"Get the SNPs.\"\"\"\n        return self._snps\n\n    @property\n    def pips(self) -&gt; pd.Series:\n        \"\"\"Get the PIPs.\"\"\"\n        return self._pips\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the CredibleSet object.\n\n        Returns\n        -------\n        str\n            String representation of the CredibleSet object.\n        \"\"\"\n        return (\n            f\"CredibleSet(\\n  tool={self.tool}, coverage={self.coverage}, n_cs={self.n_cs}, cs_sizes={self.cs_sizes}, lead_snps={self.lead_snps},\"\n            + f\"\\n  Parameters: {json.dumps(self.parameters)}\\n)\"\n        )\n\n    def copy(self) -&gt; \"CredibleSet\":\n        \"\"\"\n        Copy the CredibleSet object.\n\n        Returns\n        -------\n        CredibleSet\n            A copy of the CredibleSet object.\n        \"\"\"\n        return CredibleSet(\n            tool=self.tool,\n            parameters=self.parameters,\n            coverage=self.coverage,\n            n_cs=self.n_cs,\n            cs_sizes=self.cs_sizes,\n            lead_snps=self.lead_snps,\n            snps=self.snps,\n            pips=self.pips,\n        )\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert to dictionary for TOML storage (excluding pips).\n\n        Returns\n        -------\n        Dict[str, Any]\n            A dictionary representation of the CredibleSet excluding pips.\n        \"\"\"\n        return {\n            \"tool\": self.tool,\n            \"n_cs\": self.n_cs,\n            \"coverage\": self.coverage,\n            \"lead_snps\": self.lead_snps,\n            \"snps\": self.snps,\n            \"cs_sizes\": self.cs_sizes,\n            \"parameters\": self.parameters,\n        }\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any], pips: pd.Series) -&gt; \"CredibleSet\":\n        \"\"\"\n        Create CredibleSet from dictionary and pips.\n\n        Parameters\n        ----------\n        data : Dict[str, Any]\n            A dictionary containing the data to initialize the CredibleSet.\n        pips : pd.Series\n            Posterior inclusion probabilities.\n\n        Returns\n        -------\n        CredibleSet\n            An instance of CredibleSet initialized with the provided data and pips.\n        \"\"\"\n        return cls(\n            tool=data[\"tool\"],\n            parameters=data[\"parameters\"],\n            coverage=data[\"coverage\"],\n            n_cs=data[\"n_cs\"],\n            cs_sizes=data[\"cs_sizes\"],\n            lead_snps=data[\"lead_snps\"],\n            snps=data[\"snps\"],\n            pips=pips,\n        )\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.coverage","title":"<code>coverage</code>  <code>property</code>","text":"<p>Get the coverage.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.cs_sizes","title":"<code>cs_sizes</code>  <code>property</code>","text":"<p>Get the sizes of each credible set.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.lead_snps","title":"<code>lead_snps</code>  <code>property</code>","text":"<p>Get the lead SNPs.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.n_cs","title":"<code>n_cs</code>  <code>property</code>","text":"<p>Get the number of credible sets.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.parameters","title":"<code>parameters</code>  <code>property</code>","text":"<p>Get the parameters.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.pips","title":"<code>pips</code>  <code>property</code>","text":"<p>Get the PIPs.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.snps","title":"<code>snps</code>  <code>property</code>","text":"<p>Get the SNPs.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.tool","title":"<code>tool</code>  <code>property</code>","text":"<p>Get the tool name.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.__init__","title":"<code>__init__(tool, parameters, coverage, n_cs, cs_sizes, lead_snps, snps, pips)</code>","text":"<p>Initialize CredibleSet object.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.__init__--parameters","title":"Parameters","text":"<p>tool : str     The name of the fine-mapping tool. parameters : Dict[str, Any]     Additional parameters used by the fine-mapping tool. coverage : float     The coverage of the credible sets. n_cs : int     The number of credible sets. cs_sizes : List[int]     Sizes of each credible set. lead_snps : List[str]     List of lead SNPs. snps : List[List[str]]     List of SNPs for each credible set. pips : pd.Series     Posterior inclusion probabilities.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def __init__(\n    self,\n    tool: str,\n    parameters: Dict[str, Any],\n    coverage: float,\n    n_cs: int,\n    cs_sizes: List[int],\n    lead_snps: List[str],\n    snps: List[List[str]],\n    pips: pd.Series,\n) -&gt; None:\n    \"\"\"\n    Initialize CredibleSet object.\n\n    Parameters\n    ----------\n    tool : str\n        The name of the fine-mapping tool.\n    parameters : Dict[str, Any]\n        Additional parameters used by the fine-mapping tool.\n    coverage : float\n        The coverage of the credible sets.\n    n_cs : int\n        The number of credible sets.\n    cs_sizes : List[int]\n        Sizes of each credible set.\n    lead_snps : List[str]\n        List of lead SNPs.\n    snps : List[List[str]]\n        List of SNPs for each credible set.\n    pips : pd.Series\n        Posterior inclusion probabilities.\n    \"\"\"\n    self._tool = tool\n    self._parameters = parameters\n    self._coverage = coverage\n    self._n_cs = n_cs\n    self._cs_sizes = cs_sizes\n    self._lead_snps = lead_snps\n    self._snps = snps\n    self._pips = pips\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the CredibleSet object.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.__repr__--returns","title":"Returns","text":"<p>str     String representation of the CredibleSet object.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the CredibleSet object.\n\n    Returns\n    -------\n    str\n        String representation of the CredibleSet object.\n    \"\"\"\n    return (\n        f\"CredibleSet(\\n  tool={self.tool}, coverage={self.coverage}, n_cs={self.n_cs}, cs_sizes={self.cs_sizes}, lead_snps={self.lead_snps},\"\n        + f\"\\n  Parameters: {json.dumps(self.parameters)}\\n)\"\n    )\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.copy","title":"<code>copy()</code>","text":"<p>Copy the CredibleSet object.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.copy--returns","title":"Returns","text":"<p>CredibleSet     A copy of the CredibleSet object.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def copy(self) -&gt; \"CredibleSet\":\n    \"\"\"\n    Copy the CredibleSet object.\n\n    Returns\n    -------\n    CredibleSet\n        A copy of the CredibleSet object.\n    \"\"\"\n    return CredibleSet(\n        tool=self.tool,\n        parameters=self.parameters,\n        coverage=self.coverage,\n        n_cs=self.n_cs,\n        cs_sizes=self.cs_sizes,\n        lead_snps=self.lead_snps,\n        snps=self.snps,\n        pips=self.pips,\n    )\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.from_dict","title":"<code>from_dict(data, pips)</code>  <code>classmethod</code>","text":"<p>Create CredibleSet from dictionary and pips.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.from_dict--parameters","title":"Parameters","text":"<p>data : Dict[str, Any]     A dictionary containing the data to initialize the CredibleSet. pips : pd.Series     Posterior inclusion probabilities.</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.from_dict--returns","title":"Returns","text":"<p>CredibleSet     An instance of CredibleSet initialized with the provided data and pips.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], pips: pd.Series) -&gt; \"CredibleSet\":\n    \"\"\"\n    Create CredibleSet from dictionary and pips.\n\n    Parameters\n    ----------\n    data : Dict[str, Any]\n        A dictionary containing the data to initialize the CredibleSet.\n    pips : pd.Series\n        Posterior inclusion probabilities.\n\n    Returns\n    -------\n    CredibleSet\n        An instance of CredibleSet initialized with the provided data and pips.\n    \"\"\"\n    return cls(\n        tool=data[\"tool\"],\n        parameters=data[\"parameters\"],\n        coverage=data[\"coverage\"],\n        n_cs=data[\"n_cs\"],\n        cs_sizes=data[\"cs_sizes\"],\n        lead_snps=data[\"lead_snps\"],\n        snps=data[\"snps\"],\n        pips=pips,\n    )\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for TOML storage (excluding pips).</p>"},{"location":"API/credibleset/#credtools.credibleset.CredibleSet.to_dict--returns","title":"Returns","text":"<p>Dict[str, Any]     A dictionary representation of the CredibleSet excluding pips.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary for TOML storage (excluding pips).\n\n    Returns\n    -------\n    Dict[str, Any]\n        A dictionary representation of the CredibleSet excluding pips.\n    \"\"\"\n    return {\n        \"tool\": self.tool,\n        \"n_cs\": self.n_cs,\n        \"coverage\": self.coverage,\n        \"lead_snps\": self.lead_snps,\n        \"snps\": self.snps,\n        \"cs_sizes\": self.cs_sizes,\n        \"parameters\": self.parameters,\n    }\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.cluster_cs","title":"<code>cluster_cs(dict_sets, threshold=0.9)</code>","text":"<p>Cluster dictionaries from different sets based on continuous Jaccard similarity.</p>"},{"location":"API/credibleset/#credtools.credibleset.cluster_cs--parameters","title":"Parameters","text":"<p>dict_sets : List[List[Dict[str, float]]]     List of m sets, where each set contains dictionaries with PIP values. threshold : float, optional     Clustering threshold, by default 0.9.</p>"},{"location":"API/credibleset/#credtools.credibleset.cluster_cs--returns","title":"Returns","text":"<p>List[List[str]]     List of merged clusters, where each cluster contains     a list of unique SNP IDs from the dictionaries in that cluster.</p>"},{"location":"API/credibleset/#credtools.credibleset.cluster_cs--raises","title":"Raises","text":"<p>ValueError     If less than two sets of dictionaries are provided or if any set is empty.</p>"},{"location":"API/credibleset/#credtools.credibleset.cluster_cs--examples","title":"Examples","text":"<p>sets = [ ...     [{'a': 0.8, 'b': 0.5}], ...     [{'b': 0.6, 'c': 0.3}] ... ] clusters = cluster_cs(sets)</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def cluster_cs(\n    dict_sets: List[List[Dict[str, float]]], threshold: float = 0.9\n) -&gt; List[List[str]]:\n    \"\"\"\n    Cluster dictionaries from different sets based on continuous Jaccard similarity.\n\n    Parameters\n    ----------\n    dict_sets : List[List[Dict[str, float]]]\n        List of m sets, where each set contains dictionaries with PIP values.\n    threshold : float, optional\n        Clustering threshold, by default 0.9.\n\n    Returns\n    -------\n    List[List[str]]\n        List of merged clusters, where each cluster contains\n        a list of unique SNP IDs from the dictionaries in that cluster.\n\n    Raises\n    ------\n    ValueError\n        If less than two sets of dictionaries are provided or if any set is empty.\n\n    Examples\n    --------\n    &gt;&gt;&gt; sets = [\n    ...     [{'a': 0.8, 'b': 0.5}],\n    ...     [{'b': 0.6, 'c': 0.3}]\n    ... ]\n    &gt;&gt;&gt; clusters = cluster_cs(sets)\n    \"\"\"\n    if len(dict_sets) &lt; 2:\n        raise ValueError(\"At least two sets of dictionaries are required\")\n\n    # Validate input\n    for dict_set in dict_sets:\n        if not dict_set:\n            raise ValueError(\"Empty dictionary sets are not allowed\")\n\n    # Create similarity matrix\n    similarity_matrix, all_dicts = create_similarity_matrix(dict_sets)\n\n    # Convert similarity to distance (1 - similarity)\n    distance_matrix = 1 - similarity_matrix\n\n    # Perform hierarchical clustering\n    condensed_dist = distance_matrix[np.triu_indices(len(distance_matrix), k=1)]\n\n    if len(condensed_dist) == 0:\n        logger.warning(\"No valid distances found for clustering\")\n        return [list(set(all_dicts[0].keys()))]\n\n    linkage_matrix = linkage(condensed_dist, method=\"average\")\n\n    # Cut the dendrogram at the specified threshold\n    clusters = fcluster(linkage_matrix, threshold, criterion=\"distance\")\n\n    # Group dictionaries by cluster and merge them\n    cluster_groups: Dict[int, List[str]] = {}\n    for idx, cluster_id in enumerate(clusters):\n        if cluster_id not in cluster_groups:\n            cluster_groups[cluster_id] = []\n\n            # Merge dictionaries within cluster by merging keys (no PIP values) and removing duplicates\n            current_dict = all_dicts[idx]\n            cluster_groups[cluster_id].extend(current_dict.keys())\n\n    return [\n        list(set(cluster_groups[cluster_id])) for cluster_id in sorted(cluster_groups)\n    ]\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.combine_creds","title":"<code>combine_creds(creds, combine_cred='union', combine_pip='max', jaccard_threshold=0.1)</code>","text":"<p>Combine credible sets from multiple tools.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_creds--parameters","title":"Parameters","text":"<p>creds : List[CredibleSet]     List of credible sets from multiple tools. combine_cred : str, optional     Method to combine credible sets, by default \"union\".     Options: \"union\", \"intersection\", \"cluster\".</p> <pre><code>- \"union\": Union of all credible sets to form a merged credible set.\n- \"intersection\": First merge the credible sets from the same tool,\n    then take the intersection of all merged credible sets.\n    No credible set will be returned if no common SNPs found.\n- \"cluster\": Merge credible sets with Jaccard index &gt; jaccard_threshold.\n</code></pre> <p>combine_pip : str, optional     Method to combine PIPs, by default \"max\".     Options: \"max\", \"min\", \"mean\", \"meta\".</p> <pre><code>- \"meta\": PIP_meta = 1 - prod(1 - PIP_i), where i is the index of tools,\n    PIP_i = 0 when the SNP is not in the credible set of the tool.\n- \"max\": Maximum PIP value for each SNP across all tools.\n- \"min\": Minimum PIP value for each SNP across all tools.\n- \"mean\": Mean PIP value for each SNP across all tools.\n</code></pre> <p>jaccard_threshold : float, optional     Jaccard index threshold for the \"cluster\" method, by default 0.1.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_creds--returns","title":"Returns","text":"<p>CredibleSet     Combined credible set.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_creds--raises","title":"Raises","text":"<p>ValueError     If the method is not supported.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_creds--notes","title":"Notes","text":"<p>'union' and 'intersection' methods will merge all credible sets into one.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def combine_creds(\n    creds: List[CredibleSet],\n    combine_cred: str = \"union\",\n    combine_pip: str = \"max\",\n    jaccard_threshold: float = 0.1,\n) -&gt; CredibleSet:\n    \"\"\"\n    Combine credible sets from multiple tools.\n\n    Parameters\n    ----------\n    creds : List[CredibleSet]\n        List of credible sets from multiple tools.\n    combine_cred : str, optional\n        Method to combine credible sets, by default \"union\".\n        Options: \"union\", \"intersection\", \"cluster\".\n\n        - \"union\": Union of all credible sets to form a merged credible set.\n        - \"intersection\": First merge the credible sets from the same tool,\n            then take the intersection of all merged credible sets.\n            No credible set will be returned if no common SNPs found.\n        - \"cluster\": Merge credible sets with Jaccard index &gt; jaccard_threshold.\n    combine_pip : str, optional\n        Method to combine PIPs, by default \"max\".\n        Options: \"max\", \"min\", \"mean\", \"meta\".\n\n        - \"meta\": PIP_meta = 1 - prod(1 - PIP_i), where i is the index of tools,\n            PIP_i = 0 when the SNP is not in the credible set of the tool.\n        - \"max\": Maximum PIP value for each SNP across all tools.\n        - \"min\": Minimum PIP value for each SNP across all tools.\n        - \"mean\": Mean PIP value for each SNP across all tools.\n    jaccard_threshold : float, optional\n        Jaccard index threshold for the \"cluster\" method, by default 0.1.\n\n    Returns\n    -------\n    CredibleSet\n        Combined credible set.\n\n    Raises\n    ------\n    ValueError\n        If the method is not supported.\n\n    Notes\n    -----\n    'union' and 'intersection' methods will merge all credible sets into one.\n    \"\"\"\n    paras = creds[0].parameters\n    tool = creds[0].tool\n    # filter out the creds with no credible set\n    creds = [cred for cred in creds if cred.n_cs &gt; 0]\n    if len(creds) == 0:\n        logger.warning(\"No credible sets found in the input list.\")\n        return CredibleSet(\n            tool=tool,\n            n_cs=0,\n            coverage=0,\n            lead_snps=[],\n            snps=[],\n            cs_sizes=[],\n            pips=pd.Series(),\n            parameters=paras,\n        )\n    if len(creds) == 1:\n        return creds[0]\n    if combine_cred == \"union\":\n        merged_snps_flat = []\n        for cred in creds:\n            snps = [i for snp in cred.snps for i in snp]\n            merged_snps_flat.extend(snps)\n        merged_snps = [list(set(merged_snps_flat))]\n    elif combine_cred == \"intersection\":\n        merged_snps_set = None\n        for i, cred in enumerate(creds):\n            snps = [item for snp in cred.snps for item in snp]\n            if i == 0:\n                merged_snps_set = set(snps)\n            else:\n                if merged_snps_set is not None:\n                    merged_snps_set.intersection_update(set(snps))\n        if merged_snps_set is None or len(merged_snps_set) == 0:\n            logger.warning(\"No common SNPs found in the intersection of credible sets.\")\n            merged_snps = [[]]\n        else:\n            merged_snps = [list(merged_snps_set)]\n    elif combine_cred == \"cluster\":\n        cred_pips = []\n        for cred in creds:\n            cred_pip = [dict(cred.pips[cred.pips.index.isin(snp)]) for snp in cred.snps]\n            cred_pips.append(cred_pip)\n        merged_snps = cluster_cs(cred_pips, 1 - jaccard_threshold)\n        paras[\"jaccard_threshold\"] = jaccard_threshold\n    else:\n        raise ValueError(f\"Method {combine_cred} is not supported.\")\n    merged_pips = combine_pips([cred.pips for cred in creds], combine_pip)\n    paras[\"combine_cred\"] = combine_cred\n    paras[\"combine_pip\"] = combine_pip\n    merged = CredibleSet(\n        tool=creds[0].tool,\n        n_cs=len(merged_snps),\n        coverage=creds[0].coverage,\n        lead_snps=[\n            str(merged_pips[merged_pips.index.isin(snp)].idxmax())\n            for snp in merged_snps\n        ],\n        snps=merged_snps,\n        cs_sizes=[len(i) for i in merged_snps],\n        pips=merged_pips,\n        parameters=paras,\n    )\n    return merged\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.combine_pips","title":"<code>combine_pips(pips, method='max')</code>","text":"<p>Combine PIPs from multiple tools.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_pips--parameters","title":"Parameters","text":"<p>pips : List[pd.Series]     List of PIPs from multiple tools. method : str, optional     Method to combine PIPs, by default \"max\".     Options: \"max\", \"min\", \"mean\", \"meta\".     When \"meta\" is selected, the method will use the formula:     PIP_meta = 1 - prod(1 - PIP_i), where i is the index of tools,     PIP_i = 0 when the SNP is not in the credible set of the tool.     When \"max\", \"min\", \"mean\" is selected, the SNP not in the credible set     will be excluded from the calculation.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_pips--returns","title":"Returns","text":"<p>pd.Series     Combined PIPs.</p>"},{"location":"API/credibleset/#credtools.credibleset.combine_pips--raises","title":"Raises","text":"<p>ValueError     If the method is not supported.</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def combine_pips(pips: List[pd.Series], method: str = \"max\") -&gt; pd.Series:\n    \"\"\"\n    Combine PIPs from multiple tools.\n\n    Parameters\n    ----------\n    pips : List[pd.Series]\n        List of PIPs from multiple tools.\n    method : str, optional\n        Method to combine PIPs, by default \"max\".\n        Options: \"max\", \"min\", \"mean\", \"meta\".\n        When \"meta\" is selected, the method will use the formula:\n        PIP_meta = 1 - prod(1 - PIP_i), where i is the index of tools,\n        PIP_i = 0 when the SNP is not in the credible set of the tool.\n        When \"max\", \"min\", \"mean\" is selected, the SNP not in the credible set\n        will be excluded from the calculation.\n\n    Returns\n    -------\n    pd.Series\n        Combined PIPs.\n\n    Raises\n    ------\n    ValueError\n        If the method is not supported.\n    \"\"\"\n    logger.info(f\"Combining PIPs using method: {method}\")\n    pip_df = pd.DataFrame(pips).T\n    pip_df = pip_df.fillna(0)\n    if method == \"meta\":\n        merged = 1 - np.prod(1 - pip_df, axis=1)\n    elif method == \"max\":\n        merged = pip_df.max(axis=1)\n    elif method == \"min\":\n        merged = pip_df.min(axis=1)\n    elif method == \"mean\":\n        merged = pip_df.mean(axis=1)\n    else:\n        raise ValueError(f\"Method {method} is not supported.\")\n    return merged\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.continuous_jaccard","title":"<code>continuous_jaccard(dict1, dict2)</code>","text":"<p>Calculate modified Jaccard similarity for continuous values (PIP values).</p> <p>Formula: \u2211min(xi,yi)/\u2211max(xi,yi) where xi, yi are PIP values or 0 if missing</p>"},{"location":"API/credibleset/#credtools.credibleset.continuous_jaccard--parameters","title":"Parameters","text":"<p>dict1 : Dict[str, float]     First dictionary with keys and PIP values (0-1). dict2 : Dict[str, float]     Second dictionary with keys and PIP values (0-1).</p>"},{"location":"API/credibleset/#credtools.credibleset.continuous_jaccard--returns","title":"Returns","text":"<p>float     Modified Jaccard similarity index between 0 and 1.</p>"},{"location":"API/credibleset/#credtools.credibleset.continuous_jaccard--raises","title":"Raises","text":"<p>ValueError     If any values are not between 0 and 1.</p>"},{"location":"API/credibleset/#credtools.credibleset.continuous_jaccard--examples","title":"Examples","text":"<p>d1 = {'a': 0.8, 'b': 0.5} d2 = {'b': 0.6, 'c': 0.3} continuous_jaccard(d1, d2) 0.5</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def continuous_jaccard(dict1: Dict[str, float], dict2: Dict[str, float]) -&gt; float:\n    \"\"\"\n    Calculate modified Jaccard similarity for continuous values (PIP values).\n\n    Formula: \u2211min(xi,yi)/\u2211max(xi,yi) where xi, yi are PIP values or 0 if missing\n\n    Parameters\n    ----------\n    dict1 : Dict[str, float]\n        First dictionary with keys and PIP values (0-1).\n    dict2 : Dict[str, float]\n        Second dictionary with keys and PIP values (0-1).\n\n    Returns\n    -------\n    float\n        Modified Jaccard similarity index between 0 and 1.\n\n    Raises\n    ------\n    ValueError\n        If any values are not between 0 and 1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; d1 = {'a': 0.8, 'b': 0.5}\n    &gt;&gt;&gt; d2 = {'b': 0.6, 'c': 0.3}\n    &gt;&gt;&gt; continuous_jaccard(d1, d2)\n    0.5\n    \"\"\"\n    # Validate input values\n    for d in [dict1, dict2]:\n        invalid_values = [v for v in d.values() if not (0 &lt;= v &lt;= 1)]\n        if invalid_values:\n            raise ValueError(\"All values must be between 0 and 1\")\n\n    # Get all keys\n    all_keys = set(dict1.keys()).union(set(dict2.keys()))\n\n    # Calculate sum of minimums and maximums\n    sum_min = 0.0\n    sum_max = 0.0\n\n    for key in all_keys:\n        val1 = dict1.get(key, 0.0)\n        val2 = dict2.get(key, 0.0)\n        sum_min += min(val1, val2)\n        sum_max += max(val1, val2)\n\n    return sum_min / sum_max if sum_max &gt; 0 else 0.0\n</code></pre>"},{"location":"API/credibleset/#credtools.credibleset.create_similarity_matrix","title":"<code>create_similarity_matrix(dict_sets)</code>","text":"<p>Create a similarity matrix for all pairs of dictionaries across different sets.</p>"},{"location":"API/credibleset/#credtools.credibleset.create_similarity_matrix--parameters","title":"Parameters","text":"<p>dict_sets : List[List[Dict[str, float]]]     List of m sets, where each set contains dictionaries with PIP values.</p>"},{"location":"API/credibleset/#credtools.credibleset.create_similarity_matrix--returns","title":"Returns","text":"<p>Tuple[np.ndarray, List[Dict[str, float]]]     A tuple containing:     - Similarity matrix (n_dicts x n_dicts)     - Flattened list of dictionaries</p>"},{"location":"API/credibleset/#credtools.credibleset.create_similarity_matrix--examples","title":"Examples","text":"<p>sets = [[{'a': 0.8, 'b': 0.5}], [{'b': 0.6, 'c': 0.3}]] matrix, dicts = create_similarity_matrix(sets)</p> Source code in <code>credtools/credibleset.py</code> <pre><code>def create_similarity_matrix(\n    dict_sets: List[List[Dict[str, float]]],\n) -&gt; Tuple[np.ndarray, List[Dict[str, float]]]:\n    \"\"\"\n    Create a similarity matrix for all pairs of dictionaries across different sets.\n\n    Parameters\n    ----------\n    dict_sets : List[List[Dict[str, float]]]\n        List of m sets, where each set contains dictionaries with PIP values.\n\n    Returns\n    -------\n    Tuple[np.ndarray, List[Dict[str, float]]]\n        A tuple containing:\n        - Similarity matrix (n_dicts x n_dicts)\n        - Flattened list of dictionaries\n\n    Examples\n    --------\n    &gt;&gt;&gt; sets = [[{'a': 0.8, 'b': 0.5}], [{'b': 0.6, 'c': 0.3}]]\n    &gt;&gt;&gt; matrix, dicts = create_similarity_matrix(sets)\n    \"\"\"\n    # Flatten all dictionaries while keeping track of their set membership\n    all_dicts = []\n    for dict_set in dict_sets:\n        all_dicts.extend(dict_set)\n\n    total_dicts = len(all_dicts)\n\n    # Create similarity matrix\n    similarity_matrix = np.zeros((total_dicts, total_dicts))\n\n    # Calculate set membership ranges\n    set_ranges = []\n    current_idx = 0\n    for dict_set in dict_sets:\n        set_ranges.append((current_idx, current_idx + len(dict_set)))\n        current_idx += len(dict_set)\n\n    # Fill similarity matrix\n    for i, j in combinations(range(total_dicts), 2):\n        # Check if dictionaries are from the same set\n        same_set = False\n        for start, end in set_ranges:\n            if start &lt;= i &lt; end and start &lt;= j &lt; end:\n                same_set = True\n                break\n\n        if not same_set:\n            similarity = continuous_jaccard(all_dicts[i], all_dicts[j])\n            similarity_matrix[i, j] = similarity\n            similarity_matrix[j, i] = similarity\n\n    return similarity_matrix, all_dicts\n</code></pre>"},{"location":"API/credtools/","title":"credtools","text":"<p>Top-level package for credtools.</p>"},{"location":"API/ldmatrix/","title":"ldmatrix","text":"<p>Functions for reading and converting lower triangle matrices.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix","title":"<code>LDMatrix</code>","text":"<p>Class to store the LD matrix and the corresponding Variant IDs.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix--parameters","title":"Parameters","text":"<p>map_df : pd.DataFrame     DataFrame containing the Variant IDs. r : np.ndarray     LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix--attributes","title":"Attributes","text":"<p>map : pd.DataFrame     DataFrame containing the Variant IDs. r : np.ndarray     LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix--raises","title":"Raises","text":"<p>ValueError     If the number of rows in the map file does not match the number of rows in the LD matrix.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>class LDMatrix:\n    \"\"\"\n    Class to store the LD matrix and the corresponding Variant IDs.\n\n    Parameters\n    ----------\n    map_df : pd.DataFrame\n        DataFrame containing the Variant IDs.\n    r : np.ndarray\n        LD matrix.\n\n    Attributes\n    ----------\n    map : pd.DataFrame\n        DataFrame containing the Variant IDs.\n    r : np.ndarray\n        LD matrix.\n\n    Raises\n    ------\n    ValueError\n        If the number of rows in the map file does not match the number of rows in the LD matrix.\n    \"\"\"\n\n    def __init__(self, map_df: pd.DataFrame, r: np.ndarray) -&gt; None:\n        \"\"\"\n        Initialize the LDMatrix object.\n\n        Parameters\n        ----------\n        map_df : pd.DataFrame\n            DataFrame containing the Variant IDs.\n        r : np.ndarray\n            LD matrix.\n\n        Raises\n        ------\n        ValueError\n            If the number of rows in the map file does not match the number of rows in the LD matrix.\n        \"\"\"\n        self.map = map_df\n        self.r = r\n        self.__check_length()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the LDMatrix object.\n\n        Returns\n        -------\n        str\n            String representation showing the shapes of map and r.\n        \"\"\"\n        return f\"LDMatrix(map={self.map.shape}, r={self.r.shape})\"\n\n    def __check_length(self) -&gt; None:\n        \"\"\"\n        Check if the number of rows in the map file matches the number of rows in the LD matrix.\n\n        Raises\n        ------\n        ValueError\n            If the number of rows in the map file does not match the number of rows in the LD matrix.\n        \"\"\"\n        if len(self.map) != len(self.r):\n            raise ValueError(\n                \"The number of rows in the map file does not match the number of rows in the LD matrix.\"\n            )\n\n    def copy(self) -&gt; \"LDMatrix\":\n        \"\"\"\n        Return a copy of the LDMatrix object.\n\n        Returns\n        -------\n        LDMatrix\n            A copy of the LDMatrix object.\n        \"\"\"\n        return LDMatrix(self.map.copy(), self.r.copy())\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__check_length","title":"<code>__check_length()</code>","text":"<p>Check if the number of rows in the map file matches the number of rows in the LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__check_length--raises","title":"Raises","text":"<p>ValueError     If the number of rows in the map file does not match the number of rows in the LD matrix.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def __check_length(self) -&gt; None:\n    \"\"\"\n    Check if the number of rows in the map file matches the number of rows in the LD matrix.\n\n    Raises\n    ------\n    ValueError\n        If the number of rows in the map file does not match the number of rows in the LD matrix.\n    \"\"\"\n    if len(self.map) != len(self.r):\n        raise ValueError(\n            \"The number of rows in the map file does not match the number of rows in the LD matrix.\"\n        )\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__init__","title":"<code>__init__(map_df, r)</code>","text":"<p>Initialize the LDMatrix object.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__init__--parameters","title":"Parameters","text":"<p>map_df : pd.DataFrame     DataFrame containing the Variant IDs. r : np.ndarray     LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__init__--raises","title":"Raises","text":"<p>ValueError     If the number of rows in the map file does not match the number of rows in the LD matrix.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def __init__(self, map_df: pd.DataFrame, r: np.ndarray) -&gt; None:\n    \"\"\"\n    Initialize the LDMatrix object.\n\n    Parameters\n    ----------\n    map_df : pd.DataFrame\n        DataFrame containing the Variant IDs.\n    r : np.ndarray\n        LD matrix.\n\n    Raises\n    ------\n    ValueError\n        If the number of rows in the map file does not match the number of rows in the LD matrix.\n    \"\"\"\n    self.map = map_df\n    self.r = r\n    self.__check_length()\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the LDMatrix object.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.__repr__--returns","title":"Returns","text":"<p>str     String representation showing the shapes of map and r.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the LDMatrix object.\n\n    Returns\n    -------\n    str\n        String representation showing the shapes of map and r.\n    \"\"\"\n    return f\"LDMatrix(map={self.map.shape}, r={self.r.shape})\"\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.copy","title":"<code>copy()</code>","text":"<p>Return a copy of the LDMatrix object.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.LDMatrix.copy--returns","title":"Returns","text":"<p>LDMatrix     A copy of the LDMatrix object.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def copy(self) -&gt; \"LDMatrix\":\n    \"\"\"\n    Return a copy of the LDMatrix object.\n\n    Returns\n    -------\n    LDMatrix\n        A copy of the LDMatrix object.\n    \"\"\"\n    return LDMatrix(self.map.copy(), self.r.copy())\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld","title":"<code>load_ld(ld_path, map_path, delimiter='\\t', if_sort_alleles=True)</code>","text":"<p>Read LD matrices and Variant IDs from files. Pair each matrix with its corresponding Variant IDs.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld--parameters","title":"Parameters","text":"<p>ld_path : str     Path to the input text file containing the lower triangle matrix or .npz file. map_path : str     Path to the input text file containing the Variant IDs. delimiter : str, optional     Delimiter used in the input file, by default \"\\t\". if_sort_alleles : bool, optional     Sort alleles in the LD map in alphabetical order and change the sign of the     LD matrix if the alleles are swapped, by default True.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld--returns","title":"Returns","text":"<p>LDMatrix     Object containing the LD matrix and the Variant IDs.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld--raises","title":"Raises","text":"<p>ValueError     If the number of variants in the map file does not match the number of rows in the LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld--notes","title":"Notes","text":"<p>Future enhancements planned:</p> <ul> <li>Support for npz files (partially implemented)</li> <li>Support for plink bin4 format</li> <li>Support for ldstore bcor format</li> </ul> <p>The function validates that the LD matrix and map file have consistent dimensions and optionally sorts alleles for consistent representation.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld--examples","title":"Examples","text":"<p>ld_matrix = load_ld('data.ld', 'data.ldmap') print(f\"Loaded LD matrix with {ld_matrix.r.shape[0]} variants\") Loaded LD matrix with 1000 variants</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def load_ld(\n    ld_path: str, map_path: str, delimiter: str = \"\\t\", if_sort_alleles: bool = True\n) -&gt; LDMatrix:\n    r\"\"\"\n    Read LD matrices and Variant IDs from files. Pair each matrix with its corresponding Variant IDs.\n\n    Parameters\n    ----------\n    ld_path : str\n        Path to the input text file containing the lower triangle matrix or .npz file.\n    map_path : str\n        Path to the input text file containing the Variant IDs.\n    delimiter : str, optional\n        Delimiter used in the input file, by default \"\\t\".\n    if_sort_alleles : bool, optional\n        Sort alleles in the LD map in alphabetical order and change the sign of the\n        LD matrix if the alleles are swapped, by default True.\n\n    Returns\n    -------\n    LDMatrix\n        Object containing the LD matrix and the Variant IDs.\n\n    Raises\n    ------\n    ValueError\n        If the number of variants in the map file does not match the number of rows in the LD matrix.\n\n    Notes\n    -----\n    Future enhancements planned:\n\n    - Support for npz files (partially implemented)\n    - Support for plink bin4 format\n    - Support for ldstore bcor format\n\n    The function validates that the LD matrix and map file have consistent dimensions\n    and optionally sorts alleles for consistent representation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; ld_matrix = load_ld('data.ld', 'data.ldmap')\n    &gt;&gt;&gt; print(f\"Loaded LD matrix with {ld_matrix.r.shape[0]} variants\")\n    Loaded LD matrix with 1000 variants\n    \"\"\"\n    ld_df = load_ld_matrix(ld_path, delimiter)\n    logger.info(f\"Loaded LD matrix with shape {ld_df.shape} from '{ld_path}'.\")\n    map_df = load_ld_map(map_path, delimiter)\n    logger.info(f\"Loaded map file with shape {map_df.shape} from '{map_path}'.\")\n    if ld_df.shape[0] != map_df.shape[0]:\n        raise ValueError(\n            \"The number of variants in the map file does not match the number of rows in the LD matrix.\\n\"\n            f\"Number of variants in the map file: {map_df.shape[0]}, number of rows in the LD matrix: {ld_df.shape[0]}\"\n            f\"ld_path: {ld_path}, map_path: {map_path}\"\n        )\n    ld = LDMatrix(map_df, ld_df)\n    if if_sort_alleles:\n        ld = sort_alleles(ld)\n\n    return ld\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map","title":"<code>load_ld_map(map_path, delimiter='\\t')</code>","text":"<p>Read Variant IDs from a file.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--parameters","title":"Parameters","text":"<p>map_path : str     Path to the input text file containing the Variant IDs. delimiter : str, optional     Delimiter used in the input file, by default \"\\t\".</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame containing the Variant IDs with columns CHR, BP, A1, A2, and SNPID.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--raises","title":"Raises","text":"<p>ValueError     If the input file is empty or does not contain the required columns.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--notes","title":"Notes","text":"<p>This function assumes that the input file contains the required columns:</p> <ul> <li>Chromosome (CHR)</li> <li>Base pair position (BP)</li> <li>Allele 1 (A1)</li> <li>Allele 2 (A2)</li> </ul> <p>The function performs data cleaning including:</p> <ul> <li>Converting chromosome and position to appropriate types</li> <li>Validating alleles are valid DNA bases (A, C, G, T)</li> <li>Removing variants where A1 == A2</li> <li>Creating unique SNPID identifiers</li> </ul>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--examples","title":"Examples","text":"Source code in <code>credtools/ldmatrix.py</code> <pre><code>def load_ld_map(map_path: str, delimiter: str = \"\\t\") -&gt; pd.DataFrame:\n    r\"\"\"\n    Read Variant IDs from a file.\n\n    Parameters\n    ----------\n    map_path : str\n        Path to the input text file containing the Variant IDs.\n    delimiter : str, optional\n        Delimiter used in the input file, by default \"\\t\".\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the Variant IDs with columns CHR, BP, A1, A2, and SNPID.\n\n    Raises\n    ------\n    ValueError\n        If the input file is empty or does not contain the required columns.\n\n    Notes\n    -----\n    This function assumes that the input file contains the required columns:\n\n    - Chromosome (CHR)\n    - Base pair position (BP)\n    - Allele 1 (A1)\n    - Allele 2 (A2)\n\n    The function performs data cleaning including:\n\n    - Converting chromosome and position to appropriate types\n    - Validating alleles are valid DNA bases (A, C, G, T)\n    - Removing variants where A1 == A2\n    - Creating unique SNPID identifiers\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Create sample map file\n    &gt;&gt;&gt; contents = \"CHR\\\\tBP\\\\tA1\\\\tA2\\\\n1\\\\t1000\\\\tA\\\\tG\\\\n1\\\\t2000\\\\tC\\\\tT\\\\n2\\\\t3000\\\\tT\\\\tC\"\n    &gt;&gt;&gt; with open('map.txt', 'w') as file:\n    ...     file.write(contents)\n    &gt;&gt;&gt; df = load_ld_map('map.txt')\n    &gt;&gt;&gt; print(df)\n        SNPID       CHR    BP A1 A2\n    0   1-1000-A-G    1  1000  A  G\n    1   1-2000-C-T    1  2000  C  T\n    2   2-3000-C-T    2  3000  T  C\n    \"\"\"\n    # TODO: use REF/ALT instead of A1/A2\n    map_df = pd.read_csv(map_path, sep=delimiter)\n    missing_cols = [col for col in ColName.map_cols if col not in map_df.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing columns in the input file: {missing_cols}\")\n    outdf = munge_chr(map_df)\n    outdf = munge_bp(outdf)\n    for col in [ColName.A1, ColName.A2]:\n        pre_n = outdf.shape[0]\n        outdf = outdf[outdf[col].notnull()]\n        outdf[col] = outdf[col].astype(str).str.upper()\n        outdf = outdf[outdf[col].str.match(r\"^[ACGT]+$\")]\n        after_n = outdf.shape[0]\n        logger.debug(f\"Remove {pre_n - after_n} rows because of invalid {col}.\")\n    outdf = outdf[outdf[ColName.A1] != outdf[ColName.A2]]\n    outdf = make_SNPID_unique(\n        outdf, col_ea=ColName.A1, col_nea=ColName.A2, remove_duplicates=False\n    )\n    outdf.reset_index(drop=True, inplace=True)\n    # TODO: check if allele frequency is available\n    return outdf\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_map--create-sample-map-file","title":"Create sample map file","text":"<p>contents = \"CHR\\tBP\\tA1\\tA2\\n1\\t1000\\tA\\tG\\n1\\t2000\\tC\\tT\\n2\\t3000\\tT\\tC\" with open('map.txt', 'w') as file: ...     file.write(contents) df = load_ld_map('map.txt') print(df)     SNPID       CHR    BP A1 A2 0   1-1000-A-G    1  1000  A  G 1   1-2000-C-T    1  2000  C  T 2   2-3000-C-T    2  3000  T  C</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix","title":"<code>load_ld_matrix(file_path, delimiter='\\t')</code>","text":"<p>Convert a lower triangle matrix from a file to a symmetric square matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--parameters","title":"Parameters","text":"<p>file_path : str     Path to the input text file containing the lower triangle matrix. delimiter : str, optional     Delimiter used in the input file, by default \"\\t\".</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--returns","title":"Returns","text":"<p>np.ndarray     Symmetric square matrix with diagonal filled with 1.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--raises","title":"Raises","text":"<p>ValueError     If the input file is empty or does not contain a valid lower triangle matrix. FileNotFoundError     If the specified file does not exist.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--notes","title":"Notes","text":"<p>This function assumes that the input file contains a valid lower triangle matrix with each row on a new line and elements separated by the specified delimiter. For .npz files, it loads the first array key in the file.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--examples","title":"Examples","text":"Source code in <code>credtools/ldmatrix.py</code> <pre><code>def load_ld_matrix(file_path: str, delimiter: str = \"\\t\") -&gt; np.ndarray:\n    r\"\"\"\n    Convert a lower triangle matrix from a file to a symmetric square matrix.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input text file containing the lower triangle matrix.\n    delimiter : str, optional\n        Delimiter used in the input file, by default \"\\t\".\n\n    Returns\n    -------\n    np.ndarray\n        Symmetric square matrix with diagonal filled with 1.\n\n    Raises\n    ------\n    ValueError\n        If the input file is empty or does not contain a valid lower triangle matrix.\n    FileNotFoundError\n        If the specified file does not exist.\n\n    Notes\n    -----\n    This function assumes that the input file contains a valid lower triangle matrix\n    with each row on a new line and elements separated by the specified delimiter.\n    For .npz files, it loads the first array key in the file.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Assuming 'lower_triangle.txt' contains:\n    &gt;&gt;&gt; # 1.0\n    &gt;&gt;&gt; # 0.1 1.0\n    &gt;&gt;&gt; # 0.2 0.4 1.0\n    &gt;&gt;&gt; # 0.3 0.5 0.6 1.0\n    &gt;&gt;&gt; matrix = load_ld_matrix('lower_triangle.txt')\n    &gt;&gt;&gt; print(matrix)\n    array([[1.  , 0.1 , 0.2 , 0.3 ],\n            [0.1 , 1.  , 0.4 , 0.5 ],\n            [0.2 , 0.4 , 1.  , 0.6 ],\n            [0.3 , 0.5 , 0.6 , 1.  ]])\n    \"\"\"\n    if file_path.endswith(\".npz\"):\n        ld_file_key = np.load(file_path).files[0]\n        return np.load(file_path)[ld_file_key].astype(np.float32)\n    lower_triangle = read_lower_triangle(file_path, delimiter)\n\n    # Create the symmetric matrix\n    symmetric_matrix = lower_triangle + lower_triangle.T\n\n    # Fill the diagonal with 1\n    np.fill_diagonal(symmetric_matrix, 1)\n\n    # convert to float32\n    symmetric_matrix = symmetric_matrix.astype(np.float32)\n    return symmetric_matrix\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--assuming-lower_triangletxt-contains","title":"Assuming 'lower_triangle.txt' contains:","text":""},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--10","title":"1.0","text":""},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--01-10","title":"0.1 1.0","text":""},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--02-04-10","title":"0.2 0.4 1.0","text":""},{"location":"API/ldmatrix/#credtools.ldmatrix.load_ld_matrix--03-05-06-10","title":"0.3 0.5 0.6 1.0","text":"<p>matrix = load_ld_matrix('lower_triangle.txt') print(matrix) array([[1.  , 0.1 , 0.2 , 0.3 ],         [0.1 , 1.  , 0.4 , 0.5 ],         [0.2 , 0.4 , 1.  , 0.6 ],         [0.3 , 0.5 , 0.6 , 1.  ]])</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.read_lower_triangle","title":"<code>read_lower_triangle(file_path, delimiter='\\t')</code>","text":"<p>Read a lower triangle matrix from a file.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.read_lower_triangle--parameters","title":"Parameters","text":"<p>file_path : str     Path to the input text file containing the lower triangle matrix. delimiter : str, optional     Delimiter used in the input file, by default \"\\t\".</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.read_lower_triangle--returns","title":"Returns","text":"<p>np.ndarray     Lower triangle matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.read_lower_triangle--raises","title":"Raises","text":"<p>ValueError     If the input file is empty or does not contain a valid lower triangle matrix. FileNotFoundError     If the specified file does not exist.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.read_lower_triangle--notes","title":"Notes","text":"<p>This function reads a lower triangular matrix where each row contains elements from the diagonal down to that row position.</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def read_lower_triangle(file_path: str, delimiter: str = \"\\t\") -&gt; np.ndarray:\n    r\"\"\"\n    Read a lower triangle matrix from a file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input text file containing the lower triangle matrix.\n    delimiter : str, optional\n        Delimiter used in the input file, by default \"\\t\".\n\n    Returns\n    -------\n    np.ndarray\n        Lower triangle matrix.\n\n    Raises\n    ------\n    ValueError\n        If the input file is empty or does not contain a valid lower triangle matrix.\n    FileNotFoundError\n        If the specified file does not exist.\n\n    Notes\n    -----\n    This function reads a lower triangular matrix where each row contains\n    elements from the diagonal down to that row position.\n    \"\"\"\n    try:\n        if file_path.endswith(\".gz\"):\n            with gzip.open(file_path, \"rt\") as file:\n                rows = [\n                    list(map(float, line.strip().split(delimiter)))\n                    for line in file\n                    if line.strip()\n                ]\n        else:\n            with open(file_path, \"r\") as file:\n                rows = [\n                    list(map(float, line.strip().split(delimiter)))\n                    for line in file\n                    if line.strip()\n                ]\n    except FileNotFoundError:\n        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n\n    if not rows:\n        raise ValueError(\"The input file is empty.\")\n\n    n = len(rows)\n    lower_triangle = np.zeros((n, n))\n\n    for i, row in enumerate(rows):\n        if len(row) != i + 1:\n            raise ValueError(\n                f\"Invalid number of elements in row {i + 1}. Expected {i + 1}, got {len(row)}.\"\n            )\n        lower_triangle[i, : len(row)] = row\n\n    return lower_triangle\n</code></pre>"},{"location":"API/ldmatrix/#credtools.ldmatrix.sort_alleles","title":"<code>sort_alleles(ld)</code>","text":"<p>Sort alleles in the LD map in alphabetical order. Change the sign of the LD matrix if the alleles are swapped.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.sort_alleles--parameters","title":"Parameters","text":"<p>ld : LDMatrix     LDMatrix object containing the Variant IDs and the LD matrix.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.sort_alleles--returns","title":"Returns","text":"<p>LDMatrix     LDMatrix object containing the Variant IDs and the LD matrix with alleles sorted.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.sort_alleles--notes","title":"Notes","text":"<p>This function ensures consistent allele ordering by:</p> <ol> <li>Sorting alleles alphabetically (A1 &lt;= A2)</li> <li>Flipping the sign of LD correlations for variants where alleles were swapped</li> <li>Maintaining diagonal elements as 1.0</li> </ol> <p>This is important for consistent merging across different datasets.</p>"},{"location":"API/ldmatrix/#credtools.ldmatrix.sort_alleles--examples","title":"Examples","text":"<p>map_df = pd.DataFrame({ ...     'SNPID': ['1-1000-A-G', '1-2000-C-T'], ...     'CHR': [1, 1], ...     'BP': [1000, 2000], ...     'A1': ['A', 'T'], ...     'A2': ['G', 'C'] ... }) r_matrix = np.array([[1. , 0.1], ...                      [0.1, 1. ]]) ld = LDMatrix(map_df, r_matrix) sorted_ld = sort_alleles(ld) print(sorted_ld.map)     SNPID       CHR    BP A1 A2 0   1-1000-A-G    1  1000  A  G 1   1-2000-C-T    1  2000  C  T print(sorted_ld.r) array([[ 1. , -0.1],         [-0.1,  1. ]])</p> Source code in <code>credtools/ldmatrix.py</code> <pre><code>def sort_alleles(ld: LDMatrix) -&gt; LDMatrix:\n    \"\"\"\n    Sort alleles in the LD map in alphabetical order. Change the sign of the LD matrix if the alleles are swapped.\n\n    Parameters\n    ----------\n    ld : LDMatrix\n        LDMatrix object containing the Variant IDs and the LD matrix.\n\n    Returns\n    -------\n    LDMatrix\n        LDMatrix object containing the Variant IDs and the LD matrix with alleles sorted.\n\n    Notes\n    -----\n    This function ensures consistent allele ordering by:\n\n    1. Sorting alleles alphabetically (A1 &lt;= A2)\n    2. Flipping the sign of LD correlations for variants where alleles were swapped\n    3. Maintaining diagonal elements as 1.0\n\n    This is important for consistent merging across different datasets.\n\n    Examples\n    --------\n    &gt;&gt;&gt; map_df = pd.DataFrame({\n    ...     'SNPID': ['1-1000-A-G', '1-2000-C-T'],\n    ...     'CHR': [1, 1],\n    ...     'BP': [1000, 2000],\n    ...     'A1': ['A', 'T'],\n    ...     'A2': ['G', 'C']\n    ... })\n    &gt;&gt;&gt; r_matrix = np.array([[1. , 0.1],\n    ...                      [0.1, 1. ]])\n    &gt;&gt;&gt; ld = LDMatrix(map_df, r_matrix)\n    &gt;&gt;&gt; sorted_ld = sort_alleles(ld)\n    &gt;&gt;&gt; print(sorted_ld.map)\n        SNPID       CHR    BP A1 A2\n    0   1-1000-A-G    1  1000  A  G\n    1   1-2000-C-T    1  2000  C  T\n    &gt;&gt;&gt; print(sorted_ld.r)\n    array([[ 1. , -0.1],\n            [-0.1,  1. ]])\n    \"\"\"\n    ld_df = ld.r.copy()\n    ld_map = ld.map.copy()\n    ld_map[[\"sort_a1\", \"sort_a2\"]] = np.sort(ld_map[[ColName.A1, ColName.A2]], axis=1)\n    swapped_index = ld_map[ld_map[ColName.A1] != ld_map[\"sort_a1\"]].index\n    # Change the sign of the rows and columns the LD matrix if the alleles are swapped\n    ld_df[swapped_index] *= -1\n    ld_df[:, swapped_index] *= -1\n    np.fill_diagonal(ld_df, 1)\n\n    ld_map[ColName.A1] = ld_map[\"sort_a1\"]\n    ld_map[ColName.A2] = ld_map[\"sort_a2\"]\n    ld_map.drop(columns=[\"sort_a1\", \"sort_a2\"], inplace=True)\n    return LDMatrix(ld_map, ld_df)\n</code></pre>"},{"location":"API/locus/","title":"locus","text":"<p>Class for the input data of the fine-mapping analysis.</p>"},{"location":"API/locus/#credtools.locus.Locus","title":"<code>Locus</code>","text":"<p>Locus class to represent a genomic locus with associated summary statistics and linkage disequilibrium (LD) matrix.</p>"},{"location":"API/locus/#credtools.locus.Locus--parameters","title":"Parameters","text":"<p>popu : str     Population code. e.g. \"EUR\". Choose from [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"]. cohort : str     Cohort name. sample_size : int     Sample size. sumstats : pd.DataFrame     Summary statistics DataFrame. ld : Optional[LDMatrix], optional     LD matrix, by default None. if_intersect : bool, optional     Whether to intersect the LD matrix and summary statistics file, by default False.</p>"},{"location":"API/locus/#credtools.locus.Locus--attributes","title":"Attributes","text":"<p>original_sumstats : pd.DataFrame     The original summary statistics file. sumstats : pd.DataFrame     The processed summary statistics file. ld : LDMatrix     The LD matrix object. chrom : int     Chromosome. start : int     Start position of the locus. end : int     End position of the locus. n_snps : int     Number of SNPs in the locus. prefix : str     The prefix combining population and cohort. locus_id : str     Unique identifier for the locus. is_matched : bool     Whether the LD matrix and summary statistics file are matched.</p>"},{"location":"API/locus/#credtools.locus.Locus--notes","title":"Notes","text":"<p>If no LD matrix is provided, only ABF method can be used for fine-mapping.</p> Source code in <code>credtools/locus.py</code> <pre><code>class Locus:\n    \"\"\"\n    Locus class to represent a genomic locus with associated summary statistics and linkage disequilibrium (LD) matrix.\n\n    Parameters\n    ----------\n    popu : str\n        Population code. e.g. \"EUR\". Choose from [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"].\n    cohort : str\n        Cohort name.\n    sample_size : int\n        Sample size.\n    sumstats : pd.DataFrame\n        Summary statistics DataFrame.\n    ld : Optional[LDMatrix], optional\n        LD matrix, by default None.\n    if_intersect : bool, optional\n        Whether to intersect the LD matrix and summary statistics file, by default False.\n\n    Attributes\n    ----------\n    original_sumstats : pd.DataFrame\n        The original summary statistics file.\n    sumstats : pd.DataFrame\n        The processed summary statistics file.\n    ld : LDMatrix\n        The LD matrix object.\n    chrom : int\n        Chromosome.\n    start : int\n        Start position of the locus.\n    end : int\n        End position of the locus.\n    n_snps : int\n        Number of SNPs in the locus.\n    prefix : str\n        The prefix combining population and cohort.\n    locus_id : str\n        Unique identifier for the locus.\n    is_matched : bool\n        Whether the LD matrix and summary statistics file are matched.\n\n    Notes\n    -----\n    If no LD matrix is provided, only ABF method can be used for fine-mapping.\n    \"\"\"\n\n    def __init__(\n        self,\n        popu: str,\n        cohort: str,\n        sample_size: int,\n        sumstats: pd.DataFrame,\n        ld: Optional[LDMatrix] = None,\n        if_intersect: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Locus object.\n\n        Parameters\n        ----------\n        popu : str\n            Population code. e.g. \"EUR\". Choose from [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"].\n        cohort : str\n            Cohort name.\n        sample_size : int\n            Sample size.\n        sumstats : pd.DataFrame\n            Summary statistics DataFrame.\n        ld : Optional[LDMatrix], optional\n            LD matrix, by default None.\n        if_intersect : bool, optional\n            Whether to intersect the LD matrix and summary statistics file, by default False.\n\n        Warnings\n        --------\n        If no LD matrix is provided, a warning is logged that only ABF method can be used.\n        \"\"\"\n        self.sumstats = sumstats\n        self._original_sumstats = self.sumstats.copy()\n        self._popu = popu\n        self._cohort = cohort\n        self._sample_size = sample_size\n        if ld:\n            self.ld = ld\n            if if_intersect:\n                inters = intersect_sumstat_ld(self)\n                self.sumstats = inters.sumstats\n                self.ld = inters.ld\n        else:\n            logger.warning(\"LD matrix and map file not found. Can only run ABF method.\")\n            self.ld = LDMatrix(pd.DataFrame(), np.array([]))\n\n    @property\n    def original_sumstats(self) -&gt; pd.DataFrame:\n        \"\"\"Get the original sumstats file.\"\"\"\n        return self._original_sumstats\n\n    @property\n    def popu(self) -&gt; str:\n        \"\"\"Get the population code.\"\"\"\n        return self._popu\n\n    @property\n    def cohort(self) -&gt; str:\n        \"\"\"Get the cohort name.\"\"\"\n        return self._cohort\n\n    @property\n    def sample_size(self) -&gt; int:\n        \"\"\"Get the sample size.\"\"\"\n        return self._sample_size\n\n    @property\n    def chrom(self) -&gt; int:\n        \"\"\"Get the chromosome.\"\"\"\n        return self.sumstats[ColName.CHR].iloc[0]\n\n    @property\n    def start(self) -&gt; int:\n        \"\"\"Get the start position.\"\"\"\n        return self.sumstats[ColName.BP].min()\n\n    @property\n    def end(self) -&gt; int:\n        \"\"\"Get the end position.\"\"\"\n        return self.sumstats[ColName.BP].max()\n\n    @property\n    def n_snps(self) -&gt; int:\n        \"\"\"Get the number of SNPs.\"\"\"\n        return len(self.sumstats)\n\n    @property\n    def prefix(self) -&gt; str:\n        \"\"\"Get the prefix of the locus.\"\"\"\n        return f\"{self.popu}_{self.cohort}\"\n\n    @property\n    def locus_id(self) -&gt; str:\n        \"\"\"Get the locus ID.\"\"\"\n        return f\"{self.popu}_{self.cohort}_chr{self.chrom}:{self.start}-{self.end}\"\n\n    @property\n    def is_matched(self) -&gt; bool:\n        \"\"\"Check if the LD matrix and sumstats file are matched.\"\"\"\n        # check the order of SNPID in the LD matrix and the sumstats file are the exact same\n        if self.ld is None:\n            return False\n        return self.ld.map[ColName.SNPID].equals(self.sumstats[ColName.SNPID])\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the Locus object.\n\n        Returns\n        -------\n        str\n            String representation of the Locus object.\n        \"\"\"\n        return f\"Locus(popu={self.popu}, cohort={self.cohort}, sample_size={self.sample_size}, chr={self.chrom}, start={self.start}, end={self.end}, sumstats={self.sumstats.shape}, ld={self.ld.r.shape})\"\n\n    def copy(self) -&gt; \"Locus\":\n        \"\"\"\n        Copy the Locus object.\n\n        Returns\n        -------\n        Locus\n            A copy of the Locus object.\n        \"\"\"\n        return Locus(\n            self.popu,\n            self.cohort,\n            self.sample_size,\n            self.sumstats.copy(),\n            self.ld.copy(),\n            if_intersect=False,\n        )\n</code></pre>"},{"location":"API/locus/#credtools.locus.Locus.chrom","title":"<code>chrom</code>  <code>property</code>","text":"<p>Get the chromosome.</p>"},{"location":"API/locus/#credtools.locus.Locus.cohort","title":"<code>cohort</code>  <code>property</code>","text":"<p>Get the cohort name.</p>"},{"location":"API/locus/#credtools.locus.Locus.end","title":"<code>end</code>  <code>property</code>","text":"<p>Get the end position.</p>"},{"location":"API/locus/#credtools.locus.Locus.is_matched","title":"<code>is_matched</code>  <code>property</code>","text":"<p>Check if the LD matrix and sumstats file are matched.</p>"},{"location":"API/locus/#credtools.locus.Locus.locus_id","title":"<code>locus_id</code>  <code>property</code>","text":"<p>Get the locus ID.</p>"},{"location":"API/locus/#credtools.locus.Locus.n_snps","title":"<code>n_snps</code>  <code>property</code>","text":"<p>Get the number of SNPs.</p>"},{"location":"API/locus/#credtools.locus.Locus.original_sumstats","title":"<code>original_sumstats</code>  <code>property</code>","text":"<p>Get the original sumstats file.</p>"},{"location":"API/locus/#credtools.locus.Locus.popu","title":"<code>popu</code>  <code>property</code>","text":"<p>Get the population code.</p>"},{"location":"API/locus/#credtools.locus.Locus.prefix","title":"<code>prefix</code>  <code>property</code>","text":"<p>Get the prefix of the locus.</p>"},{"location":"API/locus/#credtools.locus.Locus.sample_size","title":"<code>sample_size</code>  <code>property</code>","text":"<p>Get the sample size.</p>"},{"location":"API/locus/#credtools.locus.Locus.start","title":"<code>start</code>  <code>property</code>","text":"<p>Get the start position.</p>"},{"location":"API/locus/#credtools.locus.Locus.__init__","title":"<code>__init__(popu, cohort, sample_size, sumstats, ld=None, if_intersect=False)</code>","text":"<p>Initialize the Locus object.</p>"},{"location":"API/locus/#credtools.locus.Locus.__init__--parameters","title":"Parameters","text":"<p>popu : str     Population code. e.g. \"EUR\". Choose from [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"]. cohort : str     Cohort name. sample_size : int     Sample size. sumstats : pd.DataFrame     Summary statistics DataFrame. ld : Optional[LDMatrix], optional     LD matrix, by default None. if_intersect : bool, optional     Whether to intersect the LD matrix and summary statistics file, by default False.</p>"},{"location":"API/locus/#credtools.locus.Locus.__init__--warnings","title":"Warnings","text":"<p>If no LD matrix is provided, a warning is logged that only ABF method can be used.</p> Source code in <code>credtools/locus.py</code> <pre><code>def __init__(\n    self,\n    popu: str,\n    cohort: str,\n    sample_size: int,\n    sumstats: pd.DataFrame,\n    ld: Optional[LDMatrix] = None,\n    if_intersect: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the Locus object.\n\n    Parameters\n    ----------\n    popu : str\n        Population code. e.g. \"EUR\". Choose from [\"AFR\", \"AMR\", \"EAS\", \"EUR\", \"SAS\"].\n    cohort : str\n        Cohort name.\n    sample_size : int\n        Sample size.\n    sumstats : pd.DataFrame\n        Summary statistics DataFrame.\n    ld : Optional[LDMatrix], optional\n        LD matrix, by default None.\n    if_intersect : bool, optional\n        Whether to intersect the LD matrix and summary statistics file, by default False.\n\n    Warnings\n    --------\n    If no LD matrix is provided, a warning is logged that only ABF method can be used.\n    \"\"\"\n    self.sumstats = sumstats\n    self._original_sumstats = self.sumstats.copy()\n    self._popu = popu\n    self._cohort = cohort\n    self._sample_size = sample_size\n    if ld:\n        self.ld = ld\n        if if_intersect:\n            inters = intersect_sumstat_ld(self)\n            self.sumstats = inters.sumstats\n            self.ld = inters.ld\n    else:\n        logger.warning(\"LD matrix and map file not found. Can only run ABF method.\")\n        self.ld = LDMatrix(pd.DataFrame(), np.array([]))\n</code></pre>"},{"location":"API/locus/#credtools.locus.Locus.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the Locus object.</p>"},{"location":"API/locus/#credtools.locus.Locus.__repr__--returns","title":"Returns","text":"<p>str     String representation of the Locus object.</p> Source code in <code>credtools/locus.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the Locus object.\n\n    Returns\n    -------\n    str\n        String representation of the Locus object.\n    \"\"\"\n    return f\"Locus(popu={self.popu}, cohort={self.cohort}, sample_size={self.sample_size}, chr={self.chrom}, start={self.start}, end={self.end}, sumstats={self.sumstats.shape}, ld={self.ld.r.shape})\"\n</code></pre>"},{"location":"API/locus/#credtools.locus.Locus.copy","title":"<code>copy()</code>","text":"<p>Copy the Locus object.</p>"},{"location":"API/locus/#credtools.locus.Locus.copy--returns","title":"Returns","text":"<p>Locus     A copy of the Locus object.</p> Source code in <code>credtools/locus.py</code> <pre><code>def copy(self) -&gt; \"Locus\":\n    \"\"\"\n    Copy the Locus object.\n\n    Returns\n    -------\n    Locus\n        A copy of the Locus object.\n    \"\"\"\n    return Locus(\n        self.popu,\n        self.cohort,\n        self.sample_size,\n        self.sumstats.copy(),\n        self.ld.copy(),\n        if_intersect=False,\n    )\n</code></pre>"},{"location":"API/locus/#credtools.locus.LocusSet","title":"<code>LocusSet</code>","text":"<p>LocusSet class to represent a set of genomic loci.</p>"},{"location":"API/locus/#credtools.locus.LocusSet--parameters","title":"Parameters","text":"<p>loci : List[Locus]     List of Locus objects.</p>"},{"location":"API/locus/#credtools.locus.LocusSet--attributes","title":"Attributes","text":"<p>loci : List[Locus]     List of Locus objects. n_loci : int     Number of loci. chrom : int     Chromosome number. start : int     Start position of the locus. end : int     End position of the locus. locus_id : str     Unique identifier for the locus.</p>"},{"location":"API/locus/#credtools.locus.LocusSet--raises","title":"Raises","text":"<p>ValueError     If the chromosomes of the loci are not the same.</p> Source code in <code>credtools/locus.py</code> <pre><code>class LocusSet:\n    \"\"\"\n    LocusSet class to represent a set of genomic loci.\n\n    Parameters\n    ----------\n    loci : List[Locus]\n        List of Locus objects.\n\n    Attributes\n    ----------\n    loci : List[Locus]\n        List of Locus objects.\n    n_loci : int\n        Number of loci.\n    chrom : int\n        Chromosome number.\n    start : int\n        Start position of the locus.\n    end : int\n        End position of the locus.\n    locus_id : str\n        Unique identifier for the locus.\n\n    Raises\n    ------\n    ValueError\n        If the chromosomes of the loci are not the same.\n    \"\"\"\n\n    def __init__(self, loci: List[Locus]) -&gt; None:\n        \"\"\"\n        Initialize the LocusSet object.\n\n        Parameters\n        ----------\n        loci : List[Locus]\n            List of Locus objects.\n        \"\"\"\n        self.loci = loci\n\n    @property\n    def n_loci(self) -&gt; int:\n        \"\"\"Get the number of loci.\"\"\"\n        return len(self.loci)\n\n    @property\n    def chrom(self) -&gt; int:\n        \"\"\"\n        Get the chromosome.\n\n        Returns\n        -------\n        int\n            Chromosome number.\n\n        Raises\n        ------\n        ValueError\n            If the chromosomes of the loci are not the same.\n        \"\"\"\n        chrom_list = [locus.chrom for locus in self.loci]\n        if len(set(chrom_list)) &gt; 1:\n            raise ValueError(\"The chromosomes of the loci are not the same.\")\n        return chrom_list[0]\n\n    @property\n    def start(self) -&gt; int:\n        \"\"\"Get the start position.\"\"\"\n        return min([locus.start for locus in self.loci])\n\n    @property\n    def end(self) -&gt; int:\n        \"\"\"Get the end position.\"\"\"\n        return max([locus.end for locus in self.loci])\n\n    @property\n    def locus_id(self) -&gt; str:\n        \"\"\"Get the locus ID.\"\"\"\n        return f\"{self.chrom}:{self.start}-{self.end}\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the LocusSet object.\n\n        Returns\n        -------\n        str\n            String representation of the LocusSet object.\n        \"\"\"\n        return (\n            f\"LocusSet(\\n n_loci={len(self.loci)}, chrom={self.chrom}, start={self.start}, end={self.end}, locus_id={self.locus_id} \\n\"\n            + \"\\n\".join([locus.__repr__() for locus in self.loci])\n            + \"\\n\"\n            + \")\"\n        )\n\n    def copy(self) -&gt; \"LocusSet\":\n        \"\"\"\n        Copy the LocusSet object.\n\n        Returns\n        -------\n        LocusSet\n            A copy of the LocusSet object.\n        \"\"\"\n        return LocusSet([locus.copy() for locus in self.loci])\n</code></pre>"},{"location":"API/locus/#credtools.locus.LocusSet.chrom","title":"<code>chrom</code>  <code>property</code>","text":"<p>Get the chromosome.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.chrom--returns","title":"Returns","text":"<p>int     Chromosome number.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.chrom--raises","title":"Raises","text":"<p>ValueError     If the chromosomes of the loci are not the same.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.end","title":"<code>end</code>  <code>property</code>","text":"<p>Get the end position.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.locus_id","title":"<code>locus_id</code>  <code>property</code>","text":"<p>Get the locus ID.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.n_loci","title":"<code>n_loci</code>  <code>property</code>","text":"<p>Get the number of loci.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.start","title":"<code>start</code>  <code>property</code>","text":"<p>Get the start position.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.__init__","title":"<code>__init__(loci)</code>","text":"<p>Initialize the LocusSet object.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.__init__--parameters","title":"Parameters","text":"<p>loci : List[Locus]     List of Locus objects.</p> Source code in <code>credtools/locus.py</code> <pre><code>def __init__(self, loci: List[Locus]) -&gt; None:\n    \"\"\"\n    Initialize the LocusSet object.\n\n    Parameters\n    ----------\n    loci : List[Locus]\n        List of Locus objects.\n    \"\"\"\n    self.loci = loci\n</code></pre>"},{"location":"API/locus/#credtools.locus.LocusSet.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a string representation of the LocusSet object.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.__repr__--returns","title":"Returns","text":"<p>str     String representation of the LocusSet object.</p> Source code in <code>credtools/locus.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a string representation of the LocusSet object.\n\n    Returns\n    -------\n    str\n        String representation of the LocusSet object.\n    \"\"\"\n    return (\n        f\"LocusSet(\\n n_loci={len(self.loci)}, chrom={self.chrom}, start={self.start}, end={self.end}, locus_id={self.locus_id} \\n\"\n        + \"\\n\".join([locus.__repr__() for locus in self.loci])\n        + \"\\n\"\n        + \")\"\n    )\n</code></pre>"},{"location":"API/locus/#credtools.locus.LocusSet.copy","title":"<code>copy()</code>","text":"<p>Copy the LocusSet object.</p>"},{"location":"API/locus/#credtools.locus.LocusSet.copy--returns","title":"Returns","text":"<p>LocusSet     A copy of the LocusSet object.</p> Source code in <code>credtools/locus.py</code> <pre><code>def copy(self) -&gt; \"LocusSet\":\n    \"\"\"\n    Copy the LocusSet object.\n\n    Returns\n    -------\n    LocusSet\n        A copy of the LocusSet object.\n    \"\"\"\n    return LocusSet([locus.copy() for locus in self.loci])\n</code></pre>"},{"location":"API/locus/#credtools.locus.intersect_loci","title":"<code>intersect_loci(list_loci)</code>","text":"<p>Intersect the Variant IDs in the LD matrices and the sumstats files of a list of Locus objects.</p>"},{"location":"API/locus/#credtools.locus.intersect_loci--parameters","title":"Parameters","text":"<p>list_loci : List[Locus]     List of Locus objects.</p>"},{"location":"API/locus/#credtools.locus.intersect_loci--returns","title":"Returns","text":"<p>List[Locus]     List of Locus objects containing the intersected LD matrices and sumstats files.</p>"},{"location":"API/locus/#credtools.locus.intersect_loci--raises","title":"Raises","text":"<p>NotImplementedError     This function is not yet implemented.</p>"},{"location":"API/locus/#credtools.locus.intersect_loci--notes","title":"Notes","text":"<p>This function is planned to intersect variant IDs across multiple loci to ensure consistent variant sets for multi-ancestry analysis.</p> Source code in <code>credtools/locus.py</code> <pre><code>def intersect_loci(list_loci: List[Locus]) -&gt; List[Locus]:\n    \"\"\"\n    Intersect the Variant IDs in the LD matrices and the sumstats files of a list of Locus objects.\n\n    Parameters\n    ----------\n    list_loci : List[Locus]\n        List of Locus objects.\n\n    Returns\n    -------\n    List[Locus]\n        List of Locus objects containing the intersected LD matrices and sumstats files.\n\n    Raises\n    ------\n    NotImplementedError\n        This function is not yet implemented.\n\n    Notes\n    -----\n    This function is planned to intersect variant IDs across multiple loci\n    to ensure consistent variant sets for multi-ancestry analysis.\n    \"\"\"\n    raise NotImplementedError(\n        \"Intersect the Variant IDs in the LD matrices and the sumstats files of a list of Locus objects.\"\n    )\n</code></pre>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld","title":"<code>intersect_sumstat_ld(locus)</code>","text":"<p>Intersect the Variant IDs in the LD matrix and the sumstats file.</p>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld--parameters","title":"Parameters","text":"<p>locus : Locus     Locus object containing LD matrix and summary statistics.</p>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld--returns","title":"Returns","text":"<p>Locus     Locus object containing the intersected LD matrix and sumstats file.</p>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld--raises","title":"Raises","text":"<p>ValueError     If LD matrix not found or no common Variant IDs found between the LD matrix and the sumstats file.</p>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld--warnings","title":"Warnings","text":"<p>If only a few common Variant IDs are found (\u2264 10), a warning is logged.</p>"},{"location":"API/locus/#credtools.locus.intersect_sumstat_ld--notes","title":"Notes","text":"<p>This function performs the following operations:</p> <ol> <li>Checks if LD matrix and summary statistics are already matched</li> <li>Finds common SNP IDs between LD matrix and summary statistics</li> <li>Subsets both datasets to common variants</li> <li>Reorders data to maintain consistency</li> <li>Returns a new Locus object with intersected data</li> </ol> Source code in <code>credtools/locus.py</code> <pre><code>def intersect_sumstat_ld(locus: Locus) -&gt; Locus:\n    \"\"\"\n    Intersect the Variant IDs in the LD matrix and the sumstats file.\n\n    Parameters\n    ----------\n    locus : Locus\n        Locus object containing LD matrix and summary statistics.\n\n    Returns\n    -------\n    Locus\n        Locus object containing the intersected LD matrix and sumstats file.\n\n    Raises\n    ------\n    ValueError\n        If LD matrix not found or no common Variant IDs found between the LD matrix and the sumstats file.\n\n    Warnings\n    --------\n    If only a few common Variant IDs are found (\u2264 10), a warning is logged.\n\n    Notes\n    -----\n    This function performs the following operations:\n\n    1. Checks if LD matrix and summary statistics are already matched\n    2. Finds common SNP IDs between LD matrix and summary statistics\n    3. Subsets both datasets to common variants\n    4. Reorders data to maintain consistency\n    5. Returns a new Locus object with intersected data\n    \"\"\"\n    if locus.ld is None:\n        raise ValueError(\"LD matrix not found.\")\n    if locus.is_matched:\n        logger.info(\"The LD matrix and sumstats file are matched.\")\n        return locus\n    ldmap = locus.ld.map.copy()\n    r = locus.ld.r.copy()\n    sumstats = locus.sumstats.copy()\n    sumstats = sumstats.sort_values([ColName.CHR, ColName.BP], ignore_index=True)\n    intersec_sumstats = sumstats[\n        sumstats[ColName.SNPID].isin(ldmap[ColName.SNPID])\n    ].copy()\n    intersec_variants = intersec_sumstats[ColName.SNPID].to_numpy()\n    if len(intersec_variants) == 0:\n        raise ValueError(\n            \"No common Variant IDs found between the LD matrix and the sumstats file.\"\n        )\n    elif len(intersec_variants) &lt;= 10:\n        logger.warning(\n            f\"Only a few common Variant IDs found between the LD matrix and the sumstats file(&lt;= 10) for locus {locus.locus_id}.\"\n        )\n    ldmap[\"idx\"] = ldmap.index\n    ldmap.set_index(ColName.SNPID, inplace=True, drop=False)\n    ldmap = ldmap.loc[intersec_variants].copy()\n    intersec_index = ldmap[\"idx\"].to_numpy()\n    r = r[intersec_index, :][:, intersec_index]\n    intersec_sumstats.reset_index(drop=True, inplace=True)\n    ldmap.drop(\"idx\", axis=1, inplace=True)\n    ldmap = ldmap.reset_index(drop=True)\n    intersec_ld = LDMatrix(ldmap, r)\n    logger.info(\n        \"Intersected the Variant IDs in the LD matrix and the sumstats file. \"\n        f\"Number of common Variant IDs: {len(intersec_index)}\"\n    )\n    return Locus(\n        locus.popu, locus.cohort, locus.sample_size, intersec_sumstats, intersec_ld\n    )\n</code></pre>"},{"location":"API/locus/#credtools.locus.load_locus","title":"<code>load_locus(prefix, popu, cohort, sample_size, if_intersect=False, **kwargs)</code>","text":"<p>Load the input data of the fine-mapping analysis.</p>"},{"location":"API/locus/#credtools.locus.load_locus--parameters","title":"Parameters","text":"<p>prefix : str     Prefix of the input files. popu : str     Population of the input data. cohort : str     Cohort of the input data. sample_size : int     Sample size of the input data. if_intersect : bool, optional     Whether to intersect the input data with the LD matrix, by default False. **kwargs : Any     Additional keyword arguments passed to loading functions.</p>"},{"location":"API/locus/#credtools.locus.load_locus--returns","title":"Returns","text":"<p>Locus     Locus object containing the input data.</p>"},{"location":"API/locus/#credtools.locus.load_locus--raises","title":"Raises","text":"<p>ValueError     If the required input files are not found.</p>"},{"location":"API/locus/#credtools.locus.load_locus--notes","title":"Notes","text":"<p>The function looks for files with the following patterns:</p> <ul> <li>Summary statistics: {prefix}.sumstat or {prefix}.sumstats.gz</li> <li>LD matrix: {prefix}.ld or {prefix}.ld.npz</li> <li>LD map: {prefix}.ldmap or {prefix}.ldmap.gz</li> </ul> <p>All files are required for proper functioning.</p>"},{"location":"API/locus/#credtools.locus.load_locus--examples","title":"Examples","text":"<p>locus = load_locus('EUR_study1', 'EUR', 'study1', 50000) print(f\"Loaded locus with {locus.n_snps} SNPs\") Loaded locus with 10000 SNPs</p> Source code in <code>credtools/locus.py</code> <pre><code>def load_locus(\n    prefix: str,\n    popu: str,\n    cohort: str,\n    sample_size: int,\n    if_intersect: bool = False,\n    **kwargs: Any,\n) -&gt; Locus:\n    \"\"\"\n    Load the input data of the fine-mapping analysis.\n\n    Parameters\n    ----------\n    prefix : str\n        Prefix of the input files.\n    popu : str\n        Population of the input data.\n    cohort : str\n        Cohort of the input data.\n    sample_size : int\n        Sample size of the input data.\n    if_intersect : bool, optional\n        Whether to intersect the input data with the LD matrix, by default False.\n    **kwargs : Any\n        Additional keyword arguments passed to loading functions.\n\n    Returns\n    -------\n    Locus\n        Locus object containing the input data.\n\n    Raises\n    ------\n    ValueError\n        If the required input files are not found.\n\n    Notes\n    -----\n    The function looks for files with the following patterns:\n\n    - Summary statistics: {prefix}.sumstat or {prefix}.sumstats.gz\n    - LD matrix: {prefix}.ld or {prefix}.ld.npz\n    - LD map: {prefix}.ldmap or {prefix}.ldmap.gz\n\n    All files are required for proper functioning.\n\n    Examples\n    --------\n    &gt;&gt;&gt; locus = load_locus('EUR_study1', 'EUR', 'study1', 50000)\n    &gt;&gt;&gt; print(f\"Loaded locus with {locus.n_snps} SNPs\")\n    Loaded locus with 10000 SNPs\n    \"\"\"\n    if os.path.exists(f\"{prefix}.sumstat\"):\n        sumstats_path = f\"{prefix}.sumstat\"\n    elif os.path.exists(f\"{prefix}.sumstats.gz\"):\n        sumstats_path = f\"{prefix}.sumstats.gz\"\n    else:\n        raise ValueError(\"Sumstats file not found.\")\n\n    sumstats = load_sumstats(sumstats_path, if_sort_alleles=True, **kwargs)\n    if os.path.exists(f\"{prefix}.ld\"):\n        ld_path = f\"{prefix}.ld\"\n    elif os.path.exists(f\"{prefix}.ld.npz\"):\n        ld_path = f\"{prefix}.ld.npz\"\n    else:\n        raise ValueError(\"LD matrix file not found.\")\n    if os.path.exists(f\"{prefix}.ldmap\"):\n        ldmap_path = f\"{prefix}.ldmap\"\n    elif os.path.exists(f\"{prefix}.ldmap.gz\"):\n        ldmap_path = f\"{prefix}.ldmap.gz\"\n    else:\n        raise ValueError(\"LD map file not found.\")\n    ld = load_ld(ld_path, ldmap_path, if_sort_alleles=True, **kwargs)\n\n    return Locus(\n        popu, cohort, sample_size, sumstats=sumstats, ld=ld, if_intersect=if_intersect\n    )\n</code></pre>"},{"location":"API/locus/#credtools.locus.load_locus_set","title":"<code>load_locus_set(locus_info, if_intersect=False, **kwargs)</code>","text":"<p>Load the input data of the fine-mapping analysis for multiple loci.</p>"},{"location":"API/locus/#credtools.locus.load_locus_set--parameters","title":"Parameters","text":"<p>locus_info : pd.DataFrame     DataFrame containing the locus information with required columns:     ['prefix', 'popu', 'cohort', 'sample_size']. if_intersect : bool, optional     Whether to intersect the input data with the LD matrix, by default False. **kwargs : Any     Additional keyword arguments passed to load_locus function.</p>"},{"location":"API/locus/#credtools.locus.load_locus_set--returns","title":"Returns","text":"<p>LocusSet     LocusSet object containing the input data.</p>"},{"location":"API/locus/#credtools.locus.load_locus_set--raises","title":"Raises","text":"<p>ValueError     If required columns are missing or if the combination of popu and cohort is not unique.</p>"},{"location":"API/locus/#credtools.locus.load_locus_set--notes","title":"Notes","text":"<p>The locus_info DataFrame must contain the following columns:</p> <ul> <li>prefix: File prefix for each locus</li> <li>popu: Population code</li> <li>cohort: Cohort name</li> <li>sample_size: Sample size for the cohort</li> </ul> <p>Each row represents one locus to be loaded.</p>"},{"location":"API/locus/#credtools.locus.load_locus_set--examples","title":"Examples","text":"<p>locus_info = pd.DataFrame({ ...     'prefix': ['EUR_study1', 'ASN_study2'], ...     'popu': ['EUR', 'ASN'], ...     'cohort': ['study1', 'study2'], ...     'sample_size': [50000, 30000] ... }) locus_set = load_locus_set(locus_info) print(f\"Loaded {locus_set.n_loci} loci\") Loaded 2 loci</p> Source code in <code>credtools/locus.py</code> <pre><code>def load_locus_set(\n    locus_info: pd.DataFrame, if_intersect: bool = False, **kwargs: Any\n) -&gt; LocusSet:\n    \"\"\"\n    Load the input data of the fine-mapping analysis for multiple loci.\n\n    Parameters\n    ----------\n    locus_info : pd.DataFrame\n        DataFrame containing the locus information with required columns:\n        ['prefix', 'popu', 'cohort', 'sample_size'].\n    if_intersect : bool, optional\n        Whether to intersect the input data with the LD matrix, by default False.\n    **kwargs : Any\n        Additional keyword arguments passed to load_locus function.\n\n    Returns\n    -------\n    LocusSet\n        LocusSet object containing the input data.\n\n    Raises\n    ------\n    ValueError\n        If required columns are missing or if the combination of popu and cohort is not unique.\n\n    Notes\n    -----\n    The locus_info DataFrame must contain the following columns:\n\n    - prefix: File prefix for each locus\n    - popu: Population code\n    - cohort: Cohort name\n    - sample_size: Sample size for the cohort\n\n    Each row represents one locus to be loaded.\n\n    Examples\n    --------\n    &gt;&gt;&gt; locus_info = pd.DataFrame({\n    ...     'prefix': ['EUR_study1', 'ASN_study2'],\n    ...     'popu': ['EUR', 'ASN'],\n    ...     'cohort': ['study1', 'study2'],\n    ...     'sample_size': [50000, 30000]\n    ... })\n    &gt;&gt;&gt; locus_set = load_locus_set(locus_info)\n    &gt;&gt;&gt; print(f\"Loaded {locus_set.n_loci} loci\")\n    Loaded 2 loci\n    \"\"\"\n    required_cols = [\"prefix\", \"popu\", \"cohort\", \"sample_size\"]\n    missing_cols = [col for col in required_cols if col not in locus_info.columns]\n    if len(missing_cols) &gt; 0:\n        raise ValueError(f\"The following columns are required: {missing_cols}\")\n    if locus_info.duplicated(subset=[\"popu\", \"cohort\"]).any():\n        raise ValueError(\"The combination of popu and cohort is not unique.\")\n    loci = []\n    for i, row in locus_info.iterrows():\n        loci.append(\n            load_locus(\n                row[\"prefix\"],\n                row[\"popu\"],\n                row[\"cohort\"],\n                row[\"sample_size\"],\n                if_intersect,\n                **kwargs,\n            )\n        )\n    return LocusSet(loci)\n</code></pre>"},{"location":"API/meta/","title":"meta","text":"<p>Meta analysis of multi-ancestry gwas data.</p>"},{"location":"API/meta/#credtools.meta.meta","title":"<code>meta(inputs, meta_method='meta_all')</code>","text":"<p>Perform meta-analysis using the specified method.</p>"},{"location":"API/meta/#credtools.meta.meta--parameters","title":"Parameters","text":"<p>inputs : LocusSet     LocusSet containing input data from multiple studies. meta_method : str, optional     Meta-analysis method to use, by default \"meta_all\".     Options:     - \"meta_all\": Meta-analyze all studies together     - \"meta_by_population\": Meta-analyze within each population separately     - \"no_meta\": No meta-analysis, just intersect individual studies</p>"},{"location":"API/meta/#credtools.meta.meta--returns","title":"Returns","text":"<p>LocusSet     LocusSet containing meta-analyzed results.</p>"},{"location":"API/meta/#credtools.meta.meta--raises","title":"Raises","text":"<p>ValueError     If an unsupported meta-analysis method is specified.</p>"},{"location":"API/meta/#credtools.meta.meta--notes","title":"Notes","text":"<p>The different methods serve different purposes:</p> <ul> <li>\"meta_all\": Maximizes power by combining all studies, but may be inappropriate   if LD patterns differ substantially between populations</li> <li>\"meta_by_population\": Preserves population-specific LD while allowing   meta-analysis within populations</li> <li>\"no_meta\": Keeps studies separate, useful for comparison or when   meta-analysis is not appropriate</li> </ul> Source code in <code>credtools/meta.py</code> <pre><code>def meta(inputs: LocusSet, meta_method: str = \"meta_all\") -&gt; LocusSet:\n    \"\"\"\n    Perform meta-analysis using the specified method.\n\n    Parameters\n    ----------\n    inputs : LocusSet\n        LocusSet containing input data from multiple studies.\n    meta_method : str, optional\n        Meta-analysis method to use, by default \"meta_all\".\n        Options:\n        - \"meta_all\": Meta-analyze all studies together\n        - \"meta_by_population\": Meta-analyze within each population separately\n        - \"no_meta\": No meta-analysis, just intersect individual studies\n\n    Returns\n    -------\n    LocusSet\n        LocusSet containing meta-analyzed results.\n\n    Raises\n    ------\n    ValueError\n        If an unsupported meta-analysis method is specified.\n\n    Notes\n    -----\n    The different methods serve different purposes:\n\n    - \"meta_all\": Maximizes power by combining all studies, but may be inappropriate\n      if LD patterns differ substantially between populations\n    - \"meta_by_population\": Preserves population-specific LD while allowing\n      meta-analysis within populations\n    - \"no_meta\": Keeps studies separate, useful for comparison or when\n      meta-analysis is not appropriate\n    \"\"\"\n    if meta_method == \"meta_all\":\n        return LocusSet([meta_all(inputs)])\n    elif meta_method == \"meta_by_population\":\n        res = meta_by_population(inputs)\n        return LocusSet([res[popu] for popu in res])\n    elif meta_method == \"no_meta\":\n        return LocusSet([intersect_sumstat_ld(i) for i in inputs.loci])\n    else:\n        raise ValueError(f\"Unsupported meta-analysis method: {meta_method}\")\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_all","title":"<code>meta_all(inputs)</code>","text":"<p>Perform comprehensive meta-analysis of both summary statistics and LD matrices.</p>"},{"location":"API/meta/#credtools.meta.meta_all--parameters","title":"Parameters","text":"<p>inputs : LocusSet     LocusSet containing input data from multiple studies.</p>"},{"location":"API/meta/#credtools.meta.meta_all--returns","title":"Returns","text":"<p>Locus     Meta-analyzed Locus object with combined population and cohort identifiers.</p>"},{"location":"API/meta/#credtools.meta.meta_all--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Performs meta-analysis of summary statistics using inverse-variance weighting</li> <li>Performs meta-analysis of LD matrices using sample-size weighting</li> <li>Combines population and cohort names from all input studies</li> <li>Sums sample sizes across studies</li> <li>Intersects the meta-analyzed data to ensure consistency</li> </ol> <p>Population and cohort names are combined with \"+\" as separator and sorted alphabetically.</p> Source code in <code>credtools/meta.py</code> <pre><code>def meta_all(inputs: LocusSet) -&gt; Locus:\n    \"\"\"\n    Perform comprehensive meta-analysis of both summary statistics and LD matrices.\n\n    Parameters\n    ----------\n    inputs : LocusSet\n        LocusSet containing input data from multiple studies.\n\n    Returns\n    -------\n    Locus\n        Meta-analyzed Locus object with combined population and cohort identifiers.\n\n    Notes\n    -----\n    This function:\n\n    1. Performs meta-analysis of summary statistics using inverse-variance weighting\n    2. Performs meta-analysis of LD matrices using sample-size weighting\n    3. Combines population and cohort names from all input studies\n    4. Sums sample sizes across studies\n    5. Intersects the meta-analyzed data to ensure consistency\n\n    Population and cohort names are combined with \"+\" as separator and sorted alphabetically.\n    \"\"\"\n    meta_sumstat = meta_sumstats(inputs)\n    meta_ld = meta_lds(inputs)\n    sample_size = sum([input.sample_size for input in inputs.loci])\n    popu_set = set()\n    for input in inputs.loci:\n        for pop in input.popu.split(\",\"):\n            popu_set.add(pop)\n    popu = \"+\".join(sorted(popu_set))\n    cohort_set = set()\n    for input in inputs.loci:\n        for cohort_name in input.cohort.split(\",\"):\n            cohort_set.add(cohort_name)\n    cohort = \"+\".join(sorted(cohort_set))\n\n    return Locus(\n        popu, cohort, sample_size, sumstats=meta_sumstat, ld=meta_ld, if_intersect=True\n    )\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_by_population","title":"<code>meta_by_population(inputs)</code>","text":"<p>Perform meta-analysis within each population separately.</p>"},{"location":"API/meta/#credtools.meta.meta_by_population--parameters","title":"Parameters","text":"<p>inputs : LocusSet     LocusSet containing input data from multiple studies.</p>"},{"location":"API/meta/#credtools.meta.meta_by_population--returns","title":"Returns","text":"<p>Dict[str, Locus]     Dictionary mapping population codes to meta-analyzed Locus objects.</p>"},{"location":"API/meta/#credtools.meta.meta_by_population--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Groups studies by population code</li> <li>Performs meta-analysis within each population group</li> <li>For single-study populations, applies intersection without meta-analysis</li> <li>Returns a dictionary with population codes as keys</li> </ol> <p>This approach preserves population-specific LD patterns while still allowing meta-analysis of multiple cohorts within the same population.</p> Source code in <code>credtools/meta.py</code> <pre><code>def meta_by_population(inputs: LocusSet) -&gt; Dict[str, Locus]:\n    \"\"\"\n    Perform meta-analysis within each population separately.\n\n    Parameters\n    ----------\n    inputs : LocusSet\n        LocusSet containing input data from multiple studies.\n\n    Returns\n    -------\n    Dict[str, Locus]\n        Dictionary mapping population codes to meta-analyzed Locus objects.\n\n    Notes\n    -----\n    This function:\n\n    1. Groups studies by population code\n    2. Performs meta-analysis within each population group\n    3. For single-study populations, applies intersection without meta-analysis\n    4. Returns a dictionary with population codes as keys\n\n    This approach preserves population-specific LD patterns while still\n    allowing meta-analysis of multiple cohorts within the same population.\n    \"\"\"\n    meta_popu = {}\n    for input in inputs.loci:\n        popu = input.popu\n        if popu not in meta_popu:\n            meta_popu[popu] = [input]\n        else:\n            meta_popu[popu].append(input)\n\n    result_dict = {}\n    for popu in meta_popu:\n        if len(meta_popu[popu]) &gt; 1:\n            result_dict[popu] = meta_all(LocusSet(meta_popu[popu]))\n        else:\n            result_dict[popu] = intersect_sumstat_ld(meta_popu[popu][0])\n    return result_dict\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_lds","title":"<code>meta_lds(inputs)</code>","text":"<p>Perform meta-analysis of LD matrices using sample-size weighted averaging.</p>"},{"location":"API/meta/#credtools.meta.meta_lds--parameters","title":"Parameters","text":"<p>inputs : LocusSet     LocusSet containing input data from multiple studies.</p>"},{"location":"API/meta/#credtools.meta.meta_lds--returns","title":"Returns","text":"<p>LDMatrix     Meta-analyzed LD matrix with merged variant map.</p>"},{"location":"API/meta/#credtools.meta.meta_lds--notes","title":"Notes","text":"<p>This function performs the following operations:</p> <ol> <li>Identifies unique variants across all studies</li> <li>Creates a master variant list sorted by chromosome and position</li> <li>Performs sample-size weighted averaging of LD correlations</li> <li>Handles missing variants by setting weights to zero</li> <li>Optionally meta-analyzes allele frequencies if available</li> </ol> <p>The meta-analysis formula: LD_meta[i,j] = \u03a3(LD_k[i,j] * N_k) / \u03a3(N_k)</p> <p>where k indexes studies, N_k is sample size, and the sum is over studies that have both variants i and j.</p> Source code in <code>credtools/meta.py</code> <pre><code>def meta_lds(inputs: LocusSet) -&gt; LDMatrix:\n    \"\"\"\n    Perform meta-analysis of LD matrices using sample-size weighted averaging.\n\n    Parameters\n    ----------\n    inputs : LocusSet\n        LocusSet containing input data from multiple studies.\n\n    Returns\n    -------\n    LDMatrix\n        Meta-analyzed LD matrix with merged variant map.\n\n    Notes\n    -----\n    This function performs the following operations:\n\n    1. Identifies unique variants across all studies\n    2. Creates a master variant list sorted by chromosome and position\n    3. Performs sample-size weighted averaging of LD correlations\n    4. Handles missing variants by setting weights to zero\n    5. Optionally meta-analyzes allele frequencies if available\n\n    The meta-analysis formula:\n    LD_meta[i,j] = \u03a3(LD_k[i,j] * N_k) / \u03a3(N_k)\n\n    where k indexes studies, N_k is sample size, and the sum is over studies\n    that have both variants i and j.\n    \"\"\"\n    # Get unique variants across all studies\n    variant_dfs = [input.ld.map for input in inputs.loci]\n    ld_matrices = [input.ld.r for input in inputs.loci]\n    sample_sizes = [input.sample_size for input in inputs.loci]\n\n    # Concatenate all variants\n    merged_variants = pd.concat(variant_dfs, ignore_index=True)\n    merged_variants.drop_duplicates(subset=[ColName.SNPID], inplace=True)\n    merged_variants.sort_values([ColName.CHR, ColName.BP], inplace=True)\n    merged_variants.reset_index(drop=True, inplace=True)\n    # meta allele frequency of LD reference, if exists\n    if all(\"AF2\" in variant_df.columns for variant_df in variant_dfs):\n        n_sum = sum([input.sample_size for input in inputs.loci])\n        weights = [input.sample_size / n_sum for input in inputs.loci]\n        af_df = merged_variants[[ColName.SNPID]].copy()\n        af_df.set_index(ColName.SNPID, inplace=True)\n        for i, variant_df in enumerate(variant_dfs):\n            df = variant_df.copy()\n            df.set_index(ColName.SNPID, inplace=True)\n            af_df[f\"AF2_{i}\"] = df[\"AF2\"] * weights[i]\n        af_df.fillna(0, inplace=True)\n        af_df[\"AF2_meta\"] = af_df.sum(axis=1)\n        merged_variants[\"AF2\"] = merged_variants[ColName.SNPID].map(af_df[\"AF2_meta\"])\n    all_variants = merged_variants[ColName.SNPID].values\n    variant_to_index = {snp: idx for idx, snp in enumerate(all_variants)}\n    n_variants = len(all_variants)\n\n    # Initialize arrays using numpy operations\n    merged_ld = np.zeros((n_variants, n_variants))\n    weight_matrix = np.zeros((n_variants, n_variants))\n\n    # Process each study\n    for ld_mat, variants_df, sample_size in zip(ld_matrices, variant_dfs, sample_sizes):\n        # coverte float16 to float32, to avoid overflow\n        # ld_mat = ld_mat.astype(np.float32)\n\n        # Get indices in the master matrix\n        study_snps = variants_df[\"SNPID\"].values\n        study_indices = np.array([variant_to_index[snp] for snp in study_snps])\n\n        # Create index meshgrid for faster indexing\n        idx_i, idx_j = np.meshgrid(study_indices, study_indices)\n\n        # Update matrices using vectorized operations\n        merged_ld[idx_i, idx_j] += ld_mat * sample_size\n        weight_matrix[idx_i, idx_j] += sample_size\n\n    # Compute weighted average\n    mask = weight_matrix != 0\n    merged_ld[mask] /= weight_matrix[mask]\n\n    return LDMatrix(merged_variants, merged_ld.astype(np.float32))\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_loci","title":"<code>meta_loci(inputs, outdir, threads=1, meta_method='meta_all')</code>","text":"<p>Perform meta-analysis on multiple loci in parallel.</p>"},{"location":"API/meta/#credtools.meta.meta_loci--parameters","title":"Parameters","text":"<p>inputs : str     Path to input file containing locus information.     Must be a tab-separated file with columns including 'locus_id'. outdir : str     Output directory path where results will be saved. threads : int, optional     Number of parallel threads to use, by default 1. meta_method : str, optional     Meta-analysis method to use, by default \"meta_all\".     See meta() function for available options.</p>"},{"location":"API/meta/#credtools.meta.meta_loci--returns","title":"Returns","text":"<p>None     Results are saved to files in the output directory.</p>"},{"location":"API/meta/#credtools.meta.meta_loci--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Reads locus information from the input file</li> <li>Groups loci by locus_id for parallel processing</li> <li>Processes each locus group using the specified meta-analysis method</li> <li>Saves results with a progress bar for user feedback</li> <li>Creates a summary file (loci_info.txt) with all processed loci</li> </ol> <p>The input file should contain columns: locus_id, prefix, popu, cohort, sample_size. Each locus_id can have multiple rows representing different cohorts/populations.</p> <p>Output files are organized as: {outdir}/{locus_id}/{prefix}.{sumstats.gz,ld.npz,ldmap.gz}</p> Source code in <code>credtools/meta.py</code> <pre><code>def meta_loci(\n    inputs: str,\n    outdir: str,\n    threads: int = 1,\n    meta_method: str = \"meta_all\",\n) -&gt; None:\n    \"\"\"\n    Perform meta-analysis on multiple loci in parallel.\n\n    Parameters\n    ----------\n    inputs : str\n        Path to input file containing locus information.\n        Must be a tab-separated file with columns including 'locus_id'.\n    outdir : str\n        Output directory path where results will be saved.\n    threads : int, optional\n        Number of parallel threads to use, by default 1.\n    meta_method : str, optional\n        Meta-analysis method to use, by default \"meta_all\".\n        See meta() function for available options.\n\n    Returns\n    -------\n    None\n        Results are saved to files in the output directory.\n\n    Notes\n    -----\n    This function:\n\n    1. Reads locus information from the input file\n    2. Groups loci by locus_id for parallel processing\n    3. Processes each locus group using the specified meta-analysis method\n    4. Saves results with a progress bar for user feedback\n    5. Creates a summary file (loci_info.txt) with all processed loci\n\n    The input file should contain columns: locus_id, prefix, popu, cohort, sample_size.\n    Each locus_id can have multiple rows representing different cohorts/populations.\n\n    Output files are organized as:\n    {outdir}/{locus_id}/{prefix}.{sumstats.gz,ld.npz,ldmap.gz}\n    \"\"\"\n    loci_info = pd.read_csv(inputs, sep=\"\\t\")\n    new_loci_info = pd.DataFrame(columns=loci_info.columns)\n\n    # Group loci by locus_id\n    grouped_loci = list(loci_info.groupby(\"locus_id\"))\n    total_loci = len(grouped_loci)\n    os.makedirs(outdir, exist_ok=True)\n\n    # Create process pool and process loci in parallel with progress bar\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        MofNCompleteColumn(),\n        TextColumn(\"\u2022\"),\n        TimeRemainingColumn(),\n    ) as progress:\n        task = progress.add_task(\"[cyan]Meta-analysing...\", total=total_loci)\n\n        with Pool(threads) as pool:\n            args = [\n                (locus_id, locus_info, outdir, meta_method)\n                for locus_id, locus_info in grouped_loci\n            ]\n            for result in pool.imap_unordered(meta_locus, args):\n                # Update results\n                for i, res in enumerate(result):\n                    new_loci_info.loc[len(new_loci_info)] = res\n                # Update progress\n                progress.advance(task)\n\n    new_loci_info.to_csv(f\"{outdir}/loci_info.txt\", sep=\"\\t\", index=False)\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_locus","title":"<code>meta_locus(args)</code>","text":"<p>Process a single locus for meta-analysis.</p>"},{"location":"API/meta/#credtools.meta.meta_locus--parameters","title":"Parameters","text":"<p>args : Tuple[str, pd.DataFrame, str, str]     A tuple containing:     - locus_id : str         The ID of the locus     - locus_info : pd.DataFrame         DataFrame containing locus information     - outdir : str         Output directory path     - meta_method : str         Method for meta-analysis</p>"},{"location":"API/meta/#credtools.meta.meta_locus--returns","title":"Returns","text":"<p>List[List[Any]]     A list of results containing processed locus information.     Each inner list contains: [chrom, start, end, popu, sample_size, cohort, out_prefix, locus_id]</p>"},{"location":"API/meta/#credtools.meta.meta_locus--notes","title":"Notes","text":"<p>This function is designed for parallel processing and:</p> <ol> <li>Loads the locus set from the provided information</li> <li>Performs meta-analysis using the specified method</li> <li>Creates output directory for the locus</li> <li>Saves results to compressed files (sumstats.gz, ld.npz, ldmap.gz)</li> <li>Returns metadata for each processed locus</li> </ol> Source code in <code>credtools/meta.py</code> <pre><code>def meta_locus(args: Tuple[str, pd.DataFrame, str, str]) -&gt; List[List[Any]]:\n    \"\"\"\n    Process a single locus for meta-analysis.\n\n    Parameters\n    ----------\n    args : Tuple[str, pd.DataFrame, str, str]\n        A tuple containing:\n        - locus_id : str\n            The ID of the locus\n        - locus_info : pd.DataFrame\n            DataFrame containing locus information\n        - outdir : str\n            Output directory path\n        - meta_method : str\n            Method for meta-analysis\n\n    Returns\n    -------\n    List[List[Any]]\n        A list of results containing processed locus information.\n        Each inner list contains: [chrom, start, end, popu, sample_size, cohort, out_prefix, locus_id]\n\n    Notes\n    -----\n    This function is designed for parallel processing and:\n\n    1. Loads the locus set from the provided information\n    2. Performs meta-analysis using the specified method\n    3. Creates output directory for the locus\n    4. Saves results to compressed files (sumstats.gz, ld.npz, ldmap.gz)\n    5. Returns metadata for each processed locus\n    \"\"\"\n    locus_id, locus_info, outdir, meta_method = args\n    results = []\n    locus_set = load_locus_set(locus_info)\n    locus_set = meta(locus_set, meta_method)\n    out_dir = f\"{outdir}/{locus_id}\"\n    out_dir = os.path.abspath(out_dir)\n    os.makedirs(out_dir, exist_ok=True)\n\n    for locus in locus_set.loci:\n        out_prefix = f\"{out_dir}/{locus.prefix}\"\n        locus.sumstats.to_csv(\n            f\"{out_prefix}.sumstats.gz\", sep=\"\\t\", index=False, compression=\"gzip\"\n        )\n        np.savez_compressed(f\"{out_prefix}.ld.npz\", ld=locus.ld.r.astype(np.float16))\n        locus.ld.map.to_csv(\n            f\"{out_prefix}.ldmap.gz\", sep=\"\\t\", index=False, compression=\"gzip\"\n        )\n        chrom, start, end = locus.chrom, locus.start, locus.end\n        results.append(\n            [\n                chrom,\n                start,\n                end,\n                locus.popu,\n                locus.sample_size,\n                locus.cohort,\n                out_prefix,\n                f\"chr{chrom}_{start}_{end}\",\n            ]\n        )\n    return results\n</code></pre>"},{"location":"API/meta/#credtools.meta.meta_sumstats","title":"<code>meta_sumstats(inputs)</code>","text":"<p>Perform fixed effect meta-analysis of summary statistics.</p>"},{"location":"API/meta/#credtools.meta.meta_sumstats--parameters","title":"Parameters","text":"<p>inputs : LocusSet     LocusSet containing input data from multiple studies.</p>"},{"location":"API/meta/#credtools.meta.meta_sumstats--returns","title":"Returns","text":"<p>pd.DataFrame     Meta-analysis summary statistics with columns: SNPID, BETA, SE, P, EAF, CHR, BP, EA, NEA.</p>"},{"location":"API/meta/#credtools.meta.meta_sumstats--notes","title":"Notes","text":"<p>This function performs inverse-variance weighted fixed-effects meta-analysis:</p> <ol> <li>Merges summary statistics from all studies on SNPID</li> <li>Calculates inverse-variance weights (1/SE\u00b2)</li> <li>Computes weighted average effect size</li> <li>Calculates meta-analysis standard error</li> <li>Computes Z-scores and p-values</li> <li>Performs sample-size weighted averaging of effect allele frequencies</li> </ol> <p>The meta-analysis formulas used: - Beta_meta = \u03a3(Beta_i * Weight_i) / \u03a3(Weight_i) - SE_meta = 1 / sqrt(\u03a3(Weight_i)) - Weight_i = 1 / SE_i\u00b2</p> Source code in <code>credtools/meta.py</code> <pre><code>def meta_sumstats(inputs: LocusSet) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform fixed effect meta-analysis of summary statistics.\n\n    Parameters\n    ----------\n    inputs : LocusSet\n        LocusSet containing input data from multiple studies.\n\n    Returns\n    -------\n    pd.DataFrame\n        Meta-analysis summary statistics with columns: SNPID, BETA, SE, P, EAF, CHR, BP, EA, NEA.\n\n    Notes\n    -----\n    This function performs inverse-variance weighted fixed-effects meta-analysis:\n\n    1. Merges summary statistics from all studies on SNPID\n    2. Calculates inverse-variance weights (1/SE\u00b2)\n    3. Computes weighted average effect size\n    4. Calculates meta-analysis standard error\n    5. Computes Z-scores and p-values\n    6. Performs sample-size weighted averaging of effect allele frequencies\n\n    The meta-analysis formulas used:\n    - Beta_meta = \u03a3(Beta_i * Weight_i) / \u03a3(Weight_i)\n    - SE_meta = 1 / sqrt(\u03a3(Weight_i))\n    - Weight_i = 1 / SE_i\u00b2\n    \"\"\"\n    # Merge all dataframes on SNPID\n    merged_df = inputs.loci[0].original_sumstats[[ColName.SNPID]].copy()\n    n_sum = sum([input.sample_size for input in inputs.loci])\n    eaf_weights = [input.sample_size / n_sum for input in inputs.loci]\n    for i, df in enumerate(inputs.loci):\n        df = df.sumstats[[ColName.SNPID, ColName.BETA, ColName.SE, ColName.EAF]].copy()\n        df.rename(\n            columns={\n                ColName.BETA: f\"BETA_{i}\",\n                ColName.SE: f\"SE_{i}\",\n                ColName.EAF: f\"EAF_{i}\",\n            },\n            inplace=True,\n        )\n        merged_df = pd.merge(\n            merged_df, df, on=ColName.SNPID, how=\"outer\", suffixes=(\"\", f\"_{i}\")\n        )\n\n    # Calculate weights (inverse of variance)\n    for i in range(len(inputs.loci)):\n        merged_df[f\"weight_{i}\"] = 1 / (merged_df[f\"SE_{i}\"] ** 2)\n        # merged_df[f\"EAF_{i}\"] = merged_df[f\"EAF_{i}\"] * eaf_weights[i]\n\n    merged_df.fillna(0, inplace=True)\n\n    # Calculate meta-analysis beta\n    beta_numerator = sum(\n        merged_df[f\"BETA_{i}\"] * merged_df[f\"weight_{i}\"]\n        for i in range(len(inputs.loci))\n    )\n    weight_sum = sum(merged_df[f\"weight_{i}\"] for i in range(len(inputs.loci)))\n    meta_beta = beta_numerator / weight_sum\n\n    # Calculate meta-analysis SE\n    meta_se = np.sqrt(1 / weight_sum)\n\n    # Calculate meta-analysis Z-score and p-value\n    meta_z = meta_beta / meta_se\n    meta_p = 2 * stats.norm.sf(abs(meta_z))\n\n    # Calculate meta-analysis EAF\n    meta_eaf = sum(\n        merged_df[f\"EAF_{i}\"] * eaf_weights[i] for i in range(len(inputs.loci))\n    )\n\n    # Create output dataframe\n    output_df = pd.DataFrame(\n        {\n            ColName.SNPID: merged_df[ColName.SNPID],\n            ColName.BETA: meta_beta,\n            ColName.SE: meta_se,\n            ColName.P: meta_p,\n            ColName.EAF: meta_eaf,\n        }\n    )\n    output_df[[ColName.CHR, ColName.BP, ColName.EA, ColName.NEA]] = merged_df[\n        ColName.SNPID\n    ].str.split(\"-\", expand=True)[[0, 1, 2, 3]]\n    return munge(output_df)\n</code></pre>"},{"location":"API/qc/","title":"qc","text":"<p>Quality control functions for CREDTOOLS data.</p>"},{"location":"API/qc/#credtools.qc.cochran_q","title":"<code>cochran_q(locus_set)</code>","text":"<p>Compute Cochran-Q statistic for heterogeneity testing across cohorts.</p>"},{"location":"API/qc/#credtools.qc.cochran_q--parameters","title":"Parameters","text":"<p>locus_set : LocusSet     LocusSet object containing multiple loci/cohorts.</p>"},{"location":"API/qc/#credtools.qc.cochran_q--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with SNPID as index and columns:     - Q: Cochran-Q test statistic     - Q_pvalue: p-value from chi-squared test     - I_squared: I\u00b2 heterogeneity statistic (percentage)</p>"},{"location":"API/qc/#credtools.qc.cochran_q--notes","title":"Notes","text":"<p>The Cochran-Q test assesses heterogeneity in effect sizes across studies:</p> <p>Q = \u03a3 w_i(\u03b2_i - \u03b2_pooled)\u00b2</p> <p>where: - w_i = 1/SE_i\u00b2 (inverse variance weights) - \u03b2_i = effect size in study i - \u03b2_pooled = weighted average effect size</p> <p>The I\u00b2 statistic quantifies the proportion of total variation due to heterogeneity rather than chance:</p> <p>I\u00b2 = max(0, (Q - df)/Q \u00d7 100%)</p> <p>Interpretation: - Q p-value &lt; 0.05: significant heterogeneity - I\u00b2 &gt; 50%: substantial heterogeneity - I\u00b2 &gt; 75%: considerable heterogeneity</p> <p>High heterogeneity may indicate: - Population differences - Different LD patterns - Batch effects - Population stratification</p> Source code in <code>credtools/qc.py</code> <pre><code>def cochran_q(locus_set: LocusSet) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute Cochran-Q statistic for heterogeneity testing across cohorts.\n\n    Parameters\n    ----------\n    locus_set : LocusSet\n        LocusSet object containing multiple loci/cohorts.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with SNPID as index and columns:\n        - Q: Cochran-Q test statistic\n        - Q_pvalue: p-value from chi-squared test\n        - I_squared: I\u00b2 heterogeneity statistic (percentage)\n\n    Notes\n    -----\n    The Cochran-Q test assesses heterogeneity in effect sizes across studies:\n\n    Q = \u03a3 w_i(\u03b2_i - \u03b2_pooled)\u00b2\n\n    where:\n    - w_i = 1/SE_i\u00b2 (inverse variance weights)\n    - \u03b2_i = effect size in study i\n    - \u03b2_pooled = weighted average effect size\n\n    The I\u00b2 statistic quantifies the proportion of total variation due to\n    heterogeneity rather than chance:\n\n    I\u00b2 = max(0, (Q - df)/Q \u00d7 100%)\n\n    Interpretation:\n    - Q p-value &lt; 0.05: significant heterogeneity\n    - I\u00b2 &gt; 50%: substantial heterogeneity\n    - I\u00b2 &gt; 75%: considerable heterogeneity\n\n    High heterogeneity may indicate:\n    - Population differences\n    - Different LD patterns\n    - Batch effects\n    - Population stratification\n    \"\"\"\n    merged_df = locus_set.loci[0].original_sumstats[[ColName.SNPID]].copy()\n    for i, locus_obj in enumerate(locus_set.loci):\n        locus_df = locus_obj.sumstats[\n            [ColName.SNPID, ColName.BETA, ColName.SE, ColName.EAF]\n        ].copy()\n        locus_df.rename(\n            columns={\n                ColName.BETA: f\"BETA_{i}\",\n                ColName.SE: f\"SE_{i}\",\n                ColName.EAF: f\"EAF_{i}\",\n            },\n            inplace=True,\n        )\n        merged_df = pd.merge(\n            merged_df, locus_df, on=ColName.SNPID, how=\"inner\", suffixes=(\"\", f\"_{i}\")\n        )\n\n    k: int = len(locus_set.loci)\n    weights = []\n    effects = []\n    for i in range(k):\n        weights.append((1 / (merged_df[f\"SE_{i}\"] ** 2)))\n        effects.append(merged_df[f\"BETA_{i}\"])\n\n    # Calculate weighted mean effect size\n    weighted_mean = np.sum([w * e for w, e in zip(weights, effects)], axis=0) / np.sum(\n        weights, axis=0\n    )\n\n    # Calculate Q statistic\n    Q = np.sum([w * (e - weighted_mean) ** 2 for w, e in zip(weights, effects)], axis=0)\n\n    # Calculate degrees of freedom\n    df = k - 1\n\n    # Calculate P-value\n    p_value = stats.chi2.sf(Q, df)\n\n    # Calculate I^2\n    with np.errstate(invalid=\"ignore\"):\n        I_squared = np.maximum(0, (Q - df) / Q * 100)\n\n    # Create output dataframe\n    output_df = pd.DataFrame(\n        {\n            \"SNPID\": merged_df[\"SNPID\"],\n            \"Q\": Q,\n            \"Q_pvalue\": p_value,\n            \"I_squared\": I_squared,\n        }\n    )\n    return output_df.set_index(ColName.SNPID)\n</code></pre>"},{"location":"API/qc/#credtools.qc.compare_maf","title":"<code>compare_maf(locus)</code>","text":"<p>Compare allele frequencies between summary statistics and LD reference.</p>"},{"location":"API/qc/#credtools.qc.compare_maf--parameters","title":"Parameters","text":"<p>locus : Locus     Locus object containing summary statistics and LD matrix.</p>"},{"location":"API/qc/#credtools.qc.compare_maf--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame containing the comparison results with columns:     - SNPID: SNP identifier     - MAF_sumstats: MAF from summary statistics     - MAF_ld: MAF from LD reference</p> <pre><code>Returns empty DataFrame if AF2 column is not available in LD matrix.\n</code></pre>"},{"location":"API/qc/#credtools.qc.compare_maf--warnings","title":"Warnings","text":"<p>If AF2 column is not present in the LD matrix, a warning is logged.</p>"},{"location":"API/qc/#credtools.qc.compare_maf--notes","title":"Notes","text":"<p>This function compares minor allele frequencies (MAF) between:</p> <ol> <li>Summary statistics (derived from EAF)</li> <li>LD reference panel (from AF2 column)</li> </ol> <p>Large discrepancies may indicate: - Population stratification - Allele frequency differences between studies - Potential data quality issues</p> <p>MAF is calculated as min(AF, 1-AF) for both sources.</p> Source code in <code>credtools/qc.py</code> <pre><code>def compare_maf(locus: Locus) -&gt; pd.DataFrame:\n    \"\"\"\n    Compare allele frequencies between summary statistics and LD reference.\n\n    Parameters\n    ----------\n    locus : Locus\n        Locus object containing summary statistics and LD matrix.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the comparison results with columns:\n        - SNPID: SNP identifier\n        - MAF_sumstats: MAF from summary statistics\n        - MAF_ld: MAF from LD reference\n\n        Returns empty DataFrame if AF2 column is not available in LD matrix.\n\n    Warnings\n    --------\n    If AF2 column is not present in the LD matrix, a warning is logged.\n\n    Notes\n    -----\n    This function compares minor allele frequencies (MAF) between:\n\n    1. Summary statistics (derived from EAF)\n    2. LD reference panel (from AF2 column)\n\n    Large discrepancies may indicate:\n    - Population stratification\n    - Allele frequency differences between studies\n    - Potential data quality issues\n\n    MAF is calculated as min(AF, 1-AF) for both sources.\n    \"\"\"\n    input_locus = locus.copy()\n    if \"AF2\" not in input_locus.ld.map.columns:\n        logger.warning(\"AF2 is not in the LD matrix.\")\n        return pd.DataFrame()\n    input_locus = intersect_sumstat_ld(input_locus)\n    df = input_locus.sumstats[[ColName.SNPID, ColName.MAF]].copy()\n    df.rename(columns={ColName.MAF: \"MAF_sumstats\"}, inplace=True)\n    df.set_index(ColName.SNPID, inplace=True)\n    af_ld = pd.Series(\n        index=input_locus.ld.map[ColName.SNPID].tolist(),\n        data=input_locus.ld.map[\"AF2\"].values,\n    )\n    maf_ld = np.minimum(af_ld, 1 - af_ld)\n    df[\"MAF_ld\"] = maf_ld\n    df[ColName.SNPID] = df.index\n    df.reset_index(drop=True, inplace=True)\n    return df\n</code></pre>"},{"location":"API/qc/#credtools.qc.compute_dentist_s","title":"<code>compute_dentist_s(locus)</code>","text":"<p>Compute Dentist-S statistic and p-value for outlier detection.</p>"},{"location":"API/qc/#credtools.qc.compute_dentist_s--parameters","title":"Parameters","text":"<p>locus : Locus     Locus object containing summary statistics and LD matrix.</p>"},{"location":"API/qc/#credtools.qc.compute_dentist_s--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame containing the results of the Dentist-S test with columns:     - SNPID: SNP identifier     - t_dentist_s: Dentist-S test statistic     - p_dentist_s: log p-value</p>"},{"location":"API/qc/#credtools.qc.compute_dentist_s--notes","title":"Notes","text":"<p>Reference: https://github.com/mkanai/slalom/blob/854976f8e19e6fad2db3123eb9249e07ba0e1c1b/slalom.py#L254</p> <p>The Dentist-S statistic tests for outliers by comparing each variant's z-score to what would be expected based on its LD with the lead variant:</p> <p>t_dentist_s = (z_j - r_jk * z_k)^2 / (1 - r_jk^2)</p> <p>where: - z_j: z-score for variant j - z_k: z-score for lead variant k - r_jk: LD correlation between variants j and k</p> <p>TODO: Use ABF to select lead variant, although in most cases the lead variant is the one with the smallest p-value.</p> Source code in <code>credtools/qc.py</code> <pre><code>def compute_dentist_s(locus: Locus) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute Dentist-S statistic and p-value for outlier detection.\n\n    Parameters\n    ----------\n    locus : Locus\n        Locus object containing summary statistics and LD matrix.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the results of the Dentist-S test with columns:\n        - SNPID: SNP identifier\n        - t_dentist_s: Dentist-S test statistic\n        - p_dentist_s: log p-value\n\n    Notes\n    -----\n    Reference: https://github.com/mkanai/slalom/blob/854976f8e19e6fad2db3123eb9249e07ba0e1c1b/slalom.py#L254\n\n    The Dentist-S statistic tests for outliers by comparing each variant's z-score\n    to what would be expected based on its LD with the lead variant:\n\n    t_dentist_s = (z_j - r_jk * z_k)^2 / (1 - r_jk^2)\n\n    where:\n    - z_j: z-score for variant j\n    - z_k: z-score for lead variant k\n    - r_jk: LD correlation between variants j and k\n\n    TODO: Use ABF to select lead variant, although in most cases the lead variant\n    is the one with the smallest p-value.\n    \"\"\"\n    input_locus = locus.copy()\n    input_locus = intersect_sumstat_ld(input_locus)\n    df = input_locus.sumstats.copy()\n    df[\"Z\"] = df[ColName.BETA] / df[ColName.SE]\n    lead_idx = df[ColName.P].idxmin()\n    # TODO: use abf to select lead variant, although in most cases the lead variant is the one with the smallest p-value\n    lead_z = df.loc[lead_idx, \"Z\"]\n    df[\"r\"] = input_locus.ld.r[lead_idx]\n\n    df[\"t_dentist_s\"] = (df[\"Z\"] - df[\"r\"] * lead_z) ** 2 / (1 - df[\"r\"] ** 2)  # type: ignore\n    df[\"t_dentist_s\"] = np.where(df[\"t_dentist_s\"] &lt; 0, np.inf, df[\"t_dentist_s\"])\n    df.at[lead_idx, \"t_dentist_s\"] = np.nan\n    df[\"p_dentist_s\"] = stats.chi2.logsf(df[\"t_dentist_s\"], df=1)\n\n    df = df[[ColName.SNPID, \"t_dentist_s\", \"p_dentist_s\"]].copy()\n    # df.set_index(ColName.SNPID, inplace=True)\n    # df.index.name = None\n    return df\n</code></pre>"},{"location":"API/qc/#credtools.qc.estimate_s_rss","title":"<code>estimate_s_rss(locus, r_tol=1e-08, method='null-mle', eigvens=None)</code>","text":"<p>Estimate s parameter in the susie_rss Model Using Regularized LD.</p>"},{"location":"API/qc/#credtools.qc.estimate_s_rss--parameters","title":"Parameters","text":"<p>locus : Locus     Locus object containing summary statistics and LD matrix. r_tol : float, optional     Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-8. method : str, optional     Method to estimate s, by default \"null-mle\".     Options: \"null-mle\", \"null-partialmle\", or \"null-pseudomle\". eigvens : Optional[Dict[str, np.ndarray]], optional     Pre-computed eigenvalues and eigenvectors, by default None.</p>"},{"location":"API/qc/#credtools.qc.estimate_s_rss--returns","title":"Returns","text":"<p>float     Estimated s value between 0 and 1 (or potentially &gt; 1 for \"null-partialmle\").</p>"},{"location":"API/qc/#credtools.qc.estimate_s_rss--raises","title":"Raises","text":"<p>ValueError     If n &lt;= 1 or if the method is not implemented.</p>"},{"location":"API/qc/#credtools.qc.estimate_s_rss--notes","title":"Notes","text":"<p>This function estimates the parameter s, which provides information about the consistency between z-scores and the LD matrix. A larger s indicates a strong inconsistency between z-scores and the LD matrix.</p> <p>The function implements three estimation methods:</p> <ul> <li>\"null-mle\": Maximum likelihood estimation under the null model</li> <li>\"null-partialmle\": Partial MLE using null space projection</li> <li>\"null-pseudomle\": Pseudo-likelihood estimation</li> </ul> <p>The z-scores are transformed using the formula: z_transformed = sqrt(sigma2) * z where sigma2 = (n-1) / (z^2 + n-2)</p> Source code in <code>credtools/qc.py</code> <pre><code>def estimate_s_rss(\n    locus: Locus,\n    r_tol: float = 1e-8,\n    method: str = \"null-mle\",\n    eigvens: Optional[Dict[str, np.ndarray]] = None,\n) -&gt; float:\n    \"\"\"\n    Estimate s parameter in the susie_rss Model Using Regularized LD.\n\n    Parameters\n    ----------\n    locus : Locus\n        Locus object containing summary statistics and LD matrix.\n    r_tol : float, optional\n        Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-8.\n    method : str, optional\n        Method to estimate s, by default \"null-mle\".\n        Options: \"null-mle\", \"null-partialmle\", or \"null-pseudomle\".\n    eigvens : Optional[Dict[str, np.ndarray]], optional\n        Pre-computed eigenvalues and eigenvectors, by default None.\n\n    Returns\n    -------\n    float\n        Estimated s value between 0 and 1 (or potentially &gt; 1 for \"null-partialmle\").\n\n    Raises\n    ------\n    ValueError\n        If n &lt;= 1 or if the method is not implemented.\n\n    Notes\n    -----\n    This function estimates the parameter s, which provides information about the\n    consistency between z-scores and the LD matrix. A larger s indicates a strong\n    inconsistency between z-scores and the LD matrix.\n\n    The function implements three estimation methods:\n\n    - \"null-mle\": Maximum likelihood estimation under the null model\n    - \"null-partialmle\": Partial MLE using null space projection\n    - \"null-pseudomle\": Pseudo-likelihood estimation\n\n    The z-scores are transformed using the formula:\n    z_transformed = sqrt(sigma2) * z\n    where sigma2 = (n-1) / (z^2 + n-2)\n    \"\"\"\n    # make sure the LD matrix and sumstats file are matched\n    input_locus = locus.copy()\n    input_locus = intersect_sumstat_ld(input_locus)\n    z = (\n        input_locus.sumstats[ColName.BETA] / input_locus.sumstats[ColName.SE]\n    ).to_numpy()\n    n = input_locus.sample_size\n    # Check and process input arguments z, R\n    z = np.where(np.isnan(z), 0, z)\n    if eigvens is not None:\n        eigvals = eigvens[\"eigvals\"]\n        eigvecs = eigvens[\"eigvecs\"]\n    else:\n        eigens = get_eigen(input_locus.ld.r)\n        eigvals = eigens[\"eigvals\"]\n        eigvecs = eigens[\"eigvecs\"]\n\n    # if np.any(eigvals &lt; -r_tol):\n    #     logger.warning(\"The LD matrix is not positive semidefinite. Negative eigenvalues are set to zero\")\n    eigvals[eigvals &lt; r_tol] = 0\n\n    if n &lt;= 1:\n        raise ValueError(\"n must be greater than 1\")\n\n    sigma2 = (n - 1) / (z**2 + n - 2)\n    z = np.sqrt(sigma2) * z\n\n    if method == \"null-mle\":\n\n        def negloglikelihood(s, ztv, d):\n            denom = (1 - s) * d + s\n            term1 = 0.5 * np.sum(np.log(denom))\n            term2 = 0.5 * np.sum((ztv / denom) * ztv)\n            return term1 + term2\n\n        ztv = eigvecs.T @ z\n        result = minimize_scalar(\n            negloglikelihood,\n            bounds=(0, 1),\n            method=\"bounded\",\n            args=(ztv, eigvals),\n            options={\"xatol\": np.sqrt(np.finfo(float).eps)},\n        )\n        s = result.x  # type: ignore\n\n    elif method == \"null-partialmle\":\n        colspace = np.where(eigvals &gt; 0)[0]\n        if len(colspace) == len(z):\n            s = 0\n        else:\n            znull = eigvecs[:, ~np.isin(np.arange(len(z)), colspace)].T @ z\n            s = np.sum(znull**2) / len(znull)\n\n    elif method == \"null-pseudomle\":\n\n        def pseudolikelihood(\n            s: float, z: np.ndarray, eigvals: np.ndarray, eigvecs: np.ndarray\n        ) -&gt; float:\n            precision = eigvecs @ (eigvecs.T / ((1 - s) * eigvals + s))\n            postmean = np.zeros_like(z)\n            postvar = np.zeros_like(z)\n            for i in range(len(z)):\n                postmean[i] = -(1 / precision[i, i]) * precision[i, :].dot(z) + z[i]\n                postvar[i] = 1 / precision[i, i]\n            return -np.sum(stats.norm.logpdf(z, loc=postmean, scale=np.sqrt(postvar)))\n\n        result = minimize_scalar(\n            pseudolikelihood,\n            bounds=(0, 1),\n            method=\"bounded\",\n            args=(z, eigvals, eigvecs),\n        )\n        s = result.x  # type: ignore\n\n    else:\n        raise ValueError(\"The method is not implemented\")\n\n    return s  # type: ignore\n</code></pre>"},{"location":"API/qc/#credtools.qc.get_eigen","title":"<code>get_eigen(ldmatrix)</code>","text":"<p>Compute eigenvalues and eigenvectors of LD matrix.</p>"},{"location":"API/qc/#credtools.qc.get_eigen--parameters","title":"Parameters","text":"<p>ldmatrix : np.ndarray     A p by p symmetric, positive semidefinite correlation matrix.</p>"},{"location":"API/qc/#credtools.qc.get_eigen--returns","title":"Returns","text":"<p>Dict[str, np.ndarray]     Dictionary containing eigenvalues and eigenvectors with keys:     - 'eigvals': eigenvalues array     - 'eigvecs': eigenvectors matrix</p>"},{"location":"API/qc/#credtools.qc.get_eigen--notes","title":"Notes","text":"<p>TODO: accelerate with joblib for large matrices.</p> <p>This function uses numpy.linalg.eigh which is optimized for symmetric matrices and returns eigenvalues in ascending order.</p> Source code in <code>credtools/qc.py</code> <pre><code>def get_eigen(ldmatrix: np.ndarray) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Compute eigenvalues and eigenvectors of LD matrix.\n\n    Parameters\n    ----------\n    ldmatrix : np.ndarray\n        A p by p symmetric, positive semidefinite correlation matrix.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        Dictionary containing eigenvalues and eigenvectors with keys:\n        - 'eigvals': eigenvalues array\n        - 'eigvecs': eigenvectors matrix\n\n    Notes\n    -----\n    TODO: accelerate with joblib for large matrices.\n\n    This function uses numpy.linalg.eigh which is optimized for symmetric matrices\n    and returns eigenvalues in ascending order.\n    \"\"\"\n    # ldmatrix = ldmatrix.astype(np.float32)\n    eigvals, eigvecs = np.linalg.eigh(ldmatrix)\n    return {\"eigvals\": eigvals, \"eigvecs\": eigvecs}\n</code></pre>"},{"location":"API/qc/#credtools.qc.kriging_rss","title":"<code>kriging_rss(locus, r_tol=1e-08, s=None, eigvens=None)</code>","text":"<p>Compute distribution of z-scores of variant j given other z-scores, and detect possible allele switch issues.</p>"},{"location":"API/qc/#credtools.qc.kriging_rss--parameters","title":"Parameters","text":"<p>locus : Locus     Locus object containing summary statistics and LD matrix. r_tol : float, optional     Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-8. s : Optional[float], optional     An estimated s parameter from estimate_s_rss function, by default None.     If None, s will be estimated automatically. eigvens : Optional[Dict[str, np.ndarray]], optional     Pre-computed eigenvalues and eigenvectors, by default None.</p>"},{"location":"API/qc/#credtools.qc.kriging_rss--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame containing the results of the kriging RSS test with columns:     - SNPID: SNP identifier     - z: transformed z-score     - condmean: conditional mean     - condvar: conditional variance     - z_std_diff: standardized difference     - logLR: log likelihood ratio</p>"},{"location":"API/qc/#credtools.qc.kriging_rss--raises","title":"Raises","text":"<p>ValueError     If n &lt;= 1.</p>"},{"location":"API/qc/#credtools.qc.kriging_rss--notes","title":"Notes","text":"<p>Under the null hypothesis, the RSS model with regularized LD matrix assumes: z|R,s ~ N(0, (1-s)R + sI)</p> <p>This function uses a mixture of normals to model the conditional distribution of z_j given other z-scores. The method can help detect:</p> <ul> <li>Allele switch issues</li> <li>Outlier variants</li> <li>LD inconsistencies</li> </ul> <p>The algorithm: 1. Computes conditional means and variances for each variant 2. Fits a Gaussian mixture model to capture heterogeneity 3. Calculates likelihood ratios for allele switch detection</p> Source code in <code>credtools/qc.py</code> <pre><code>def kriging_rss(\n    locus: Locus,\n    r_tol: float = 1e-8,\n    s: Optional[float] = None,\n    eigvens: Optional[Dict[str, np.ndarray]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute distribution of z-scores of variant j given other z-scores, and detect possible allele switch issues.\n\n    Parameters\n    ----------\n    locus : Locus\n        Locus object containing summary statistics and LD matrix.\n    r_tol : float, optional\n        Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-8.\n    s : Optional[float], optional\n        An estimated s parameter from estimate_s_rss function, by default None.\n        If None, s will be estimated automatically.\n    eigvens : Optional[Dict[str, np.ndarray]], optional\n        Pre-computed eigenvalues and eigenvectors, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing the results of the kriging RSS test with columns:\n        - SNPID: SNP identifier\n        - z: transformed z-score\n        - condmean: conditional mean\n        - condvar: conditional variance\n        - z_std_diff: standardized difference\n        - logLR: log likelihood ratio\n\n    Raises\n    ------\n    ValueError\n        If n &lt;= 1.\n\n    Notes\n    -----\n    Under the null hypothesis, the RSS model with regularized LD matrix assumes:\n    z|R,s ~ N(0, (1-s)R + sI)\n\n    This function uses a mixture of normals to model the conditional distribution\n    of z_j given other z-scores. The method can help detect:\n\n    - Allele switch issues\n    - Outlier variants\n    - LD inconsistencies\n\n    The algorithm:\n    1. Computes conditional means and variances for each variant\n    2. Fits a Gaussian mixture model to capture heterogeneity\n    3. Calculates likelihood ratios for allele switch detection\n    \"\"\"\n    # Check and process input arguments z, R\n    input_locus = locus.copy()\n    input_locus = intersect_sumstat_ld(input_locus)\n    z = (\n        input_locus.sumstats[ColName.BETA] / input_locus.sumstats[ColName.SE]\n    ).to_numpy()\n    n = input_locus.sample_size\n    z = np.where(np.isnan(z), 0, z)\n\n    # Compute eigenvalues and eigenvectors\n    if eigvens is not None:\n        eigvals = eigvens[\"eigvals\"]\n        eigvecs = eigvens[\"eigvecs\"]\n    else:\n        eigens = get_eigen(input_locus.ld.r)\n        eigvals = eigens[\"eigvals\"]\n        eigvecs = eigens[\"eigvecs\"]\n    if s is None:\n        s = estimate_s_rss(locus, eigvens={\"eigvals\": eigvals, \"eigvecs\": eigvecs})\n    eigvals = eigvals[::-1]\n    eigvecs = eigvecs[:, ::-1]\n\n    eigvals[eigvals &lt; r_tol] = 0\n\n    if n &lt;= 1:\n        raise ValueError(\"n must be greater than 1\")\n\n    sigma2 = (n - 1) / (z**2 + n - 2)\n    z = np.sqrt(sigma2) * z\n\n    dinv = 1 / ((1 - s) * eigvals + s)\n    dinv[np.isinf(dinv)] = 0\n    precision = eigvecs @ (eigvecs * dinv).T\n    condmean = np.zeros_like(z)\n    condvar = np.zeros_like(z)\n    for i in range(len(z)):\n        condmean[i] = -(1 / precision[i, i]) * precision[i, :i].dot(z[:i]) - (\n            1 / precision[i, i]\n        ) * precision[i, i + 1 :].dot(z[i + 1 :])\n        condvar[i] = 1 / precision[i, i]\n    z_std_diff = (z - condmean) / np.sqrt(condvar)\n\n    # Obtain grid\n    a_min = 0.8\n    a_max = 2 if np.max(z_std_diff**2) &lt; 1 else 2 * np.sqrt(np.max(z_std_diff**2))\n    npoint = int(np.ceil(np.log2(a_max / a_min) / np.log2(1.05)))\n    # Ensure npoint doesn't exceed number of samples\n    npoint = min(npoint, len(z) - 1)\n    a_grid = 1.05 ** np.arange(-npoint, 1) * a_max\n\n    # Compute likelihood\n    sd_mtx = np.outer(np.sqrt(condvar), a_grid)\n    matrix_llik = stats.norm.logpdf(\n        z[:, np.newaxis] - condmean[:, np.newaxis], scale=sd_mtx\n    )\n    lfactors = np.max(matrix_llik, axis=1)\n    matrix_llik = matrix_llik - lfactors[:, np.newaxis]\n\n    # Estimate weight using Gaussian Mixture Model\n    gmm = GaussianMixture(\n        n_components=len(a_grid), covariance_type=\"diag\", max_iter=1000\n    )\n    gmm.fit(matrix_llik)\n    w = gmm.weights_\n\n    # Compute denominators in likelihood ratios\n    logl0mix = np.log(np.sum(np.exp(matrix_llik) * (w + 1e-15), axis=1)) + lfactors  # type: ignore\n\n    # Compute numerators in likelihood ratios\n    matrix_llik = stats.norm.logpdf(\n        z[:, np.newaxis] + condmean[:, np.newaxis], scale=sd_mtx\n    )\n    lfactors = np.max(matrix_llik, axis=1)\n    matrix_llik = matrix_llik - lfactors[:, np.newaxis]\n    logl1mix = np.log(np.sum(np.exp(matrix_llik) * (w + 1e-15), axis=1)) + lfactors  # type: ignore\n\n    # Compute (log) likelihood ratios\n    logLRmix = logl1mix - logl0mix\n\n    res = pd.DataFrame(\n        {\n            \"SNPID\": input_locus.sumstats[ColName.SNPID].to_numpy(),\n            \"z\": z,\n            \"condmean\": condmean,\n            \"condvar\": condvar,\n            \"z_std_diff\": z_std_diff,\n            \"logLR\": logLRmix,\n        },\n        # index=input_locus.sumstats[ColName.SNPID].to_numpy(),\n    )\n    # TODO: remove variants with logLR &gt; 2 and abs(z) &gt; 2\n\n    return res\n</code></pre>"},{"location":"API/qc/#credtools.qc.ld_4th_moment","title":"<code>ld_4th_moment(locus_set)</code>","text":"<p>Compute the 4th moment of the LD matrix as a measure of LD structure.</p>"},{"location":"API/qc/#credtools.qc.ld_4th_moment--parameters","title":"Parameters","text":"<p>locus_set : LocusSet     LocusSet object containing multiple loci/cohorts.</p>"},{"location":"API/qc/#credtools.qc.ld_4th_moment--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with variants as rows and cohorts as columns, containing     the 4th moment values (sum of r^4 - 1 for each variant).</p>"},{"location":"API/qc/#credtools.qc.ld_4th_moment--notes","title":"Notes","text":"<p>The 4th moment is calculated as: \u03a3(r_ij^4) - 1 for each variant i</p> <p>This metric provides information about: - LD structure complexity - Potential issues with LD matrix quality - Differences in LD patterns between populations</p> <p>Higher values may indicate: - Strong local LD structure - Potential genotyping errors - Population stratification effects</p> <p>The function intersects variants across all cohorts to ensure fair comparison.</p> Source code in <code>credtools/qc.py</code> <pre><code>def ld_4th_moment(locus_set: LocusSet) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the 4th moment of the LD matrix as a measure of LD structure.\n\n    Parameters\n    ----------\n    locus_set : LocusSet\n        LocusSet object containing multiple loci/cohorts.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with variants as rows and cohorts as columns, containing\n        the 4th moment values (sum of r^4 - 1 for each variant).\n\n    Notes\n    -----\n    The 4th moment is calculated as:\n    \u03a3(r_ij^4) - 1 for each variant i\n\n    This metric provides information about:\n    - LD structure complexity\n    - Potential issues with LD matrix quality\n    - Differences in LD patterns between populations\n\n    Higher values may indicate:\n    - Strong local LD structure\n    - Potential genotyping errors\n    - Population stratification effects\n\n    The function intersects variants across all cohorts to ensure fair comparison.\n    \"\"\"\n    ld_4th_res = []\n    # intersect between loci\n    overlap_snps = set(locus_set.loci[0].sumstats[ColName.SNPID])\n    for locus in locus_set.loci[1:]:\n        overlap_snps = overlap_snps.intersection(set(locus.sumstats[ColName.SNPID]))\n    for locus in locus_set.loci:\n        locus = locus.copy()\n        locus.sumstats = locus.sumstats[\n            locus.sumstats[ColName.SNPID].isin(overlap_snps)\n        ]\n        locus = intersect_sumstat_ld(locus)\n        r_4th = pd.Series(\n            index=locus.ld.map[ColName.SNPID], data=np.power(locus.ld.r, 4).sum(axis=0)\n        )\n        r_4th = r_4th - 1\n        r_4th.name = f\"{locus.popu}_{locus.cohort}\"\n        ld_4th_res.append(r_4th)\n    return pd.concat(ld_4th_res, axis=1)\n</code></pre>"},{"location":"API/qc/#credtools.qc.ld_decay","title":"<code>ld_decay(locus_set)</code>","text":"<p>Compute LD decay patterns across cohorts.</p>"},{"location":"API/qc/#credtools.qc.ld_decay--parameters","title":"Parameters","text":"<p>locus_set : LocusSet     LocusSet object containing multiple loci/cohorts.</p>"},{"location":"API/qc/#credtools.qc.ld_decay--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame containing LD decay information with columns:     - distance_kb: distance in kilobases     - r2_avg: average r\u00b2 value at that distance     - decay_rate: fitted exponential decay rate parameter     - cohort: cohort identifier</p>"},{"location":"API/qc/#credtools.qc.ld_decay--notes","title":"Notes","text":"<p>This function analyzes LD decay by:</p> <ol> <li>Computing pairwise distances between all variants</li> <li>Binning distances into 1kb windows</li> <li>Calculating average r\u00b2 within each distance bin</li> <li>Fitting an exponential decay model: r\u00b2 = a * exp(-b * distance)</li> </ol> <p>LD decay patterns can reveal: - Population-specific recombination patterns - Effective population size differences - Demographic history effects</p> <p>Different populations typically show different decay rates due to: - Historical effective population sizes - Admixture patterns - Founder effects</p> Source code in <code>credtools/qc.py</code> <pre><code>def ld_decay(locus_set: LocusSet) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute LD decay patterns across cohorts.\n\n    Parameters\n    ----------\n    locus_set : LocusSet\n        LocusSet object containing multiple loci/cohorts.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing LD decay information with columns:\n        - distance_kb: distance in kilobases\n        - r2_avg: average r\u00b2 value at that distance\n        - decay_rate: fitted exponential decay rate parameter\n        - cohort: cohort identifier\n\n    Notes\n    -----\n    This function analyzes LD decay by:\n\n    1. Computing pairwise distances between all variants\n    2. Binning distances into 1kb windows\n    3. Calculating average r\u00b2 within each distance bin\n    4. Fitting an exponential decay model: r\u00b2 = a * exp(-b * distance)\n\n    LD decay patterns can reveal:\n    - Population-specific recombination patterns\n    - Effective population size differences\n    - Demographic history effects\n\n    Different populations typically show different decay rates due to:\n    - Historical effective population sizes\n    - Admixture patterns\n    - Founder effects\n    \"\"\"\n\n    def fit_exp(x: np.ndarray, a: float, b: float) -&gt; np.ndarray:\n        with np.errstate(over=\"ignore\"):\n            return a * np.exp(-b * x)\n\n    binsize = 1000\n    decay_res = []\n    for locus in locus_set.loci:\n        ldmap = locus.ld.map.copy()\n        r = locus.ld.r.copy()\n        distance_mat = np.array(\n            [ldmap[\"BP\"] - ldmap[\"BP\"].values[i] for i in range(len(ldmap))]\n        )\n        distance_mat = distance_mat[np.tril_indices_from(distance_mat, k=-1)].flatten()\n        distance_mat = np.abs(distance_mat)\n        r = r[np.tril_indices_from(r, k=-1)].flatten()\n        r = np.square(r)\n        bins = np.arange(0, ldmap[\"BP\"].max() - ldmap[\"BP\"].min() + binsize, binsize)\n\n        r_sum, _ = np.histogram(distance_mat, bins=bins, weights=r)\n        count, _ = np.histogram(distance_mat, bins=bins)\n\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            r2_avg = np.where(count &gt; 0, r_sum / count, 0)\n        popt, _ = curve_fit(fit_exp, bins[1:] / binsize, r2_avg)\n        res = pd.DataFrame(\n            {\n                \"distance_kb\": bins[1:] / binsize,\n                \"r2_avg\": r2_avg,\n                \"decay_rate\": popt[0],\n                \"cohort\": f\"{locus.popu}_{locus.cohort}\",\n            }\n        )\n        decay_res.append(res)\n    return pd.concat(decay_res, axis=0)\n</code></pre>"},{"location":"API/qc/#credtools.qc.loci_qc","title":"<code>loci_qc(inputs, out_dir, threads=1)</code>","text":"<p>Perform quality control analysis on multiple loci in parallel.</p>"},{"location":"API/qc/#credtools.qc.loci_qc--parameters","title":"Parameters","text":"<p>inputs : str     Path to input file containing locus information.     Must be tab-separated with columns including 'locus_id'. out_dir : str     Output directory path where results will be saved. threads : int, optional     Number of parallel threads to use, by default 1.</p>"},{"location":"API/qc/#credtools.qc.loci_qc--returns","title":"Returns","text":"<p>None     Results are saved to files in the output directory.</p>"},{"location":"API/qc/#credtools.qc.loci_qc--raises","title":"Raises","text":"<p>ValueError     If the number of threads is less than 1.</p>"},{"location":"API/qc/#credtools.qc.loci_qc--notes","title":"Notes","text":"<p>This function processes multiple loci in parallel with the following workflow:</p> <ol> <li>Reads locus information from input file</li> <li>Groups loci by locus_id</li> <li>Processes each locus group using multiprocessing</li> <li>Displays progress bar for user feedback</li> <li>Saves results organized by locus_id</li> </ol> <p>The input file should contain columns: locus_id, prefix, popu, cohort, sample_size.</p> <p>Output structure: {out_dir}/{locus_id}/{qc_metric}.txt.gz</p> <p>Each locus gets its own subdirectory with compressed QC result files.</p> Source code in <code>credtools/qc.py</code> <pre><code>def loci_qc(inputs: str, out_dir: str, threads: int = 1) -&gt; None:\n    \"\"\"\n    Perform quality control analysis on multiple loci in parallel.\n\n    Parameters\n    ----------\n    inputs : str\n        Path to input file containing locus information.\n        Must be tab-separated with columns including 'locus_id'.\n    out_dir : str\n        Output directory path where results will be saved.\n    threads : int, optional\n        Number of parallel threads to use, by default 1.\n\n    Returns\n    -------\n    None\n        Results are saved to files in the output directory.\n\n    Raises\n    ------\n    ValueError\n        If the number of threads is less than 1.\n\n    Notes\n    -----\n    This function processes multiple loci in parallel with the following workflow:\n\n    1. Reads locus information from input file\n    2. Groups loci by locus_id\n    3. Processes each locus group using multiprocessing\n    4. Displays progress bar for user feedback\n    5. Saves results organized by locus_id\n\n    The input file should contain columns: locus_id, prefix, popu, cohort, sample_size.\n\n    Output structure:\n    {out_dir}/{locus_id}/{qc_metric}.txt.gz\n\n    Each locus gets its own subdirectory with compressed QC result files.\n    \"\"\"\n    loci_info = pd.read_csv(inputs, sep=\"\\t\")\n\n    # Create progress bar\n    progress = Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        MofNCompleteColumn(),\n        TimeRemainingColumn(),\n    )\n\n    # Prepare arguments for multiprocessing\n    locus_groups = [\n        (locus_id, locus_info, out_dir)\n        for locus_id, locus_info in loci_info.groupby(\"locus_id\")\n    ]\n\n    with progress:\n        task = progress.add_task(\"[cyan]Processing loci...\", total=len(locus_groups))\n\n        # Process loci in parallel with progress updates\n        with Pool(threads) as pool:\n            for _ in pool.imap_unordered(qc_locus_cli, locus_groups):  # type: ignore\n                progress.update(task, advance=1)\n</code></pre>"},{"location":"API/qc/#credtools.qc.locus_qc","title":"<code>locus_qc(locus_set, r_tol=0.001, method='null-mle', out_dir=None)</code>","text":"<p>Perform comprehensive quality control analysis for a locus.</p>"},{"location":"API/qc/#credtools.qc.locus_qc--parameters","title":"Parameters","text":"<p>locus_set : LocusSet     LocusSet object containing loci to analyze. r_tol : float, optional     Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-3. method : str, optional     Method to estimate s parameter, by default \"null-mle\".     Options: \"null-mle\", \"null-partialmle\", or \"null-pseudomle\". out_dir : Optional[str], optional     Output directory to save results, by default None.</p>"},{"location":"API/qc/#credtools.qc.locus_qc--returns","title":"Returns","text":"<p>Dict[str, pd.DataFrame]     Dictionary of quality control results with keys:     - 'expected_z': kriging RSS results     - 'dentist_s': Dentist-S test results     - 'compare_maf': MAF comparison results     - 'ld_4th_moment': 4th moment of LD matrix     - 'ld_decay': LD decay analysis     - 'cochran_q': heterogeneity test (if multiple cohorts)     - 'snp_missingness': missingness analysis (if multiple cohorts)</p>"},{"location":"API/qc/#credtools.qc.locus_qc--notes","title":"Notes","text":"<p>This function performs comprehensive QC including:</p> <p>Single-locus analyses: - Kriging RSS for outlier detection - Dentist-S for outlier detection - MAF comparison between sumstats and LD reference - LD matrix 4th moment analysis - LD decay pattern analysis</p> <p>Multi-locus analyses (when applicable): - Cochran-Q heterogeneity testing - SNP missingness across cohorts</p> <p>TODO: Add LAVA (Local Analysis of Variant Associations) analysis.</p> <p>If out_dir is provided, results are saved as tab-separated files.</p> Source code in <code>credtools/qc.py</code> <pre><code>def locus_qc(\n    locus_set: LocusSet,\n    r_tol: float = 1e-3,\n    method: str = \"null-mle\",\n    out_dir: Optional[str] = None,\n) -&gt; Dict[str, pd.DataFrame]:\n    \"\"\"\n    Perform comprehensive quality control analysis for a locus.\n\n    Parameters\n    ----------\n    locus_set : LocusSet\n        LocusSet object containing loci to analyze.\n    r_tol : float, optional\n        Tolerance level for eigenvalue check of positive semidefinite matrix, by default 1e-3.\n    method : str, optional\n        Method to estimate s parameter, by default \"null-mle\".\n        Options: \"null-mle\", \"null-partialmle\", or \"null-pseudomle\".\n    out_dir : Optional[str], optional\n        Output directory to save results, by default None.\n\n    Returns\n    -------\n    Dict[str, pd.DataFrame]\n        Dictionary of quality control results with keys:\n        - 'expected_z': kriging RSS results\n        - 'dentist_s': Dentist-S test results\n        - 'compare_maf': MAF comparison results\n        - 'ld_4th_moment': 4th moment of LD matrix\n        - 'ld_decay': LD decay analysis\n        - 'cochran_q': heterogeneity test (if multiple cohorts)\n        - 'snp_missingness': missingness analysis (if multiple cohorts)\n\n    Notes\n    -----\n    This function performs comprehensive QC including:\n\n    Single-locus analyses:\n    - Kriging RSS for outlier detection\n    - Dentist-S for outlier detection\n    - MAF comparison between sumstats and LD reference\n    - LD matrix 4th moment analysis\n    - LD decay pattern analysis\n\n    Multi-locus analyses (when applicable):\n    - Cochran-Q heterogeneity testing\n    - SNP missingness across cohorts\n\n    TODO: Add LAVA (Local Analysis of Variant Associations) analysis.\n\n    If out_dir is provided, results are saved as tab-separated files.\n    \"\"\"\n    qc_metrics = {}\n    all_expected_z = []\n    all_dentist_s = []\n    all_compare_maf = []\n    for locus in locus_set.loci:\n        lo = intersect_sumstat_ld(locus)\n        eigens = get_eigen(lo.ld.r)\n        lambda_s = estimate_s_rss(locus, r_tol, method, eigens)\n        expected_z = kriging_rss(locus, r_tol, lambda_s, eigens)\n        expected_z[\"lambda_s\"] = lambda_s\n        expected_z[\"cohort\"] = f\"{locus.popu}_{locus.cohort}\"\n        dentist_s = compute_dentist_s(locus)\n        dentist_s[\"cohort\"] = f\"{locus.popu}_{locus.cohort}\"\n        compare_maf_res = compare_maf(locus)\n        compare_maf_res[\"cohort\"] = f\"{locus.popu}_{locus.cohort}\"\n        all_expected_z.append(expected_z)\n        all_dentist_s.append(dentist_s)\n        all_compare_maf.append(compare_maf_res)\n    all_expected_z = pd.concat(all_expected_z, axis=0)\n    all_dentist_s = pd.concat(all_dentist_s, axis=0)\n    all_compare_maf = pd.concat(all_compare_maf, axis=0)\n    qc_metrics[\"expected_z\"] = all_expected_z\n    qc_metrics[\"dentist_s\"] = all_dentist_s\n    qc_metrics[\"compare_maf\"] = all_compare_maf\n\n    qc_metrics[\"ld_4th_moment\"] = ld_4th_moment(locus_set)\n    qc_metrics[\"ld_decay\"] = ld_decay(locus_set)\n\n    if len(locus_set.loci) &gt; 1:\n        qc_metrics[\"cochran_q\"] = cochran_q(locus_set)\n        qc_metrics[\"snp_missingness\"] = snp_missingness(locus_set)\n\n    if out_dir is not None:\n        os.makedirs(out_dir, exist_ok=True)\n        for metric_name, metric_data in qc_metrics.items():\n            metric_data.to_csv(f\"{out_dir}/{metric_name}.txt\", sep=\"\\t\", index=False)\n\n    return qc_metrics\n</code></pre>"},{"location":"API/qc/#credtools.qc.qc_locus_cli","title":"<code>qc_locus_cli(args)</code>","text":"<p>Quality control for a single locus (command-line interface wrapper).</p>"},{"location":"API/qc/#credtools.qc.qc_locus_cli--parameters","title":"Parameters","text":"<p>args : Tuple[str, pd.DataFrame, str]     Tuple containing:     - locus_id : str         Locus identifier     - locus_info : pd.DataFrame         DataFrame with locus information     - base_out_dir : str         Base output directory</p>"},{"location":"API/qc/#credtools.qc.qc_locus_cli--returns","title":"Returns","text":"<p>str     The locus_id that was processed.</p>"},{"location":"API/qc/#credtools.qc.qc_locus_cli--notes","title":"Notes","text":"<p>This function is designed for multiprocessing and:</p> <ol> <li>Loads the locus set from the provided information</li> <li>Performs comprehensive QC analysis</li> <li>Creates locus-specific output directory</li> <li>Saves all QC results as compressed files</li> <li>Returns the processed locus_id for tracking</li> </ol> <p>Output files are saved as: {base_out_dir}/{locus_id}/{qc_metric}.txt.gz</p> Source code in <code>credtools/qc.py</code> <pre><code>def qc_locus_cli(args: Tuple[str, pd.DataFrame, str]) -&gt; str:\n    \"\"\"\n    Quality control for a single locus (command-line interface wrapper).\n\n    Parameters\n    ----------\n    args : Tuple[str, pd.DataFrame, str]\n        Tuple containing:\n        - locus_id : str\n            Locus identifier\n        - locus_info : pd.DataFrame\n            DataFrame with locus information\n        - base_out_dir : str\n            Base output directory\n\n    Returns\n    -------\n    str\n        The locus_id that was processed.\n\n    Notes\n    -----\n    This function is designed for multiprocessing and:\n\n    1. Loads the locus set from the provided information\n    2. Performs comprehensive QC analysis\n    3. Creates locus-specific output directory\n    4. Saves all QC results as compressed files\n    5. Returns the processed locus_id for tracking\n\n    Output files are saved as:\n    {base_out_dir}/{locus_id}/{qc_metric}.txt.gz\n    \"\"\"\n    locus_id, locus_info, base_out_dir = args\n    locus_set = load_locus_set(locus_info)\n    qc_metrics = locus_qc(locus_set)\n    locus_out_dir = f\"{base_out_dir}/{locus_id}\"\n    os.makedirs(locus_out_dir, exist_ok=True)\n    for metric_name, metric_data in qc_metrics.items():\n        metric_data.to_csv(\n            f\"{locus_out_dir}/{metric_name}.txt.gz\",\n            sep=\"\\t\",\n            index=False,\n            compression=\"gzip\",\n        )\n    return locus_id\n</code></pre>"},{"location":"API/qc/#credtools.qc.snp_missingness","title":"<code>snp_missingness(locus_set)</code>","text":"<p>Compute the missingness rate of each cohort across variants.</p>"},{"location":"API/qc/#credtools.qc.snp_missingness--parameters","title":"Parameters","text":"<p>locus_set : LocusSet     LocusSet object containing multiple loci/cohorts.</p>"},{"location":"API/qc/#credtools.qc.snp_missingness--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with variants as rows and cohorts as columns, where 1 indicates     presence and 0 indicates absence of the variant in that cohort.</p>"},{"location":"API/qc/#credtools.qc.snp_missingness--warnings","title":"Warnings","text":"<p>If any cohort has a missing rate &gt; 0.1, a warning is logged.</p>"},{"location":"API/qc/#credtools.qc.snp_missingness--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Identifies all unique variants across cohorts</li> <li>Creates a binary matrix indicating variant presence/absence</li> <li>Calculates and logs missing rates for each cohort</li> <li>Issues warnings for cohorts with high missing rates (&gt;10%)</li> </ol> <p>High missing rates may indicate: - Different genotyping platforms - Different quality control criteria - Population-specific variants</p> Source code in <code>credtools/qc.py</code> <pre><code>def snp_missingness(locus_set: LocusSet) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the missingness rate of each cohort across variants.\n\n    Parameters\n    ----------\n    locus_set : LocusSet\n        LocusSet object containing multiple loci/cohorts.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with variants as rows and cohorts as columns, where 1 indicates\n        presence and 0 indicates absence of the variant in that cohort.\n\n    Warnings\n    --------\n    If any cohort has a missing rate &gt; 0.1, a warning is logged.\n\n    Notes\n    -----\n    This function:\n\n    1. Identifies all unique variants across cohorts\n    2. Creates a binary matrix indicating variant presence/absence\n    3. Calculates and logs missing rates for each cohort\n    4. Issues warnings for cohorts with high missing rates (&gt;10%)\n\n    High missing rates may indicate:\n    - Different genotyping platforms\n    - Different quality control criteria\n    - Population-specific variants\n    \"\"\"\n    missingness_df = []\n    for locus in locus_set.loci:\n        loc = intersect_sumstat_ld(locus)\n        loc = loc.sumstats[[ColName.SNPID]].copy()\n        loc[f\"{locus.popu}_{locus.cohort}\"] = 1\n        loc.set_index(ColName.SNPID, inplace=True)\n        missingness_df.append(loc)\n    missingness_df = pd.concat(missingness_df, axis=1)\n    missingness_df.fillna(0, inplace=True)\n    # log warning if missing rate &gt; 0.1\n    for col in missingness_df.columns:\n        missing_rate = float(\n            round(1 - missingness_df[col].sum() / missingness_df.shape[0], 3)\n        )\n        if missing_rate &gt; 0.1:\n            logger.warning(f\"The missing rate of {col} is {missing_rate}\")\n        else:\n            logger.info(f\"The missing rate of {col} is {missing_rate}\")\n\n    return missingness_df\n</code></pre>"},{"location":"API/sumstats/","title":"sumstats","text":"<p>Functions for processing summary statistics data.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_colnames","title":"<code>check_colnames(df)</code>","text":"<p>Check column names in the DataFrame and fill missing columns with None.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_colnames--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame to check for column names.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_colnames--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with all required columns, filling missing ones with None.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_colnames--notes","title":"Notes","text":"<p>This function ensures that all required summary statistics columns are present in the DataFrame. Missing columns are added with None values.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def check_colnames(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Check column names in the DataFrame and fill missing columns with None.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame to check for column names.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with all required columns, filling missing ones with None.\n\n    Notes\n    -----\n    This function ensures that all required summary statistics columns are present\n    in the DataFrame. Missing columns are added with None values.\n    \"\"\"\n    outdf: pd.DataFrame = df.copy()\n    for col in ColName.sumstat_cols:\n        if col not in outdf.columns:\n            outdf[col] = None\n    return outdf[ColName.sumstat_cols]\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.check_mandatory_cols","title":"<code>check_mandatory_cols(df)</code>","text":"<p>Check if the DataFrame contains all mandatory columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_mandatory_cols--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame to check for mandatory columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_mandatory_cols--returns","title":"Returns","text":"<p>None</p>"},{"location":"API/sumstats/#credtools.sumstats.check_mandatory_cols--raises","title":"Raises","text":"<p>ValueError     If any mandatory columns are missing.</p>"},{"location":"API/sumstats/#credtools.sumstats.check_mandatory_cols--notes","title":"Notes","text":"<p>Mandatory columns are defined in ColName.mandatory_cols and typically include essential fields like chromosome, position, alleles, effect size, and p-value.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def check_mandatory_cols(df: pd.DataFrame) -&gt; None:\n    \"\"\"\n    Check if the DataFrame contains all mandatory columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame to check for mandatory columns.\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        If any mandatory columns are missing.\n\n    Notes\n    -----\n    Mandatory columns are defined in ColName.mandatory_cols and typically include\n    essential fields like chromosome, position, alleles, effect size, and p-value.\n    \"\"\"\n    outdf = df.copy()\n    missing_cols = set(ColName.mandatory_cols) - set(outdf.columns)\n    if missing_cols:\n        raise ValueError(f\"Missing mandatory columns: {missing_cols}\")\n    return None\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps","title":"<code>get_significant_snps(df, pvalue_threshold=5e-08, use_most_sig_if_no_sig=True)</code>","text":"<p>Retrieve significant SNPs from the input DataFrame based on a p-value threshold.</p>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input summary statistics containing SNP information. pvalue_threshold : float, optional     The p-value threshold for significance, by default 5e-8. use_most_sig_if_no_sig : bool, optional     Whether to return the most significant SNP if no SNP meets the threshold, by default True.</p>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame containing significant SNPs, sorted by p-value in ascending order.</p>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps--raises","title":"Raises","text":"<p>ValueError     If no significant SNPs are found and <code>use_most_sig_if_no_sig</code> is False,     or if the DataFrame is empty. KeyError     If required columns are not present in the input DataFrame.</p>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps--notes","title":"Notes","text":"<p>If no SNPs meet the significance threshold and <code>use_most_sig_if_no_sig</code> is True, the function returns the SNP with the smallest p-value.</p>"},{"location":"API/sumstats/#credtools.sumstats.get_significant_snps--examples","title":"Examples","text":"<p>data = { ...     'SNPID': ['rs1', 'rs2', 'rs3'], ...     'P': [1e-9, 0.05, 1e-8] ... } df = pd.DataFrame(data) significant_snps = get_significant_snps(df, pvalue_threshold=5e-8) print(significant_snps)     SNPID         P 0    rs1  1.000000e-09 2    rs3  1.000000e-08</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def get_significant_snps(\n    df: pd.DataFrame,\n    pvalue_threshold: float = 5e-8,\n    use_most_sig_if_no_sig: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Retrieve significant SNPs from the input DataFrame based on a p-value threshold.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input summary statistics containing SNP information.\n    pvalue_threshold : float, optional\n        The p-value threshold for significance, by default 5e-8.\n    use_most_sig_if_no_sig : bool, optional\n        Whether to return the most significant SNP if no SNP meets the threshold, by default True.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing significant SNPs, sorted by p-value in ascending order.\n\n    Raises\n    ------\n    ValueError\n        If no significant SNPs are found and `use_most_sig_if_no_sig` is False,\n        or if the DataFrame is empty.\n    KeyError\n        If required columns are not present in the input DataFrame.\n\n    Notes\n    -----\n    If no SNPs meet the significance threshold and `use_most_sig_if_no_sig` is True,\n    the function returns the SNP with the smallest p-value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = {\n    ...     'SNPID': ['rs1', 'rs2', 'rs3'],\n    ...     'P': [1e-9, 0.05, 1e-8]\n    ... }\n    &gt;&gt;&gt; df = pd.DataFrame(data)\n    &gt;&gt;&gt; significant_snps = get_significant_snps(df, pvalue_threshold=5e-8)\n    &gt;&gt;&gt; print(significant_snps)\n        SNPID         P\n    0    rs1  1.000000e-09\n    2    rs3  1.000000e-08\n    \"\"\"\n    required_columns = {ColName.P, ColName.SNPID}\n    missing_columns = required_columns - set(df.columns)\n    if missing_columns:\n        raise KeyError(\n            f\"The following required columns are missing from the DataFrame: {missing_columns}\"\n        )\n\n    sig_df = df.loc[df[ColName.P] &lt;= pvalue_threshold].copy()\n\n    if sig_df.empty:\n        if use_most_sig_if_no_sig:\n            min_pvalue = df[ColName.P].min()\n            sig_df = df.loc[df[ColName.P] == min_pvalue].copy()\n            if sig_df.empty:\n                raise ValueError(\"The DataFrame is empty. No SNPs available to select.\")\n            logging.debug(\n                f\"Using the most significant SNP: {sig_df.iloc[0][ColName.SNPID]}\"\n            )\n            logging.debug(f\"p-value: {sig_df.iloc[0][ColName.P]}\")\n        else:\n            raise ValueError(\"No significant SNPs found.\")\n    else:\n        sig_df.sort_values(by=ColName.P, inplace=True)\n        sig_df.reset_index(drop=True, inplace=True)\n\n    return sig_df\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats","title":"<code>load_sumstats(filename, if_sort_alleles=True, sep=None, nrows=None, skiprows=0, comment=None, gzipped=None)</code>","text":"<p>Load summary statistics from a file.</p>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--parameters","title":"Parameters","text":"<p>filename : str     The path to the file containing the summary statistics.     The header must contain the column names: CHR, BP, EA, NEA, EAF, BETA, SE, P. if_sort_alleles : bool, optional     Whether to sort alleles in alphabetical order, by default True. sep : Optional[str], optional     The delimiter to use. If None, the delimiter is inferred from the file, by default None. nrows : Optional[int], optional     Number of rows to read. If None, all rows are read, by default None. skiprows : int, optional     Number of lines to skip at the start of the file, by default 0. comment : Optional[str], optional     Character to split comments in the file, by default None. gzipped : Optional[bool], optional     Whether the file is gzipped. If None, it is inferred from the file extension, by default None.</p>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame containing the loaded summary statistics.</p>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--notes","title":"Notes","text":"<p>The function performs the following operations:</p> <ol> <li>Auto-detects file compression (gzip) from file extension</li> <li>Auto-detects delimiter (tab, comma, or space) from file content</li> <li>Loads the data using pandas.read_csv</li> <li>Applies comprehensive data munging and quality control</li> <li>Optionally sorts alleles for consistency</li> </ol> <p>The function infers the delimiter if not provided and handles gzipped files automatically. Comprehensive quality control is applied including validation of chromosomes, positions, alleles, p-values, effect sizes, and frequencies.</p>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--examples","title":"Examples","text":"Source code in <code>credtools/sumstats.py</code> <pre><code>def load_sumstats(\n    filename: str,\n    if_sort_alleles: bool = True,\n    sep: Optional[str] = None,\n    nrows: Optional[int] = None,\n    skiprows: int = 0,\n    comment: Optional[str] = None,\n    gzipped: Optional[bool] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Load summary statistics from a file.\n\n    Parameters\n    ----------\n    filename : str\n        The path to the file containing the summary statistics.\n        The header must contain the column names: CHR, BP, EA, NEA, EAF, BETA, SE, P.\n    if_sort_alleles : bool, optional\n        Whether to sort alleles in alphabetical order, by default True.\n    sep : Optional[str], optional\n        The delimiter to use. If None, the delimiter is inferred from the file, by default None.\n    nrows : Optional[int], optional\n        Number of rows to read. If None, all rows are read, by default None.\n    skiprows : int, optional\n        Number of lines to skip at the start of the file, by default 0.\n    comment : Optional[str], optional\n        Character to split comments in the file, by default None.\n    gzipped : Optional[bool], optional\n        Whether the file is gzipped. If None, it is inferred from the file extension, by default None.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame containing the loaded summary statistics.\n\n    Notes\n    -----\n    The function performs the following operations:\n\n    1. Auto-detects file compression (gzip) from file extension\n    2. Auto-detects delimiter (tab, comma, or space) from file content\n    3. Loads the data using pandas.read_csv\n    4. Applies comprehensive data munging and quality control\n    5. Optionally sorts alleles for consistency\n\n    The function infers the delimiter if not provided and handles gzipped files automatically.\n    Comprehensive quality control is applied including validation of chromosomes, positions,\n    alleles, p-values, effect sizes, and frequencies.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Load summary statistics with automatic format detection\n    &gt;&gt;&gt; sumstats = load_sumstats('gwas_results.txt.gz')\n    &gt;&gt;&gt; print(f\"Loaded {len(sumstats)} variants\")\n    Loaded 1000000 variants\n\n    &gt;&gt;&gt; # Load with specific parameters\n    &gt;&gt;&gt; sumstats = load_sumstats('gwas_results.csv', sep=',', nrows=10000)\n    &gt;&gt;&gt; print(sumstats.columns.tolist())\n    ['SNPID', 'CHR', 'BP', 'EA', 'NEA', 'EAF', 'BETA', 'SE', 'P', 'MAF', 'RSID']\n    \"\"\"\n    # determine whether the file is gzipped\n    if gzipped is None:\n        gzipped = filename.endswith(\"gz\")\n\n    # read the first line of the file to determine the separator\n    if sep is None:\n        if gzipped:\n            f = gzip.open(filename, \"rt\")\n\n        else:\n            f = open(filename, \"rt\")\n        if skiprows &gt; 0:\n            for _ in range(skiprows):\n                f.readline()\n        line = f.readline()\n        f.close()\n        if \"\\t\" in line:\n            sep = \"\\t\"\n        elif \",\" in line:\n            sep = \",\"\n        else:\n            sep = \" \"\n    logger.debug(f\"File {filename} is gzipped: {gzipped}\")\n    logger.debug(f\"Separator is {sep}\")\n    logger.debug(f\"loading data from {filename}\")\n    # determine the separator, automatically if not specified\n    sumstats = pd.read_csv(\n        filename,\n        sep=sep,\n        nrows=nrows,\n        skiprows=skiprows,\n        comment=comment,\n        compression=\"gzip\" if gzipped else None,\n    )\n    sumstats = munge(sumstats)\n    logger.info(f\"Loaded {len(sumstats)} rows sumstats from {filename}\")\n    if if_sort_alleles:\n        sumstats = sort_alleles(sumstats)\n    return sumstats\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--load-summary-statistics-with-automatic-format-detection","title":"Load summary statistics with automatic format detection","text":"<p>sumstats = load_sumstats('gwas_results.txt.gz') print(f\"Loaded {len(sumstats)} variants\") Loaded 1000000 variants</p>"},{"location":"API/sumstats/#credtools.sumstats.load_sumstats--load-with-specific-parameters","title":"Load with specific parameters","text":"<p>sumstats = load_sumstats('gwas_results.csv', sep=',', nrows=10000) print(sumstats.columns.tolist()) ['SNPID', 'CHR', 'BP', 'EA', 'NEA', 'EAF', 'BETA', 'SE', 'P', 'MAF', 'RSID']</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique","title":"<code>make_SNPID_unique(sumstat, remove_duplicates=True, col_chr=ColName.CHR, col_bp=ColName.BP, col_ea=ColName.EA, col_nea=ColName.NEA, col_p=ColName.P)</code>","text":"<p>Generate unique SNP identifiers to facilitate the combination of multiple summary statistics datasets.</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique--parameters","title":"Parameters","text":"<p>sumstat : pd.DataFrame     The input summary statistics containing SNP information. remove_duplicates : bool, optional     Whether to remove duplicated SNPs, keeping the one with the smallest p-value, by default True. col_chr : str, optional     The column name for chromosome information, by default ColName.CHR. col_bp : str, optional     The column name for base-pair position information, by default ColName.BP. col_ea : str, optional     The column name for effect allele information, by default ColName.EA. col_nea : str, optional     The column name for non-effect allele information, by default ColName.NEA. col_p : str, optional     The column name for p-value information, by default ColName.P.</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique--returns","title":"Returns","text":"<p>pd.DataFrame     The summary statistics DataFrame with unique SNPIDs, suitable for merging with other datasets.</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique--raises","title":"Raises","text":"<p>KeyError     If required columns are missing from the input DataFrame. ValueError     If the input DataFrame is empty or becomes empty after processing.</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique--notes","title":"Notes","text":"<p>This function constructs a unique SNPID by concatenating chromosome, base-pair position, and sorted alleles (EA and NEA). This unique identifier allows for efficient merging of multiple summary statistics without the need for extensive duplicate comparisons.</p> <p>The unique SNPID format: \"chr-bp-sortedEA-sortedNEA\"</p> <p>If duplicates are found and <code>remove_duplicates</code> is False, a suffix \"-N\" is added to make identifiers unique, where N is the occurrence number.</p>"},{"location":"API/sumstats/#credtools.sumstats.make_SNPID_unique--examples","title":"Examples","text":"<p>data = { ...     'CHR': ['1', '1', '2'], ...     'BP': [12345, 12345, 67890], ...     'EA': ['A', 'A', 'G'], ...     'NEA': ['G', 'G', 'A'], ...     'rsID': ['rs1', 'rs2', 'rs3'], ...     'P': [1e-5, 1e-6, 1e-7] ... } df = pd.DataFrame(data) unique_df = make_SNPID_unique(df, remove_duplicates=True) print(unique_df)     SNPID       CHR     BP EA NEA rsID         P 0  1-12345-A-G    1  12345  A   G  rs2  1.000000e-06 1  2-67890-A-G    2  67890  G   A  rs3  1.000000e-07</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def make_SNPID_unique(\n    sumstat: pd.DataFrame,\n    remove_duplicates: bool = True,\n    col_chr: str = ColName.CHR,\n    col_bp: str = ColName.BP,\n    col_ea: str = ColName.EA,\n    col_nea: str = ColName.NEA,\n    col_p: str = ColName.P,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate unique SNP identifiers to facilitate the combination of multiple summary statistics datasets.\n\n    Parameters\n    ----------\n    sumstat : pd.DataFrame\n        The input summary statistics containing SNP information.\n    remove_duplicates : bool, optional\n        Whether to remove duplicated SNPs, keeping the one with the smallest p-value, by default True.\n    col_chr : str, optional\n        The column name for chromosome information, by default ColName.CHR.\n    col_bp : str, optional\n        The column name for base-pair position information, by default ColName.BP.\n    col_ea : str, optional\n        The column name for effect allele information, by default ColName.EA.\n    col_nea : str, optional\n        The column name for non-effect allele information, by default ColName.NEA.\n    col_p : str, optional\n        The column name for p-value information, by default ColName.P.\n\n    Returns\n    -------\n    pd.DataFrame\n        The summary statistics DataFrame with unique SNPIDs, suitable for merging with other datasets.\n\n    Raises\n    ------\n    KeyError\n        If required columns are missing from the input DataFrame.\n    ValueError\n        If the input DataFrame is empty or becomes empty after processing.\n\n    Notes\n    -----\n    This function constructs a unique SNPID by concatenating chromosome, base-pair position,\n    and sorted alleles (EA and NEA). This unique identifier allows for efficient merging of\n    multiple summary statistics without the need for extensive duplicate comparisons.\n\n    The unique SNPID format: \"chr-bp-sortedEA-sortedNEA\"\n\n    If duplicates are found and `remove_duplicates` is False, a suffix \"-N\" is added to make\n    identifiers unique, where N is the occurrence number.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = {\n    ...     'CHR': ['1', '1', '2'],\n    ...     'BP': [12345, 12345, 67890],\n    ...     'EA': ['A', 'A', 'G'],\n    ...     'NEA': ['G', 'G', 'A'],\n    ...     'rsID': ['rs1', 'rs2', 'rs3'],\n    ...     'P': [1e-5, 1e-6, 1e-7]\n    ... }\n    &gt;&gt;&gt; df = pd.DataFrame(data)\n    &gt;&gt;&gt; unique_df = make_SNPID_unique(df, remove_duplicates=True)\n    &gt;&gt;&gt; print(unique_df)\n        SNPID       CHR     BP EA NEA rsID         P\n    0  1-12345-A-G    1  12345  A   G  rs2  1.000000e-06\n    1  2-67890-A-G    2  67890  G   A  rs3  1.000000e-07\n    \"\"\"\n    required_columns = {\n        col_chr,\n        col_bp,\n        col_ea,\n        col_nea,\n    }\n    missing_columns = required_columns - set(sumstat.columns)\n    if missing_columns:\n        raise KeyError(\n            f\"The following required columns are missing from the DataFrame: {missing_columns}\"\n        )\n\n    if sumstat.empty:\n        raise ValueError(\"The input DataFrame is empty.\")\n\n    df = sumstat.copy()\n\n    # Sort alleles to ensure unique representation (EA &lt;= NEA)\n    allele_df = df[[col_ea, col_nea]].apply(\n        lambda row: sorted([row[col_ea], row[col_nea]]), axis=1, result_type=\"expand\"\n    )\n    allele_df.columns = [col_ea, col_nea]\n\n    # Create unique SNPID\n    df[ColName.SNPID] = (\n        df[col_chr].astype(str)\n        + \"-\"\n        + df[col_bp].astype(str)\n        + \"-\"\n        + allele_df[col_ea]\n        + \"-\"\n        + allele_df[col_nea]\n    )\n\n    # move SNPID to the first column\n    cols = df.columns.tolist()\n    cols.insert(0, cols.pop(cols.index(ColName.SNPID)))\n    df = df[cols]\n\n    n_duplicated = df.duplicated(subset=[ColName.SNPID]).sum()\n\n    if remove_duplicates and n_duplicated &gt; 0:\n        logger.debug(f\"Number of duplicated SNPs: {n_duplicated}\")\n        if col_p in df.columns:\n            # Sort by p-value to keep the SNP with the smallest p-value\n            df.sort_values(by=col_p, inplace=True)\n        df.drop_duplicates(subset=[ColName.SNPID], keep=\"first\", inplace=True)\n        # Sort DataFrame by chromosome and base-pair position\n        df.sort_values(by=[col_chr, col_bp], inplace=True)\n        df.reset_index(drop=True, inplace=True)\n    elif n_duplicated &gt; 0 and not remove_duplicates:\n        logger.warning(\n            \"\"\"Duplicated SNPs detected. To remove duplicates, set `remove_duplicates=True`.\n            Change the Unique SNP identifier to make it unique.\"\"\"\n        )\n        # Change the Unique SNP identifier to make it unique. add a number to the end of the SNP identifier\n        #  for example, 1-12345-A-G to 1-12345-A-G-1, 1-12345-A-G-2, etc. no alteration to the original SNP identifier\n        dup_tail = \"-\" + df.groupby(ColName.SNPID).cumcount().astype(str)\n        dup_tail = dup_tail.str.replace(\"-0\", \"\")\n        df[ColName.SNPID] = df[ColName.SNPID] + dup_tail\n\n    logger.debug(\"Unique SNPIDs have been successfully created.\")\n    logger.debug(f\"Total unique SNPs: {len(df)}\")\n\n    return df\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge","title":"<code>munge(df)</code>","text":"<p>Munge the summary statistics DataFrame by performing a series of transformations.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The input DataFrame containing summary statistics.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge--returns","title":"Returns","text":"<p>pd.DataFrame     The munged DataFrame with necessary transformations applied.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge--raises","title":"Raises","text":"<p>ValueError     If any mandatory columns are missing.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge--notes","title":"Notes","text":"<p>This function performs comprehensive data cleaning and standardization:</p> <ol> <li>Validates mandatory columns are present</li> <li>Removes entirely missing columns</li> <li>Cleans chromosome and position data</li> <li>Validates and standardizes allele information</li> <li>Creates unique SNP identifiers</li> <li>Validates p-values, effect sizes, and standard errors</li> <li>Processes allele frequencies</li> <li>Handles rsID information if present</li> </ol> <p>The function applies strict quality control and may remove variants that don't meet validation criteria.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge the summary statistics DataFrame by performing a series of transformations.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The input DataFrame containing summary statistics.\n\n    Returns\n    -------\n    pd.DataFrame\n        The munged DataFrame with necessary transformations applied.\n\n    Raises\n    ------\n    ValueError\n        If any mandatory columns are missing.\n\n    Notes\n    -----\n    This function performs comprehensive data cleaning and standardization:\n\n    1. Validates mandatory columns are present\n    2. Removes entirely missing columns\n    3. Cleans chromosome and position data\n    4. Validates and standardizes allele information\n    5. Creates unique SNP identifiers\n    6. Validates p-values, effect sizes, and standard errors\n    7. Processes allele frequencies\n    8. Handles rsID information if present\n\n    The function applies strict quality control and may remove variants\n    that don't meet validation criteria.\n    \"\"\"\n    check_mandatory_cols(df)\n    outdf = df.copy()\n    outdf = rm_col_allna(outdf)\n    outdf = munge_chr(outdf)\n    outdf = munge_bp(outdf)\n    outdf = munge_allele(outdf)\n    outdf = make_SNPID_unique(outdf)\n    outdf = munge_pvalue(outdf)\n    outdf = outdf.sort_values(by=[ColName.CHR, ColName.BP])\n    outdf = munge_beta(outdf)\n    outdf = munge_se(outdf)\n    outdf = munge_eaf(outdf)\n    outdf[ColName.MAF] = outdf[ColName.EAF]\n    outdf = munge_maf(outdf)\n    if ColName.RSID in outdf.columns:\n        outdf = munge_rsid(outdf)\n    outdf = check_colnames(outdf)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_allele","title":"<code>munge_allele(df)</code>","text":"<p>Munge allele columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_allele--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with allele columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_allele--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged allele columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_allele--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Removes rows with missing allele values</li> <li>Converts alleles to uppercase</li> <li>Validates alleles contain only valid DNA bases (A, C, G, T)</li> <li>Removes variants where effect allele equals non-effect allele</li> </ol> <p>Invalid alleles and monomorphic variants are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_allele(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge allele columns.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with allele columns.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged allele columns.\n\n    Notes\n    -----\n    This function:\n\n    1. Removes rows with missing allele values\n    2. Converts alleles to uppercase\n    3. Validates alleles contain only valid DNA bases (A, C, G, T)\n    4. Removes variants where effect allele equals non-effect allele\n\n    Invalid alleles and monomorphic variants are removed and logged.\n    \"\"\"\n    outdf = df.copy()\n    for col in [ColName.EA, ColName.NEA]:\n        pre_n = outdf.shape[0]\n        outdf = outdf[outdf[col].notnull()]\n        outdf[col] = outdf[col].astype(str).str.upper()\n        outdf = outdf[outdf[col].str.match(r\"^[ACGT]+$\")]\n        after_n = outdf.shape[0]\n        logger.debug(f\"Remove {pre_n - after_n} rows because of invalid {col}.\")\n    outdf = outdf[outdf[ColName.EA] != outdf[ColName.NEA]]\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_beta","title":"<code>munge_beta(df)</code>","text":"<p>Munge beta column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_beta--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with beta column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_beta--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged beta column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_beta--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Converts beta values to numeric type</li> <li>Removes rows with missing beta values</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid beta values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_beta(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge beta column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with beta column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged beta column.\n\n    Notes\n    -----\n    This function:\n\n    1. Converts beta values to numeric type\n    2. Removes rows with missing beta values\n    3. Converts to appropriate data type\n\n    Invalid beta values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df.copy()\n    outdf[ColName.BETA] = pd.to_numeric(outdf[ColName.BETA], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.BETA].notnull()]\n    after_n = outdf.shape[0]\n    logger.debug(f\"Remove {pre_n - after_n} rows because of invalid beta.\")\n    outdf[ColName.BETA] = outdf[ColName.BETA].astype(ColType.BETA)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_bp","title":"<code>munge_bp(df)</code>","text":"<p>Munge position column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_bp--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with position column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_bp--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged position column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_bp--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Removes rows with missing position values</li> <li>Converts position to numeric type</li> <li>Validates positions are within acceptable range</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid position values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_bp(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge position column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with position column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged position column.\n\n    Notes\n    -----\n    This function:\n\n    1. Removes rows with missing position values\n    2. Converts position to numeric type\n    3. Validates positions are within acceptable range\n    4. Converts to appropriate data type\n\n    Invalid position values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df[df[ColName.BP].notnull()].copy()\n    outdf[ColName.BP] = pd.to_numeric(outdf[ColName.BP], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.BP].notnull()]\n    outdf = outdf[\n        (outdf[ColName.BP] &gt; ColRange.BP_MIN) &amp; (outdf[ColName.BP] &lt; ColRange.BP_MAX)\n    ]\n    after_n = outdf.shape[0]\n    logger.debug(f\"Remove {pre_n - after_n} rows because of invalid position.\")\n    outdf[ColName.BP] = outdf[ColName.BP].astype(ColType.BP)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_chr","title":"<code>munge_chr(df)</code>","text":"<p>Munge chromosome column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_chr--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with chromosome column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_chr--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged chromosome column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_chr--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Removes rows with missing chromosome values</li> <li>Converts chromosome to string and removes 'chr' prefix</li> <li>Converts X chromosome to numeric value (23)</li> <li>Validates chromosome values are within acceptable range</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid chromosome values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_chr(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge chromosome column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with chromosome column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged chromosome column.\n\n    Notes\n    -----\n    This function:\n\n    1. Removes rows with missing chromosome values\n    2. Converts chromosome to string and removes 'chr' prefix\n    3. Converts X chromosome to numeric value (23)\n    4. Validates chromosome values are within acceptable range\n    5. Converts to appropriate data type\n\n    Invalid chromosome values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df[df[ColName.CHR].notnull()].copy()\n    outdf[ColName.CHR] = outdf[ColName.CHR].astype(str)\n    outdf[ColName.CHR] = outdf[ColName.CHR].str.replace(\"chr\", \"\")\n    outdf[ColName.CHR] = outdf[ColName.CHR].replace([\"X\", \"x\"], 23)\n    outdf[ColName.CHR] = pd.to_numeric(outdf[ColName.CHR], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.CHR].notnull()]\n    outdf = outdf[\n        (outdf[ColName.CHR] &gt;= ColRange.CHR_MIN)\n        &amp; (outdf[ColName.CHR] &lt;= ColRange.CHR_MAX)\n    ]\n    after_n = outdf.shape[0]\n    logger.debug(f\"Remove {pre_n - after_n} rows because of invalid chromosome.\")\n    outdf[ColName.CHR] = outdf[ColName.CHR].astype(ColType.CHR)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_eaf","title":"<code>munge_eaf(df)</code>","text":"<p>Munge effect allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_eaf--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with effect allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_eaf--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged effect allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_eaf--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Converts EAF values to numeric type</li> <li>Removes rows with missing EAF values</li> <li>Validates EAF values are within range [0, 1]</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid EAF values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_eaf(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge effect allele frequency column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with effect allele frequency column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged effect allele frequency column.\n\n    Notes\n    -----\n    This function:\n\n    1. Converts EAF values to numeric type\n    2. Removes rows with missing EAF values\n    3. Validates EAF values are within range [0, 1]\n    4. Converts to appropriate data type\n\n    Invalid EAF values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df.copy()\n    outdf[ColName.EAF] = pd.to_numeric(outdf[ColName.EAF], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.EAF].notnull()]\n    outdf = outdf[\n        (outdf[ColName.EAF] &gt;= ColRange.EAF_MIN)\n        &amp; (outdf[ColName.EAF] &lt;= ColRange.EAF_MAX)\n    ]\n    after_n = outdf.shape[0]\n    logger.debug(\n        f\"Remove {pre_n - after_n} rows because of invalid effect allele frequency.\"\n    )\n    outdf[ColName.EAF] = outdf[ColName.EAF].astype(ColType.EAF)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_maf","title":"<code>munge_maf(df)</code>","text":"<p>Munge minor allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_maf--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with minor allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_maf--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged minor allele frequency column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_maf--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Converts MAF values to numeric type</li> <li>Removes rows with missing MAF values</li> <li>Converts frequencies &gt; 0.5 to 1 - frequency (to ensure minor allele)</li> <li>Validates MAF values are within acceptable range</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid MAF values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_maf(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge minor allele frequency column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with minor allele frequency column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged minor allele frequency column.\n\n    Notes\n    -----\n    This function:\n\n    1. Converts MAF values to numeric type\n    2. Removes rows with missing MAF values\n    3. Converts frequencies &gt; 0.5 to 1 - frequency (to ensure minor allele)\n    4. Validates MAF values are within acceptable range\n    5. Converts to appropriate data type\n\n    Invalid MAF values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df.copy()\n    outdf[ColName.MAF] = pd.to_numeric(outdf[ColName.MAF], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.MAF].notnull()]\n    outdf[ColName.MAF] = outdf[ColName.MAF].apply(lambda x: 1 - x if x &gt; 0.5 else x)\n    outdf = outdf[\n        (outdf[ColName.MAF] &gt;= ColRange.MAF_MIN)\n        &amp; (outdf[ColName.MAF] &lt;= ColRange.MAF_MAX)\n    ]\n    after_n = outdf.shape[0]\n    logger.debug(\n        f\"Remove {pre_n - after_n} rows because of invalid minor allele frequency.\"\n    )\n    outdf[ColName.MAF] = outdf[ColName.MAF].astype(ColType.MAF)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_pvalue","title":"<code>munge_pvalue(df)</code>","text":"<p>Munge p-value column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_pvalue--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with p-value column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_pvalue--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged p-value column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_pvalue--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Converts p-values to numeric type</li> <li>Removes rows with missing p-values</li> <li>Validates p-values are within acceptable range (0, 1)</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid p-values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_pvalue(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge p-value column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with p-value column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged p-value column.\n\n    Notes\n    -----\n    This function:\n\n    1. Converts p-values to numeric type\n    2. Removes rows with missing p-values\n    3. Validates p-values are within acceptable range (0, 1)\n    4. Converts to appropriate data type\n\n    Invalid p-values are removed and logged.\n    \"\"\"\n    outdf = df.copy()\n    pre_n = outdf.shape[0]\n    outdf[ColName.P] = pd.to_numeric(outdf[ColName.P], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.P].notnull()]\n    outdf = outdf[\n        (outdf[ColName.P] &gt; ColRange.P_MIN) &amp; (outdf[ColName.P] &lt; ColRange.P_MAX)\n    ]\n    after_n = outdf.shape[0]\n    logger.debug(f\"Remove {pre_n - after_n} rows because of invalid p-value.\")\n    outdf[ColName.P] = outdf[ColName.P].astype(ColType.P)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_rsid","title":"<code>munge_rsid(df)</code>","text":"<p>Munge rsID column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_rsid--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with rsID column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_rsid--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged rsID column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_rsid--notes","title":"Notes","text":"<p>This function converts the rsID column to the appropriate data type as defined in ColType.RSID.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_rsid(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge rsID column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with rsID column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged rsID column.\n\n    Notes\n    -----\n    This function converts the rsID column to the appropriate data type\n    as defined in ColType.RSID.\n    \"\"\"\n    outdf = df.copy()\n    outdf[ColName.RSID] = outdf[ColName.RSID].astype(ColType.RSID)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.munge_se","title":"<code>munge_se(df)</code>","text":"<p>Munge standard error column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_se--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with standard error column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_se--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with munged standard error column.</p>"},{"location":"API/sumstats/#credtools.sumstats.munge_se--notes","title":"Notes","text":"<p>This function:</p> <ol> <li>Converts standard error values to numeric type</li> <li>Removes rows with missing standard error values</li> <li>Validates standard errors are positive</li> <li>Converts to appropriate data type</li> </ol> <p>Invalid standard error values are removed and logged.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def munge_se(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Munge standard error column.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with standard error column.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with munged standard error column.\n\n    Notes\n    -----\n    This function:\n\n    1. Converts standard error values to numeric type\n    2. Removes rows with missing standard error values\n    3. Validates standard errors are positive\n    4. Converts to appropriate data type\n\n    Invalid standard error values are removed and logged.\n    \"\"\"\n    pre_n = df.shape[0]\n    outdf = df.copy()\n    outdf[ColName.SE] = pd.to_numeric(outdf[ColName.SE], errors=\"coerce\")\n    outdf = outdf[outdf[ColName.SE].notnull()]\n    outdf = outdf[outdf[ColName.SE] &gt; ColRange.SE_MIN]\n    after_n = outdf.shape[0]\n    logger.debug(f\"Remove {pre_n - after_n} rows because of invalid standard error.\")\n    outdf[ColName.SE] = outdf[ColName.SE].astype(ColType.SE)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.rm_col_allna","title":"<code>rm_col_allna(df)</code>","text":"<p>Remove columns from the DataFrame that are entirely NA.</p>"},{"location":"API/sumstats/#credtools.sumstats.rm_col_allna--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame from which to remove columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.rm_col_allna--returns","title":"Returns","text":"<p>pd.DataFrame     A DataFrame with columns that are entirely NA removed.</p>"},{"location":"API/sumstats/#credtools.sumstats.rm_col_allna--notes","title":"Notes","text":"<p>This function also converts empty strings to None before checking for all-NA columns. Columns that contain only missing values are dropped to reduce memory usage and improve processing efficiency.</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def rm_col_allna(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove columns from the DataFrame that are entirely NA.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame from which to remove columns.\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame with columns that are entirely NA removed.\n\n    Notes\n    -----\n    This function also converts empty strings to None before checking for\n    all-NA columns. Columns that contain only missing values are dropped\n    to reduce memory usage and improve processing efficiency.\n    \"\"\"\n    outdf = df.copy()\n    outdf = outdf.replace(\"\", None)\n    for col in outdf.columns:\n        if outdf[col].isnull().all():\n            logger.debug(f\"Remove column {col} because it is all NA.\")\n            outdf.drop(col, axis=1, inplace=True)\n    return outdf\n</code></pre>"},{"location":"API/sumstats/#credtools.sumstats.sort_alleles","title":"<code>sort_alleles(df)</code>","text":"<p>Sort EA and NEA in alphabetical order. Change the sign of beta if EA is not sorted as the first allele.</p>"},{"location":"API/sumstats/#credtools.sumstats.sort_alleles--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     Input DataFrame with allele columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.sort_alleles--returns","title":"Returns","text":"<p>pd.DataFrame     DataFrame with sorted allele columns.</p>"},{"location":"API/sumstats/#credtools.sumstats.sort_alleles--notes","title":"Notes","text":"<p>This function ensures consistent allele ordering by:</p> <ol> <li>Sorting effect allele (EA) and non-effect allele (NEA) alphabetically</li> <li>Flipping the sign of beta if alleles were swapped</li> <li>Adjusting effect allele frequency (EAF) if alleles were swapped (EAF = 1 - EAF)</li> </ol> <p>This standardization is important for: - Consistent merging across datasets - Meta-analysis compatibility - LD matrix alignment</p> Source code in <code>credtools/sumstats.py</code> <pre><code>def sort_alleles(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Sort EA and NEA in alphabetical order. Change the sign of beta if EA is not sorted as the first allele.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Input DataFrame with allele columns.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with sorted allele columns.\n\n    Notes\n    -----\n    This function ensures consistent allele ordering by:\n\n    1. Sorting effect allele (EA) and non-effect allele (NEA) alphabetically\n    2. Flipping the sign of beta if alleles were swapped\n    3. Adjusting effect allele frequency (EAF) if alleles were swapped (EAF = 1 - EAF)\n\n    This standardization is important for:\n    - Consistent merging across datasets\n    - Meta-analysis compatibility\n    - LD matrix alignment\n    \"\"\"\n    outdf = df.copy()\n    outdf[[\"sorted_a1\", \"sorted_a2\"]] = np.sort(\n        outdf[[ColName.EA, ColName.NEA]], axis=1\n    )\n    outdf[ColName.BETA] = np.where(\n        outdf[ColName.EA] == outdf[\"sorted_a1\"],\n        outdf[ColName.BETA],\n        -outdf[ColName.BETA],\n    )\n    if ColName.EAF in outdf.columns:\n        outdf[ColName.EAF] = np.where(\n            outdf[ColName.EA] == outdf[\"sorted_a1\"],\n            outdf[ColName.EAF],\n            1 - outdf[ColName.EAF],\n        )\n    outdf[ColName.EA] = outdf[\"sorted_a1\"]\n    outdf[ColName.NEA] = outdf[\"sorted_a2\"]\n    outdf.drop(columns=[\"sorted_a1\", \"sorted_a2\"], inplace=True)\n    return outdf\n</code></pre>"},{"location":"API/utils/","title":"utils","text":"<p>Functions and decorators for common tasks in Python programming.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool","title":"<code>ExternalTool</code>","text":"<p>A class to manage and run external tools.</p> <p>This class provides a unified interface for managing external bioinformatics tools, handling path resolution, and executing commands with proper error checking.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool--parameters","title":"Parameters","text":"<p>name : str     The name of the external tool. default_path : Optional[str], optional     The default path to the tool if not found in the system PATH, by default None.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool--attributes","title":"Attributes","text":"<p>name : str     The name of the external tool. default_path : Optional[str]     The default path to the tool if not found in the system PATH. custom_path : Optional[str]     A custom path set by the user.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool--methods","title":"Methods","text":"<p>set_custom_path(path: str) -&gt; None     Sets a custom path for the tool if it exists. get_path() -&gt; str     Retrieves the path to the tool, checking custom, system, and default paths. run(command: List[str], log_file: str, output_file_path: Optional[Union[str, List[str]]]) -&gt; None     Runs the tool with the given arguments.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool--examples","title":"Examples","text":"<p>tool = ExternalTool(\"samtools\", \"/usr/local/bin/samtools\") tool.set_custom_path(\"/opt/samtools/bin/samtools\") tool.run([\"view\", \"-h\", \"input.bam\"], \"samtools.log\", \"output.sam\")</p> Source code in <code>credtools/utils.py</code> <pre><code>class ExternalTool:\n    \"\"\"\n    A class to manage and run external tools.\n\n    This class provides a unified interface for managing external bioinformatics tools,\n    handling path resolution, and executing commands with proper error checking.\n\n    Parameters\n    ----------\n    name : str\n        The name of the external tool.\n    default_path : Optional[str], optional\n        The default path to the tool if not found in the system PATH, by default None.\n\n    Attributes\n    ----------\n    name : str\n        The name of the external tool.\n    default_path : Optional[str]\n        The default path to the tool if not found in the system PATH.\n    custom_path : Optional[str]\n        A custom path set by the user.\n\n    Methods\n    -------\n    set_custom_path(path: str) -&gt; None\n        Sets a custom path for the tool if it exists.\n    get_path() -&gt; str\n        Retrieves the path to the tool, checking custom, system, and default paths.\n    run(command: List[str], log_file: str, output_file_path: Optional[Union[str, List[str]]]) -&gt; None\n        Runs the tool with the given arguments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tool = ExternalTool(\"samtools\", \"/usr/local/bin/samtools\")\n    &gt;&gt;&gt; tool.set_custom_path(\"/opt/samtools/bin/samtools\")\n    &gt;&gt;&gt; tool.run([\"view\", \"-h\", \"input.bam\"], \"samtools.log\", \"output.sam\")\n    \"\"\"\n\n    def __init__(self, name: str, default_path: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Initialize the ExternalTool with a name and an optional default path.\n\n        Parameters\n        ----------\n        name : str\n            The name of the external tool.\n        default_path : Optional[str], optional\n            The default path to the tool if not found in the system PATH, by default None.\n        \"\"\"\n        self.name = name\n        self.default_path = default_path\n        self.custom_path: Optional[str] = None\n\n    def set_custom_path(self, path: str) -&gt; None:\n        \"\"\"\n        Set a custom path for the tool if it exists.\n\n        Parameters\n        ----------\n        path : str\n            The custom path to set.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the custom path does not exist.\n        \"\"\"\n        if os.path.exists(path):\n            self.custom_path = path\n        else:\n            raise FileNotFoundError(\n                f\"Custom path for {self.name} does not exist: {path}\"\n            )\n\n    def get_path(self) -&gt; str:\n        \"\"\"\n        Retrieve the path to the tool, checking custom, system, and default paths.\n\n        The function checks paths in the following order:\n        1. Custom path (if set via set_custom_path)\n        2. System PATH (using shutil.which)\n        3. Default path (relative to package directory)\n\n        Returns\n        -------\n        str\n            The path to the tool.\n\n        Raises\n        ------\n        FileNotFoundError\n            If the tool cannot be found in any of the paths.\n        \"\"\"\n        if self.custom_path:\n            return self.custom_path\n\n        system_tool = shutil.which(self.name)\n        if system_tool:\n            return system_tool\n\n        if self.default_path:\n            package_dir = Path(__file__).parent\n            internal_tool = package_dir / self.default_path\n            if internal_tool.exists():\n                return str(internal_tool)\n\n        raise FileNotFoundError(f\"Could not find {self.name} executable\")\n\n    def run(\n        self,\n        command: List[str],\n        log_file: str,\n        output_file_path: Optional[Union[str, List[str]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Execute a command line instruction, log the output, and handle errors.\n\n        This function runs the given command, captures stdout and stderr,\n        logs them using logging.debug, and raises exceptions for command failures\n        or missing output files.\n\n        Parameters\n        ----------\n        command : List[str]\n            The command line instruction to be executed (without the tool name).\n        log_file : str\n            The file to log the output.\n        output_file_path : Optional[Union[str, List[str]]], optional\n            The expected output file path(s). If provided, the function will check\n            if these files exist after command execution, by default None.\n\n        Raises\n        ------\n        subprocess.CalledProcessError\n            If the command execution fails.\n        FileNotFoundError\n            If the specified output file is not found after command execution.\n\n        Examples\n        --------\n        &gt;&gt;&gt; tool = ExternalTool(\"finemap\")\n        &gt;&gt;&gt; tool.run([\"--help\"], \"finemap.log\")\n        &gt;&gt;&gt; tool.run([\"--in-files\", \"data.master\"], \"finemap.log\", \"output.snp\")\n        \"\"\"\n        full_command = [self.get_path()] + command\n        try:\n            # Run the command and capture output\n            logger.debug(f\"Run command: {' '.join(full_command)}\")\n            with open(log_file, \"w\") as log:\n                subprocess.run(\n                    full_command, shell=False, check=True, stdout=log, stderr=log\n                )\n\n            # Check for output file if path is provided\n            if output_file_path:\n                if isinstance(output_file_path, str):\n                    output_file_path = [output_file_path]\n                for path in output_file_path:\n                    if not os.path.exists(path):\n                        raise FileNotFoundError(\n                            f\"Expected output file not found: {path}\"\n                        )\n\n        except Exception as e:\n            logger.error(f\"Command execution failed: {e}\\nSee {log_file} for details.\")\n            raise\n</code></pre>"},{"location":"API/utils/#credtools.utils.ExternalTool.__init__","title":"<code>__init__(name, default_path=None)</code>","text":"<p>Initialize the ExternalTool with a name and an optional default path.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.__init__--parameters","title":"Parameters","text":"<p>name : str     The name of the external tool. default_path : Optional[str], optional     The default path to the tool if not found in the system PATH, by default None.</p> Source code in <code>credtools/utils.py</code> <pre><code>def __init__(self, name: str, default_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Initialize the ExternalTool with a name and an optional default path.\n\n    Parameters\n    ----------\n    name : str\n        The name of the external tool.\n    default_path : Optional[str], optional\n        The default path to the tool if not found in the system PATH, by default None.\n    \"\"\"\n    self.name = name\n    self.default_path = default_path\n    self.custom_path: Optional[str] = None\n</code></pre>"},{"location":"API/utils/#credtools.utils.ExternalTool.get_path","title":"<code>get_path()</code>","text":"<p>Retrieve the path to the tool, checking custom, system, and default paths.</p> <p>The function checks paths in the following order: 1. Custom path (if set via set_custom_path) 2. System PATH (using shutil.which) 3. Default path (relative to package directory)</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.get_path--returns","title":"Returns","text":"<p>str     The path to the tool.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.get_path--raises","title":"Raises","text":"<p>FileNotFoundError     If the tool cannot be found in any of the paths.</p> Source code in <code>credtools/utils.py</code> <pre><code>def get_path(self) -&gt; str:\n    \"\"\"\n    Retrieve the path to the tool, checking custom, system, and default paths.\n\n    The function checks paths in the following order:\n    1. Custom path (if set via set_custom_path)\n    2. System PATH (using shutil.which)\n    3. Default path (relative to package directory)\n\n    Returns\n    -------\n    str\n        The path to the tool.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the tool cannot be found in any of the paths.\n    \"\"\"\n    if self.custom_path:\n        return self.custom_path\n\n    system_tool = shutil.which(self.name)\n    if system_tool:\n        return system_tool\n\n    if self.default_path:\n        package_dir = Path(__file__).parent\n        internal_tool = package_dir / self.default_path\n        if internal_tool.exists():\n            return str(internal_tool)\n\n    raise FileNotFoundError(f\"Could not find {self.name} executable\")\n</code></pre>"},{"location":"API/utils/#credtools.utils.ExternalTool.run","title":"<code>run(command, log_file, output_file_path=None)</code>","text":"<p>Execute a command line instruction, log the output, and handle errors.</p> <p>This function runs the given command, captures stdout and stderr, logs them using logging.debug, and raises exceptions for command failures or missing output files.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.run--parameters","title":"Parameters","text":"<p>command : List[str]     The command line instruction to be executed (without the tool name). log_file : str     The file to log the output. output_file_path : Optional[Union[str, List[str]]], optional     The expected output file path(s). If provided, the function will check     if these files exist after command execution, by default None.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.run--raises","title":"Raises","text":"<p>subprocess.CalledProcessError     If the command execution fails. FileNotFoundError     If the specified output file is not found after command execution.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.run--examples","title":"Examples","text":"<p>tool = ExternalTool(\"finemap\") tool.run([\"--help\"], \"finemap.log\") tool.run([\"--in-files\", \"data.master\"], \"finemap.log\", \"output.snp\")</p> Source code in <code>credtools/utils.py</code> <pre><code>def run(\n    self,\n    command: List[str],\n    log_file: str,\n    output_file_path: Optional[Union[str, List[str]]] = None,\n) -&gt; None:\n    \"\"\"\n    Execute a command line instruction, log the output, and handle errors.\n\n    This function runs the given command, captures stdout and stderr,\n    logs them using logging.debug, and raises exceptions for command failures\n    or missing output files.\n\n    Parameters\n    ----------\n    command : List[str]\n        The command line instruction to be executed (without the tool name).\n    log_file : str\n        The file to log the output.\n    output_file_path : Optional[Union[str, List[str]]], optional\n        The expected output file path(s). If provided, the function will check\n        if these files exist after command execution, by default None.\n\n    Raises\n    ------\n    subprocess.CalledProcessError\n        If the command execution fails.\n    FileNotFoundError\n        If the specified output file is not found after command execution.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tool = ExternalTool(\"finemap\")\n    &gt;&gt;&gt; tool.run([\"--help\"], \"finemap.log\")\n    &gt;&gt;&gt; tool.run([\"--in-files\", \"data.master\"], \"finemap.log\", \"output.snp\")\n    \"\"\"\n    full_command = [self.get_path()] + command\n    try:\n        # Run the command and capture output\n        logger.debug(f\"Run command: {' '.join(full_command)}\")\n        with open(log_file, \"w\") as log:\n            subprocess.run(\n                full_command, shell=False, check=True, stdout=log, stderr=log\n            )\n\n        # Check for output file if path is provided\n        if output_file_path:\n            if isinstance(output_file_path, str):\n                output_file_path = [output_file_path]\n            for path in output_file_path:\n                if not os.path.exists(path):\n                    raise FileNotFoundError(\n                        f\"Expected output file not found: {path}\"\n                    )\n\n    except Exception as e:\n        logger.error(f\"Command execution failed: {e}\\nSee {log_file} for details.\")\n        raise\n</code></pre>"},{"location":"API/utils/#credtools.utils.ExternalTool.set_custom_path","title":"<code>set_custom_path(path)</code>","text":"<p>Set a custom path for the tool if it exists.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.set_custom_path--parameters","title":"Parameters","text":"<p>path : str     The custom path to set.</p>"},{"location":"API/utils/#credtools.utils.ExternalTool.set_custom_path--raises","title":"Raises","text":"<p>FileNotFoundError     If the custom path does not exist.</p> Source code in <code>credtools/utils.py</code> <pre><code>def set_custom_path(self, path: str) -&gt; None:\n    \"\"\"\n    Set a custom path for the tool if it exists.\n\n    Parameters\n    ----------\n    path : str\n        The custom path to set.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the custom path does not exist.\n    \"\"\"\n    if os.path.exists(path):\n        self.custom_path = path\n    else:\n        raise FileNotFoundError(\n            f\"Custom path for {self.name} does not exist: {path}\"\n        )\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager","title":"<code>ToolManager</code>","text":"<p>A class to manage multiple external tools.</p> <p>This class provides a centralized registry for managing multiple external tools, allowing for easy registration, configuration, and execution of bioinformatics software.</p>"},{"location":"API/utils/#credtools.utils.ToolManager--attributes","title":"Attributes","text":"<p>tools : Dict[str, ExternalTool]     A dictionary to store registered tools by their names.</p>"},{"location":"API/utils/#credtools.utils.ToolManager--methods","title":"Methods","text":"<p>register_tool(name: str, default_path: Optional[str] = None) -&gt; None     Registers a new tool with an optional default path. set_tool_path(name: str, path: str) -&gt; None     Sets a custom path for a registered tool. get_tool(name: str) -&gt; ExternalTool     Retrieves a registered tool by its name. run_tool(name: str, args: List[str], log_file: str, output_file_path: Optional[Union[str, List[str]]]) -&gt; None     Runs a registered tool with the given arguments.</p>"},{"location":"API/utils/#credtools.utils.ToolManager--examples","title":"Examples","text":"<p>manager = ToolManager() manager.register_tool(\"finemap\", \"bin/finemap\") manager.set_tool_path(\"finemap\", \"/usr/local/bin/finemap\") manager.run_tool(\"finemap\", [\"--help\"], \"finemap.log\")</p> Source code in <code>credtools/utils.py</code> <pre><code>class ToolManager:\n    \"\"\"\n    A class to manage multiple external tools.\n\n    This class provides a centralized registry for managing multiple external tools,\n    allowing for easy registration, configuration, and execution of bioinformatics software.\n\n    Attributes\n    ----------\n    tools : Dict[str, ExternalTool]\n        A dictionary to store registered tools by their names.\n\n    Methods\n    -------\n    register_tool(name: str, default_path: Optional[str] = None) -&gt; None\n        Registers a new tool with an optional default path.\n    set_tool_path(name: str, path: str) -&gt; None\n        Sets a custom path for a registered tool.\n    get_tool(name: str) -&gt; ExternalTool\n        Retrieves a registered tool by its name.\n    run_tool(name: str, args: List[str], log_file: str, output_file_path: Optional[Union[str, List[str]]]) -&gt; None\n        Runs a registered tool with the given arguments.\n\n    Examples\n    --------\n    &gt;&gt;&gt; manager = ToolManager()\n    &gt;&gt;&gt; manager.register_tool(\"finemap\", \"bin/finemap\")\n    &gt;&gt;&gt; manager.set_tool_path(\"finemap\", \"/usr/local/bin/finemap\")\n    &gt;&gt;&gt; manager.run_tool(\"finemap\", [\"--help\"], \"finemap.log\")\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the ToolManager with an empty dictionary of tools.\"\"\"\n        self.tools: Dict[str, ExternalTool] = {}\n\n    def register_tool(self, name: str, default_path: Optional[str] = None) -&gt; None:\n        \"\"\"\n        Register a new tool with an optional default path.\n\n        Parameters\n        ----------\n        name : str\n            The name of the tool to register.\n        default_path : Optional[str], optional\n            The default path to the tool if not found in the system PATH, by default None.\n        \"\"\"\n        self.tools[name] = ExternalTool(name, default_path)\n\n    def set_tool_path(self, name: str, path: str) -&gt; None:\n        \"\"\"\n        Set a custom path for a registered tool.\n\n        Parameters\n        ----------\n        name : str\n            The name of the registered tool.\n        path : str\n            The custom path to set for the tool.\n\n        Raises\n        ------\n        KeyError\n            If the tool is not registered.\n        \"\"\"\n        if name not in self.tools:\n            raise KeyError(f\"Tool {name} is not registered\")\n        self.tools[name].set_custom_path(path)\n\n    def get_tool(self, name: str) -&gt; ExternalTool:\n        \"\"\"\n        Retrieve a registered tool by its name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the registered tool.\n\n        Returns\n        -------\n        ExternalTool\n            The registered tool.\n\n        Raises\n        ------\n        KeyError\n            If the tool is not registered.\n        \"\"\"\n        if name not in self.tools:\n            raise KeyError(f\"Tool {name} is not registered\")\n        return self.tools[name]\n\n    def run_tool(\n        self,\n        name: str,\n        args: List[str],\n        log_file: str,\n        output_file_path: Optional[Union[str, List[str]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Run a registered tool with the given arguments.\n\n        Parameters\n        ----------\n        name : str\n            The name of the registered tool.\n        args : List[str]\n            The arguments to pass to the tool.\n        log_file : str\n            The file to log the output.\n        output_file_path : Optional[Union[str, List[str]]], optional\n            The expected output file path(s). If provided, the function will check\n            if these files exist after command execution, by default None.\n\n        Raises\n        ------\n        KeyError\n            If the tool is not registered.\n        subprocess.CalledProcessError\n            If the subprocess call fails.\n        FileNotFoundError\n            If expected output files are not found after execution.\n        \"\"\"\n        if name not in self.tools:\n            raise KeyError(f\"Tool {name} is not registered\")\n        return self.get_tool(name).run(args, log_file, output_file_path)\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the ToolManager with an empty dictionary of tools.</p> Source code in <code>credtools/utils.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the ToolManager with an empty dictionary of tools.\"\"\"\n    self.tools: Dict[str, ExternalTool] = {}\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager.get_tool","title":"<code>get_tool(name)</code>","text":"<p>Retrieve a registered tool by its name.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.get_tool--parameters","title":"Parameters","text":"<p>name : str     The name of the registered tool.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.get_tool--returns","title":"Returns","text":"<p>ExternalTool     The registered tool.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.get_tool--raises","title":"Raises","text":"<p>KeyError     If the tool is not registered.</p> Source code in <code>credtools/utils.py</code> <pre><code>def get_tool(self, name: str) -&gt; ExternalTool:\n    \"\"\"\n    Retrieve a registered tool by its name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the registered tool.\n\n    Returns\n    -------\n    ExternalTool\n        The registered tool.\n\n    Raises\n    ------\n    KeyError\n        If the tool is not registered.\n    \"\"\"\n    if name not in self.tools:\n        raise KeyError(f\"Tool {name} is not registered\")\n    return self.tools[name]\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager.register_tool","title":"<code>register_tool(name, default_path=None)</code>","text":"<p>Register a new tool with an optional default path.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.register_tool--parameters","title":"Parameters","text":"<p>name : str     The name of the tool to register. default_path : Optional[str], optional     The default path to the tool if not found in the system PATH, by default None.</p> Source code in <code>credtools/utils.py</code> <pre><code>def register_tool(self, name: str, default_path: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Register a new tool with an optional default path.\n\n    Parameters\n    ----------\n    name : str\n        The name of the tool to register.\n    default_path : Optional[str], optional\n        The default path to the tool if not found in the system PATH, by default None.\n    \"\"\"\n    self.tools[name] = ExternalTool(name, default_path)\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager.run_tool","title":"<code>run_tool(name, args, log_file, output_file_path=None)</code>","text":"<p>Run a registered tool with the given arguments.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.run_tool--parameters","title":"Parameters","text":"<p>name : str     The name of the registered tool. args : List[str]     The arguments to pass to the tool. log_file : str     The file to log the output. output_file_path : Optional[Union[str, List[str]]], optional     The expected output file path(s). If provided, the function will check     if these files exist after command execution, by default None.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.run_tool--raises","title":"Raises","text":"<p>KeyError     If the tool is not registered. subprocess.CalledProcessError     If the subprocess call fails. FileNotFoundError     If expected output files are not found after execution.</p> Source code in <code>credtools/utils.py</code> <pre><code>def run_tool(\n    self,\n    name: str,\n    args: List[str],\n    log_file: str,\n    output_file_path: Optional[Union[str, List[str]]] = None,\n) -&gt; None:\n    \"\"\"\n    Run a registered tool with the given arguments.\n\n    Parameters\n    ----------\n    name : str\n        The name of the registered tool.\n    args : List[str]\n        The arguments to pass to the tool.\n    log_file : str\n        The file to log the output.\n    output_file_path : Optional[Union[str, List[str]]], optional\n        The expected output file path(s). If provided, the function will check\n        if these files exist after command execution, by default None.\n\n    Raises\n    ------\n    KeyError\n        If the tool is not registered.\n    subprocess.CalledProcessError\n        If the subprocess call fails.\n    FileNotFoundError\n        If expected output files are not found after execution.\n    \"\"\"\n    if name not in self.tools:\n        raise KeyError(f\"Tool {name} is not registered\")\n    return self.get_tool(name).run(args, log_file, output_file_path)\n</code></pre>"},{"location":"API/utils/#credtools.utils.ToolManager.set_tool_path","title":"<code>set_tool_path(name, path)</code>","text":"<p>Set a custom path for a registered tool.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.set_tool_path--parameters","title":"Parameters","text":"<p>name : str     The name of the registered tool. path : str     The custom path to set for the tool.</p>"},{"location":"API/utils/#credtools.utils.ToolManager.set_tool_path--raises","title":"Raises","text":"<p>KeyError     If the tool is not registered.</p> Source code in <code>credtools/utils.py</code> <pre><code>def set_tool_path(self, name: str, path: str) -&gt; None:\n    \"\"\"\n    Set a custom path for a registered tool.\n\n    Parameters\n    ----------\n    name : str\n        The name of the registered tool.\n    path : str\n        The custom path to set for the tool.\n\n    Raises\n    ------\n    KeyError\n        If the tool is not registered.\n    \"\"\"\n    if name not in self.tools:\n        raise KeyError(f\"Tool {name} is not registered\")\n    self.tools[name].set_custom_path(path)\n</code></pre>"},{"location":"API/utils/#credtools.utils.check_r_package","title":"<code>check_r_package(package_name)</code>","text":"<p>Check if R version is 4.0 or later and if a specified R package is installed.</p> <p>This function first checks the R version, then verifies if the specified R package is installed on the system.</p>"},{"location":"API/utils/#credtools.utils.check_r_package--parameters","title":"Parameters","text":"<p>package_name : str     The name of the R package to check.</p>"},{"location":"API/utils/#credtools.utils.check_r_package--returns","title":"Returns","text":"<p>None     If the R version is 4.0 or later and the package is installed.</p>"},{"location":"API/utils/#credtools.utils.check_r_package--raises","title":"Raises","text":"<p>RuntimeError     If R version is earlier than 4.0 or if the specified package is not installed. subprocess.CalledProcessError     If there's an error executing the R commands. FileNotFoundError     If R is not installed or not found in the system PATH.</p>"},{"location":"API/utils/#credtools.utils.check_r_package--examples","title":"Examples","text":"<p>check_r_package(\"ggplot2\")</p> Source code in <code>credtools/utils.py</code> <pre><code>def check_r_package(package_name: str) -&gt; None:\n    \"\"\"\n    Check if R version is 4.0 or later and if a specified R package is installed.\n\n    This function first checks the R version, then verifies if the specified\n    R package is installed on the system.\n\n    Parameters\n    ----------\n    package_name : str\n        The name of the R package to check.\n\n    Returns\n    -------\n    None\n        If the R version is 4.0 or later and the package is installed.\n\n    Raises\n    ------\n    RuntimeError\n        If R version is earlier than 4.0 or if the specified package is not installed.\n    subprocess.CalledProcessError\n        If there's an error executing the R commands.\n    FileNotFoundError\n        If R is not installed or not found in the system PATH.\n\n    Examples\n    --------\n    &gt;&gt;&gt; check_r_package(\"ggplot2\")\n    &gt;&gt;&gt; # No output if successful\n    &gt;&gt;&gt; check_r_package(\"nonexistentpackage\")\n    RuntimeError: R package 'nonexistentpackage' is not installed.\n    \"\"\"\n    # Check R version\n    try:\n        r_version_cmd = \"R --version\"\n        r_version_output = subprocess.check_output(\n            r_version_cmd, shell=True, universal_newlines=True\n        )\n        version_match = re.search(r\"R version (\\d+\\.\\d+\\.\\d+)\", r_version_output)\n        if version_match:\n            r_version = version_match.group(1)\n            if tuple(map(int, r_version.split(\".\"))) &lt; (4, 0, 0):\n                raise RuntimeError(f\"R version {r_version} is earlier than 4.0\")\n        else:\n            raise RuntimeError(\"Unable to determine R version\")\n    except subprocess.CalledProcessError:\n        raise RuntimeError(\"Failed to check R version\")\n    except FileNotFoundError:\n        raise FileNotFoundError(\"R is not installed or not found in the system PATH.\")\n\n    # Check if the package is installed\n    r_command = f\"R --slave -e \\\"if (requireNamespace('{package_name}', quietly = TRUE)) quit(status = 0) else quit(status = 1)\\\"\"\n\n    try:\n        result = subprocess.run(\n            r_command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n        if result.returncode != 0:\n            raise RuntimeError(f\"R package '{package_name}' is not installed.\")\n    except subprocess.CalledProcessError:\n        raise RuntimeError(f\"R package '{package_name}' is not installed.\")\n</code></pre>"},{"location":"API/utils/#credtools.utils.check_r_package--no-output-if-successful","title":"No output if successful","text":"<p>check_r_package(\"nonexistentpackage\") RuntimeError: R package 'nonexistentpackage' is not installed.</p>"},{"location":"API/utils/#credtools.utils.io_in_tempdir","title":"<code>io_in_tempdir(dir='./tmp')</code>","text":"<p>Create a temporary directory for I/O operations during function execution.</p> <p>This decorator creates a temporary directory before executing the decorated function and provides the path to this directory via the <code>temp_dir</code> keyword argument. After the function execution, the temporary directory is removed based on the logging level: - If the logging level is set to <code>INFO</code> or higher, the temporary directory is deleted. - If the logging level is lower than <code>INFO</code> (e.g., <code>DEBUG</code>), the directory is retained for inspection.</p>"},{"location":"API/utils/#credtools.utils.io_in_tempdir--parameters","title":"Parameters","text":"<p>dir : str, optional     The parent directory where the temporary directory will be created, by default \"./tmp\".</p>"},{"location":"API/utils/#credtools.utils.io_in_tempdir--returns","title":"Returns","text":"<p>Callable[[F], F]     A decorator that manages a temporary directory for the decorated function.</p>"},{"location":"API/utils/#credtools.utils.io_in_tempdir--raises","title":"Raises","text":"<p>OSError     If the temporary directory cannot be created.</p>"},{"location":"API/utils/#credtools.utils.io_in_tempdir--examples","title":"Examples","text":"<pre><code>@io_in_tempdir(dir=\"./temporary\")\ndef process_data(temp_dir: str, data: str) -&gt; None:\n    # Perform I/O operations using temp_dir\n    with open(f\"{temp_dir}/data.txt\", \"w\") as file:\n        file.write(data)\n\nprocess_data(data=\"Sample data\")\n</code></pre> Source code in <code>credtools/utils.py</code> <pre><code>def io_in_tempdir(dir: str = \"./tmp\") -&gt; Callable[[F], F]:\n    \"\"\"\n    Create a temporary directory for I/O operations during function execution.\n\n    This decorator creates a temporary directory before executing the decorated function and\n    provides the path to this directory via the `temp_dir` keyword argument. After the function\n    execution, the temporary directory is removed based on the logging level:\n    - If the logging level is set to `INFO` or higher, the temporary directory is deleted.\n    - If the logging level is lower than `INFO` (e.g., `DEBUG`), the directory is retained for inspection.\n\n    Parameters\n    ----------\n    dir : str, optional\n        The parent directory where the temporary directory will be created, by default \"./tmp\".\n\n    Returns\n    -------\n    Callable[[F], F]\n        A decorator that manages a temporary directory for the decorated function.\n\n    Raises\n    ------\n    OSError\n        If the temporary directory cannot be created.\n\n    Examples\n    --------\n    ```python\n    @io_in_tempdir(dir=\"./temporary\")\n    def process_data(temp_dir: str, data: str) -&gt; None:\n        # Perform I/O operations using temp_dir\n        with open(f\"{temp_dir}/data.txt\", \"w\") as file:\n            file.write(data)\n\n    process_data(data=\"Sample data\")\n    ```\n    \"\"\"\n\n    def decorator(func: F) -&gt; F:\n        @wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n            if not os.path.exists(dir):\n                os.makedirs(dir, exist_ok=True)\n            temp_dir = tempfile.mkdtemp(dir=dir)\n            logger.debug(f\"Created temporary directory: {temp_dir}\")\n\n            try:\n                # Inject temp_dir into the function's keyword arguments\n                result = func(*args, temp_dir=temp_dir, **kwargs)\n            except Exception as e:\n                logger.error(f\"An error occurred in function '{func.__name__}': {e}\")\n                raise\n            else:\n                # Determine whether to remove the temporary directory based on the logging level\n                if logger.getEffectiveLevel() &gt;= logging.INFO:\n                    try:\n                        shutil.rmtree(temp_dir)\n                        logger.debug(f\"Removed temporary directory: {temp_dir}\")\n                    except Exception as cleanup_error:\n                        logger.warning(\n                            f\"Failed to remove temporary directory '{temp_dir}': {cleanup_error}\"\n                        )\n                else:\n                    logger.debug(\n                        f\"Retaining temporary directory '{temp_dir}' for inspection due to logging level.\"\n                    )\n                return result\n\n        return wrapper  # type: ignore\n\n    return decorator\n</code></pre>"},{"location":"tutorial/advanced/","title":"Advanced Topics","text":"<p>This section covers advanced CREDTOOLS usage including detailed parameter optimization, custom workflows, and troubleshooting complex scenarios.</p>"},{"location":"tutorial/advanced/#tool-specific-parameter-optimization","title":"Tool-Specific Parameter Optimization","text":""},{"location":"tutorial/advanced/#susie-advanced-parameters","title":"SuSiE Advanced Parameters","text":"<p>SuSiE is the most commonly used tool in CREDTOOLS. Here's how to optimize its performance:</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool susie \\\n    --max-causal 10 \\\n    --max-iter 100 \\\n    --estimate-residual-variance \\\n    --min-abs-corr 0.5 \\\n    --convergence-tol 1e-3 \\\n    --coverage 0.95\n</code></pre> <p>SuSiE Parameter Tuning</p> <code>--max-causal</code> (L parameter) Start with COJO-estimated value (<code>--set-L-by-cojo</code>) Increase if credible sets seem too restrictive Rule of thumb: L \u2248 number of genome-wide significant hits in region <code>--max-iter</code> Default 100 is usually sufficient Increase to 500+ for complex regions with many causal variants Monitor convergence warnings <code>--estimate-residual-variance</code> Use <code>True</code> when phenotype variance is unknown Use <code>False</code> for standardized effect sizes Can improve convergence in some cases <code>--min-abs-corr</code> Minimum correlation threshold for credible sets Lower values (0.1-0.3) for diverse LD patterns Higher values (0.5-0.8) for strong LD regions"},{"location":"tutorial/advanced/#finemap-advanced-configuration","title":"FINEMAP Advanced Configuration","text":"<p>FINEMAP offers extensive Bayesian model configuration:</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool finemap \\\n    --max-causal 5 \\\n    --n-iter 1000000 \\\n    --n-threads 8 \\\n    --coverage 0.95\n</code></pre> <p>FINEMAP Considerations</p> Computational Requirements Memory usage scales with region size\u00b2  Use more iterations (1M+) for stable results Parallel threads improve speed significantly Model Space Exploration FINEMAP explores all possible causal combinations Exponential complexity limits max-causal to ~5-8 Consider region subdivision for larger L"},{"location":"tutorial/advanced/#multisusie-population-parameters","title":"MultiSuSiE Population Parameters","text":"<p>Fine-tune multi-population analysis:</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy multi_input \\\n    --tool multisusie \\\n    --max-causal 10 \\\n    --rho 0.75 \\\n    --scaled-prior-variance 0.2 \\\n    --pop-spec-standardization \\\n    --estimate-prior-variance \\\n    --pop-spec-effect-priors \\\n    --iter-before-zeroing-effects 5 \\\n    --prior-tol 1e-9\n</code></pre> <p>MultiSuSiE Optimization</p> <code>--rho</code> (correlation parameter) 0.9-0.95: Strong sharing across populations (most traits) 0.7-0.8: Moderate sharing with some population specificity 0.5-0.6: Weak sharing, mostly population-specific effects Population-specific options Use <code>--pop-spec-standardization</code> when sample sizes vary &gt;10x Use <code>--pop-spec-effect-priors</code> for very different populations Monitor convergence with complex population structure"},{"location":"tutorial/advanced/#carma-model-selection","title":"CARMA Model Selection","text":"<p>CARMA offers sophisticated model uncertainty quantification:</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool carma \\\n    --max-causal 10 \\\n    --effect-size-prior \"Spike-slab\" \\\n    --y-var 1.0 \\\n    --bf-threshold 10.0 \\\n    --outlier-bf-threshold 0.31 \\\n    --max-model-dim 200000 \\\n    --tau 0.04 \\\n    --em-dist \"logistic\"\n</code></pre> <p>CARMA Model Configuration</p> Prior Selection \"Spike-slab\" for sparse genetic architecture \"Cauchy\" for more diffuse effects Bayes Factor Thresholds Lower <code>--bf-threshold</code> includes more models Higher values focus on strongest evidence <code>--outlier-bf-threshold</code> controls outlier detection"},{"location":"tutorial/advanced/#custom-workflow-development","title":"Custom Workflow Development","text":""},{"location":"tutorial/advanced/#manual-pipeline-execution","title":"Manual Pipeline Execution","text":"<p>For maximum control, run pipeline steps separately:</p> <pre><code># Step 1: Meta-analysis\ncredtools meta input_loci.txt meta_output/ \\\n    --meta-method meta_by_population \\\n    --threads 4\n\n# Step 2: Quality control  \ncredtools qc meta_output/updated_loci.txt qc_output/ \\\n    --threads 4\n\n# Step 3: Review QC and filter problematic studies\n# (Manual inspection of QC outputs)\n\n# Step 4: Fine-mapping with optimized parameters\ncredtools finemap filtered_loci.txt final_output/ \\\n    --strategy post_hoc_combine \\\n    --tool susie \\\n    --max-causal 15 \\\n    --combine-cred cluster \\\n    --jaccard-threshold 0.2\n</code></pre>"},{"location":"tutorial/advanced/#parallel-processing-for-multiple-loci","title":"Parallel Processing for Multiple Loci","text":"<p>Process many loci efficiently:</p> <pre><code># Create per-locus input files\nsplit -l 2 all_loci.txt locus_\n\n# Process in parallel\nfor locus_file in locus_*; do\n    locus_id=$(tail -1 $locus_file | cut -f8)\n    credtools pipeline $locus_file results/$locus_id/ \\\n        --tool susie --max-causal 5 &amp;\ndone\nwait\n\n# Combine results\nfind results/ -name \"pips.txt\" -exec cat {} \\; &gt; combined_pips.txt\n</code></pre>"},{"location":"tutorial/advanced/#custom-combination-strategies","title":"Custom Combination Strategies","text":"<p>Implement custom result combination:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\ndef custom_pip_combination(pip_files, method=\"harmonic_mean\"):\n    \"\"\"Custom PIP combination across studies.\"\"\"\n    all_pips = []\n\n    for file in pip_files:\n        pips = pd.read_csv(file, sep='\\t', header=None, \n                          names=['SNP', 'PIP'], index_col=0)\n        all_pips.append(pips)\n\n    # Align all studies to same SNP set\n    common_snps = set.intersection(*[set(p.index) for p in all_pips])\n    aligned_pips = [p.loc[common_snps] for p in all_pips]\n\n    if method == \"harmonic_mean\":\n        # Harmonic mean of PIPs\n        pip_matrix = pd.concat(aligned_pips, axis=1)\n        combined = len(pip_matrix.columns) / (1/pip_matrix).sum(axis=1)\n    elif method == \"geometric_mean\":\n        # Geometric mean of PIPs  \n        pip_matrix = pd.concat(aligned_pips, axis=1)\n        combined = pip_matrix.prod(axis=1) ** (1/pip_matrix.shape[1])\n\n    return combined.sort_values(ascending=False)\n</code></pre>"},{"location":"tutorial/advanced/#quality-control-deep-dive","title":"Quality Control Deep Dive","text":""},{"location":"tutorial/advanced/#interpreting-qc-metrics","title":"Interpreting QC Metrics","text":""},{"location":"tutorial/advanced/#s-parameter-interpretation","title":"S Parameter Interpretation","text":"<p>The s parameter measures data quality:</p> <pre><code>import pandas as pd\n\ns_estimates = pd.read_csv('qc_output/s_estimate.txt', sep='\\t')\nprint(f\"Mean s: {s_estimates['s'].mean():.3f}\")\nprint(f\"Max s: {s_estimates['s'].max():.3f}\")\n\n# Flag problematic studies\nproblematic = s_estimates[s_estimates['s'] &gt; 0.2]\nprint(f\"Problematic studies: {len(problematic)}\")\n</code></pre> <p>S Parameter Thresholds</p> <ul> <li>s &lt; 0.1: Excellent data quality</li> <li>0.1 &lt; s &lt; 0.2: Acceptable quality  </li> <li>s &gt; 0.2: Potential issues (investigate further)</li> <li>s &gt; 0.5: Serious problems (consider excluding)</li> </ul>"},{"location":"tutorial/advanced/#cochrans-q-analysis","title":"Cochran's Q Analysis","text":"<p>Assess effect size heterogeneity:</p> <pre><code>cochran_q = pd.read_csv('qc_output/cochran_q.txt', sep='\\t')\n\n# High Q indicates heterogeneity\nhigh_het = cochran_q[cochran_q['Q_pval'] &lt; 0.05]\nprint(f\"SNPs with significant heterogeneity: {len(high_het)}\")\n\n# Examine I\u00b2 statistic\nmean_i2 = cochran_q['I2'].mean()\nprint(f\"Mean I\u00b2: {mean_i2:.1f}%\")\n</code></pre>"},{"location":"tutorial/advanced/#advanced-qc-filtering","title":"Advanced QC Filtering","text":"<pre><code># Filter studies based on QC metrics\npython filter_studies.py \\\n    --input original_loci.txt \\\n    --s-threshold 0.2 \\\n    --het-threshold 0.05 \\\n    --output filtered_loci.txt\n</code></pre> filter_studies.py<pre><code>#!/usr/bin/env python3\nimport pandas as pd\nimport argparse\n\ndef filter_studies(input_file, s_threshold, het_threshold, output_file):\n    # Load original loci\n    loci = pd.read_csv(input_file, sep='\\t')\n\n    # Load QC metrics\n    s_est = pd.read_csv('qc_output/s_estimate.txt', sep='\\t')\n\n    # Filter based on s parameter\n    good_studies = s_est[s_est['s'] &lt;= s_threshold]['study_id']\n    filtered_loci = loci[loci['prefix'].isin(good_studies)]\n\n    # Additional filtering logic here...\n\n    filtered_loci.to_csv(output_file, sep='\\t', index=False)\n    print(f\"Filtered from {len(loci)} to {len(filtered_loci)} studies\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input', required=True)\n    parser.add_argument('--s-threshold', type=float, default=0.2)\n    parser.add_argument('--het-threshold', type=float, default=0.05)\n    parser.add_argument('--output', required=True)\n    args = parser.parse_args()\n\n    filter_studies(args.input, args.s_threshold, args.het_threshold, args.output)\n</code></pre>"},{"location":"tutorial/advanced/#performance-optimization","title":"Performance Optimization","text":""},{"location":"tutorial/advanced/#memory-management","title":"Memory Management","text":"<p>For large regions or many populations:</p> <pre><code># Monitor memory usage\ncredtools finemap input.txt output/ \\\n    --tool susie \\\n    --max-causal 5 \\\n    --verbose 2&gt;&amp;1 | grep -i memory\n\n# Reduce memory footprint\nexport OMP_NUM_THREADS=1  # Limit thread memory\nulimit -v 32000000       # Set memory limit (32GB)\n</code></pre>"},{"location":"tutorial/advanced/#computational-scaling","title":"Computational Scaling","text":"<pre><code># Scale across multiple nodes\nfor chr in {1..22}; do\n    sbatch --job-name=chr${chr} run_chr.sh $chr\ndone\n</code></pre> run_chr.sh<pre><code>#!/bin/bash\n#SBATCH --mem=32G\n#SBATCH --cpus-per-task=8\n\nchr=$1\ncredtools pipeline chr${chr}_loci.txt results/chr${chr}/ \\\n    --threads 8 \\\n    --tool susie \\\n    --max-causal 10\n</code></pre>"},{"location":"tutorial/advanced/#troubleshooting-complex-issues","title":"Troubleshooting Complex Issues","text":""},{"location":"tutorial/advanced/#convergence-problems","title":"Convergence Problems","text":"<p>When tools fail to converge:</p> <p>Convergence Issues</p> SuSiE not converging Increase <code>--max-iter</code> to 500+ Try <code>--estimate-residual-variance</code> Reduce <code>--max-causal</code> if very high Check for numerical instability in LD matrix FINEMAP stuck Increase <code>--n-iter</code> substantially Reduce <code>--max-causal</code> Check for perfect correlation in LD matrix MultiSuSiE unstable Reduce <code>--rho</code> if populations very different Use <code>--pop-spec-effect-priors</code> Check population-specific sample sizes"},{"location":"tutorial/advanced/#ld-matrix-issues","title":"LD Matrix Issues","text":"<p>Common LD matrix problems:</p> <pre><code>import numpy as np\nfrom scipy.linalg import LinAlgError\n\ndef diagnose_ld_matrix(ld_file):\n    \"\"\"Diagnose LD matrix problems.\"\"\"\n    ld = np.load(ld_file)['ld']\n\n    # Check for numerical issues\n    print(f\"Matrix shape: {ld.shape}\")\n    print(f\"Diagonal range: {np.diag(ld).min():.3f} - {np.diag(ld).max():.3f}\")\n    print(f\"Off-diagonal range: {ld[~np.eye(ld.shape[0], dtype=bool)].min():.3f} - {ld[~np.eye(ld.shape[0], dtype=bool)].max():.3f}\")\n\n    # Check positive definiteness\n    try:\n        eigvals = np.linalg.eigvals(ld)\n        min_eigval = eigvals.min()\n        print(f\"Minimum eigenvalue: {min_eigval:.6f}\")\n\n        if min_eigval &lt; -1e-8:\n            print(\"WARNING: Matrix not positive semidefinite\")\n\n        # Check condition number\n        cond_num = np.linalg.cond(ld)\n        print(f\"Condition number: {cond_num:.2e}\")\n\n        if cond_num &gt; 1e12:\n            print(\"WARNING: Matrix is near-singular\")\n\n    except LinAlgError:\n        print(\"ERROR: Cannot compute eigenvalues\")\n\n# Usage\ndiagnose_ld_matrix('data/EUR.UKBB.chr8_41242482_42492482.ld.npz')\n</code></pre>"},{"location":"tutorial/advanced/#missing-data-patterns","title":"Missing Data Patterns","text":"<p>Handle systematic missingness:</p> <pre><code>def analyze_missingness(loci_file):\n    \"\"\"Analyze variant missingness patterns across studies.\"\"\"\n    import pandas as pd\n    from pathlib import Path\n\n    loci = pd.read_csv(loci_file, sep='\\t')\n\n    all_variants = set()\n    study_variants = {}\n\n    for _, row in loci.iterrows():\n        # Load variant list for each study\n        ldmap_file = f\"{row['prefix']}.ldmap\"\n        if Path(ldmap_file).exists():\n            variants = pd.read_csv(ldmap_file, sep='\\t')['SNPID'].tolist()\n            study_variants[row['prefix']] = set(variants)\n            all_variants.update(variants)\n\n    # Create missingness matrix\n    missingness = pd.DataFrame(index=sorted(all_variants), \n                             columns=study_variants.keys())\n\n    for study, variants in study_variants.items():\n        missingness[study] = missingness.index.isin(variants)\n\n    # Summary statistics\n    variant_coverage = missingness.sum(axis=1)\n    study_coverage = missingness.sum(axis=0)\n\n    print(f\"Total variants: {len(all_variants)}\")\n    print(f\"Variants in all studies: {sum(variant_coverage == len(study_variants))}\")\n    print(f\"Study coverage range: {study_coverage.min()} - {study_coverage.max()}\")\n\n    return missingness\n\n# Usage\nmissingness = analyze_missingness('input_loci.txt')\n</code></pre>"},{"location":"tutorial/advanced/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"tutorial/advanced/#analysis-strategy-selection","title":"Analysis Strategy Selection","text":"<p>Strategy Decision Tree</p> <p>Single well-powered study \u2192 <code>single_input</code> + <code>susie</code></p> <p>Multiple studies, similar populations \u2192 <code>meta_all</code> + <code>multi_input</code> + <code>multisusie</code></p> <p>Multiple studies, different populations \u2192 <code>meta_by_population</code> + <code>post_hoc_combine</code> + <code>susie</code></p> <p>Exploratory analysis \u2192 <code>no_meta</code> + <code>post_hoc_combine</code> + <code>susie</code></p> <p>Research/publication \u2192 Multiple strategies + comparison</p>"},{"location":"tutorial/advanced/#parameter-selection-guidelines","title":"Parameter Selection Guidelines","text":"<ol> <li>Start conservative: Use default parameters initially</li> <li>Validate with simulations: Test on simulated data when possible</li> <li>Compare strategies: Run multiple approaches and compare</li> <li>Document choices: Keep detailed records of parameter decisions</li> <li>Iterate based on results: Adjust parameters based on initial findings</li> </ol>"},{"location":"tutorial/advanced/#quality-control-workflow","title":"Quality Control Workflow","text":"<ol> <li>Always run QC first: Never skip quality control</li> <li>Manual review: Don't rely solely on automated flags</li> <li>Population-specific checks: Different populations may have different issues</li> <li>Iterative filtering: Remove problematic studies and re-run</li> <li>Document exclusions: Keep records of why studies were excluded</li> </ol> <p>This completes the comprehensive CREDTOOLS tutorial series. Each section builds upon the previous ones to provide a complete guide to multi-ancestry fine-mapping with CREDTOOLS. </p>"},{"location":"tutorial/getting-started/","title":"Getting Started","text":""},{"location":"tutorial/getting-started/#what-is-credtools","title":"What is CREDTOOLS?","text":"<p>CREDTOOLS (Credible Set Tools) is a comprehensive pipeline for performing statistical fine-mapping analysis across multiple ancestries and cohorts. It provides a unified framework for:</p> <ul> <li>Quality Control: Assess the reliability of summary statistics and LD matrices</li> <li>Meta-Analysis: Combine data across populations and cohorts  </li> <li>Fine-Mapping: Identify causal variants using multiple statistical methods</li> <li>Post-Processing: Combine and interpret results across studies</li> </ul>"},{"location":"tutorial/getting-started/#the-credtools-framework","title":"The CREDTOOLS Framework","text":"<p>CREDTOOLS's workflow can be visualized as follows:</p> <pre><code>graph TD\n    A[Multiple GWAS&lt;br/&gt;Summary Statistics] --&gt; B[Quality Control]\n    C[Multiple LD&lt;br/&gt;Matrices] --&gt; B\n    B --&gt; D{Meta-Analysis Strategy}\n    D --&gt;|Cross-Ancestry| E[Cross-Ancestry Meta]\n    D --&gt;|Within-Ancestry| F[Within-Ancestry Meta]\n    D --&gt;|No Meta| G[Keep Separate]\n    E --&gt; H[Fine-Mapping]\n    F --&gt; H\n    G --&gt; H\n    H --&gt; I{Fine-Mapping Strategy}\n    I --&gt;|Single Input| J[Run on Each&lt;br/&gt;Study Separately]\n    I --&gt;|Multi Input| K[Run on Combined&lt;br/&gt;Studies Together]\n    I --&gt;|Post-hoc Combine| L[Run Separately&lt;br/&gt;Then Combine Results]\n    J --&gt; M[Results Integration]\n    K --&gt; M\n    L --&gt; M\n    M --&gt; N[Credible Sets &amp;&lt;br/&gt;Posterior Probabilities]</code></pre>"},{"location":"tutorial/getting-started/#input-data","title":"Input Data","text":"<p>CREDTOOLS works with two main types of input data:</p> <ul> <li>Summary Statistics: GWAS results containing effect sizes, standard errors, and p-values</li> <li>LD Matrices: Linkage disequilibrium correlation matrices from reference panels</li> </ul>"},{"location":"tutorial/getting-started/#meta-analysis-methods","title":"Meta-analysis Methods","text":"<p>CREDTOOLS supports three meta-analysis approaches:</p> <ul> <li><code>meta_all</code>: Combine all studies regardless of ancestry</li> <li><code>meta_by_population</code>: Combine studies within each ancestry separately  </li> <li><code>no_meta</code>: Keep all studies separate</li> </ul>"},{"location":"tutorial/getting-started/#fine-mapping-strategies","title":"Fine-mapping Strategies","text":"<p>CREDTOOLS offers three complementary fine-mapping strategies:</p> <ul> <li> Single Input (<code>single_input</code>) Use traditional fine-mapping tools that analyze one study at a time Best for: Well-powered single studies, ancestry-specific analysis </li> <li> Multi Input (<code>multi_input</code>) Use tools designed to analyze multiple studies simultaneously Best for: Leveraging shared causal architecture across populations </li> <li> Post-hoc Combine (<code>post_hoc_combine</code>) Run single-input tools on each study, then combine results Best for: Maximum flexibility and interpretability </li> </ul>"},{"location":"tutorial/getting-started/#quality-control","title":"Quality Control","text":"<p>CREDTOOLS provides comprehensive QC including:</p> <ul> <li>Consistency checks: Kriging RSS to detect allele switches</li> <li>LD structure: Eigenvalue decomposition and 4th moment analysis  </li> <li>Cross-study comparisons: Cochran's Q test for heterogeneity</li> <li>Frequency comparisons: MAF consistency across studies</li> </ul>"},{"location":"tutorial/getting-started/#output-files","title":"Output Files","text":"<p>CREDTOOLS generates:</p> <ul> <li>Credible sets: Sets of variants likely to contain causal variants</li> <li>Posterior inclusion probabilities (PIPs): Individual variant probabilities</li> <li>QC reports: Detailed quality control metrics</li> <li>Meta-analysis results: Combined summary statistics and LD matrices</li> </ul>"},{"location":"tutorial/getting-started/#when-to-use-credtools","title":"When to Use CREDTOOLS","text":"<p>Use Cases</p> Multi-ancestry studies Leverage power across populations while accounting for LD differences Multiple cohorts per ancestry Combine studies within ancestry groups for increased power Heterogeneous effect sizes Use post-hoc combination to preserve ancestry-specific signals Quality control focus Extensive QC metrics help identify problematic data <p>Considerations</p> <ul> <li>Requires matched summary statistics and LD matrices</li> <li>Computational requirements scale with number of studies</li> <li>Some tools require specific data formats or parameter tuning</li> </ul>"},{"location":"tutorial/getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you understand the CREDTOOLS framework, let's jump into a practical example:</p> <p>\ud83d\udc49 Quick Start Guide - Run your first CREDTOOLS analysis</p> <p>Or explore specific scenarios:</p> <ul> <li>Single-Input Fine-Mapping for single-study analysis</li> <li>Multi-Input Fine-Mapping for multi-study analysis </li> </ul>"},{"location":"tutorial/multi-input/","title":"Multi-Input Analysis","text":"<p>Multi-input fine-mapping analyzes multiple cohorts and ancestries simultaneously to leverage shared genetic architecture while accounting for population differences. This is where CREDTOOLS truly shines, offering sophisticated approaches to multi-ancestry genetic analysis.</p>"},{"location":"tutorial/multi-input/#when-to-use-multi-input-strategy","title":"When to Use Multi-Input Strategy","text":"<p>Multi-Input Use Cases</p> Multiple ancestries available Leverage power across populations while modeling LD differences Shared causal architecture When you expect similar causal variants across populations Increased statistical power Combine sample sizes for improved fine-mapping resolution Cross-population validation Identify variants with consistent effects across ancestries"},{"location":"tutorial/multi-input/#multi-input-workflow-components","title":"Multi-Input Workflow Components","text":""},{"location":"tutorial/multi-input/#1-meta-analysis-strategies","title":"1. Meta-Analysis Strategies","text":"<p>The first step is deciding how to combine your studies:</p>"},{"location":"tutorial/multi-input/#cross-ancestry-meta-analysis-meta_all","title":"Cross-Ancestry Meta-Analysis (<code>meta_all</code>)","text":"<p>Combines all studies regardless of ancestry:</p> <pre><code>credtools meta input_loci.txt meta_output/ \\\n    --meta-method meta_all \\\n    --threads 4\n</code></pre> <p>When to use: - Strong prior belief in shared causal variants - Large effect sizes relative to population differences - Increased power is the primary goal</p> <p>What it does: - Performs inverse-variance weighted meta-analysis of summary statistics - Sample-size weighted averaging of LD matrices - Creates single meta-analyzed dataset per locus</p>"},{"location":"tutorial/multi-input/#within-ancestry-meta-analysis-meta_by_population","title":"Within-Ancestry Meta-Analysis (<code>meta_by_population</code>)","text":"<p>Combines studies within each ancestry separately:</p> <pre><code>credtools meta input_loci.txt meta_output/ \\\n    --meta-method meta_by_population \\\n    --threads 4\n</code></pre> <p>When to use: - Population-specific effect sizes expected - Want to preserve ancestry-specific signals - Balanced approach between power and specificity</p> <p>What it does: - Meta-analyzes EUR studies together, AFR studies together, etc. - Maintains population-specific LD structure - Creates separate datasets for each ancestry</p>"},{"location":"tutorial/multi-input/#no-meta-analysis-no_meta","title":"No Meta-Analysis (<code>no_meta</code>)","text":"<p>Keeps all studies separate:</p> <pre><code>credtools meta input_loci.txt meta_output/ \\\n    --meta-method no_meta \\\n    --threads 4\n</code></pre> <p>When to use: - Highly heterogeneous effect sizes - Study-specific technical factors - Maximum preservation of individual study characteristics</p>"},{"location":"tutorial/multi-input/#2-quality-control-across-studies","title":"2. Quality Control Across Studies","text":"<p>Multi-study QC provides insights impossible with single studies:</p> <pre><code>credtools qc input_loci.txt qc_output/ --threads 4\n</code></pre>"},{"location":"tutorial/multi-input/#cross-study-qc-metrics","title":"Cross-Study QC Metrics","text":"Cochran's Q Test Tests for heterogeneity in effect sizes across studies High Q values suggest population or study-specific effects SNP Missingness Patterns Identifies variants missing in specific populations Helps understand coverage differences across ancestries LD Structure Comparison Compares LD patterns across populations Identifies regions with dramatically different LD structure Cross-Ancestry MAF Correlations Compares allele frequencies across populations Detects potential population stratification or technical issues <p>QC Output Files</p> <pre><code>qc_output/\n\u251c\u2500\u2500 cochran_q.txt           # Effect size heterogeneity\n\u251c\u2500\u2500 snp_missingness.txt     # Coverage patterns\n\u251c\u2500\u2500 ld_4th_moment.txt       # LD structure comparison\n\u251c\u2500\u2500 s_estimate.txt          # Consistency parameters\n\u2514\u2500\u2500 maf_comparison.txt      # Frequency comparisons\n</code></pre>"},{"location":"tutorial/multi-input/#3-multi-input-fine-mapping-strategies","title":"3. Multi-Input Fine-Mapping Strategies","text":"<p>CREDTOOLS offers three strategies for multi-input fine-mapping:</p>"},{"location":"tutorial/multi-input/#a-multi-input-tools-multi_input","title":"A. Multi-Input Tools (<code>multi_input</code>)","text":"<p>Use tools specifically designed for multi-population analysis:</p> <pre><code>credtools finemap input_loci.txt output/ \\\n    --strategy multi_input \\\n    --tool multisusie \\\n    --max-causal 5\n</code></pre> <p>Supported Tools:</p> <ul> <li>MultiSuSiE: Multi-population extension of SuSiE</li> <li>SuSiEx: Cross-ancestry fine-mapping tool</li> </ul>"},{"location":"tutorial/multi-input/#b-post-hoc-combination-post_hoc_combine","title":"B. Post-hoc Combination (<code>post_hoc_combine</code>)","text":"<p>Run single-input tools on each study, then intelligently combine results:</p> <pre><code>credtools finemap input_loci.txt output/ \\\n    --strategy post_hoc_combine \\\n    --tool susie \\\n    --combine-cred union \\\n    --combine-pip max \\\n    --jaccard-threshold 0.1\n</code></pre> <p>Combination Methods:</p> <p>Credible Set Combination (<code>--combine-cred</code>)</p> <code>union</code> (default) Union of all credible sets from individual studies Most inclusive, captures all potential causal variants <code>intersection</code> Only variants present in credible sets from all studies Most conservative, highest confidence variants only <code>cluster</code> Groups overlapping credible sets using Jaccard similarity Balanced approach, creates meta-credible sets <p>PIP Combination (<code>--combine-pip</code>)</p> <code>max</code> (default) Maximum PIP across all studies for each variant Emphasizes strongest signals <code>mean</code> Average PIP across studies Balanced view of evidence <code>meta</code> Meta-analysis formula: 1 - \u220f(1 - PIP_i) Accounts for independence assumption"},{"location":"tutorial/multi-input/#detailed-tool-descriptions","title":"Detailed Tool Descriptions","text":""},{"location":"tutorial/multi-input/#multisusie","title":"MultiSuSiE","text":"<p>Best for: Multi-population analysis with shared and population-specific effects</p> <pre><code>credtools finemap input_loci.txt output/ \\\n    --strategy multi_input \\\n    --tool multisusie \\\n    --max-causal 10 \\\n    --rho 0.75 \\\n    --scaled-prior-variance 0.2 \\\n    --pop-spec-standardization \\\n    --estimate-prior-variance\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--rho</code>: Prior correlation between causal variants across populations (0.75)</li> <li><code>--scaled-prior-variance</code>: Prior effect size variance (0.2)  </li> <li><code>--pop-spec-standardization</code>: Use population-specific standardization</li> <li><code>--estimate-prior-variance</code>: Estimate rather than fix prior variance</li> <li><code>--pop-spec-effect-priors</code>: Population-specific effect size priors</li> </ul> <p>MultiSuSiE Guidance</p> <ul> <li>Higher <code>--rho</code> assumes more sharing across populations</li> <li>Use <code>--pop-spec-standardization</code> when sample sizes vary greatly</li> <li><code>--estimate-prior-variance</code> is usually recommended</li> </ul>"},{"location":"tutorial/multi-input/#susiex","title":"SuSiEx","text":"<p>Best for: Cross-ancestry fine-mapping with explicit modeling of population differences</p> <pre><code>credtools finemap input_loci.txt output/ \\\n    --strategy multi_input \\\n    --tool susiex \\\n    --max-causal 5 \\\n    --mult-step \\\n    --keep-ambig \\\n    --min-purity 0.5 \\\n    --tol 1e-3\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--mult-step</code>: Use multiple refinement steps  </li> <li><code>--keep-ambig</code>: Keep ambiguous SNPs in analysis</li> <li><code>--min-purity</code>: Minimum purity for credible sets (0.5)</li> <li><code>--tol</code>: Convergence tolerance (1e-3)</li> </ul>"},{"location":"tutorial/multi-input/#comprehensive-multi-input-example","title":"Comprehensive Multi-Input Example","text":"<p>Here's a complete workflow for multi-ancestry fine-mapping:</p>"},{"location":"tutorial/multi-input/#step-1-input-preparation","title":"Step 1: Input Preparation","text":"multi_ancestry_loci.txt<pre><code>chr start   end popu    sample_size cohort  prefix  locus_id\n8   41242482    42492482    AFR 89499   MVP data/AFR.MVP.chr8_41242482_42492482 chr8_41242482_42492482\n8   41242482    42492482    EUR 337465  MVP data/EUR.MVP.chr8_41242482_42492482 chr8_41242482_42492482\n8   41242482    42492482    EUR 442817  UKBB    data/EUR.UKBB.chr8_41242482_42492482    chr8_41242482_42492482\n8   41242482    42492482    SAS 8253    UKBB    data/SAS.UKBB.chr8_41242482_42492482    chr8_41242482_42492482\n</code></pre>"},{"location":"tutorial/multi-input/#step-2-quality-control","title":"Step 2: Quality Control","text":"<pre><code># Run comprehensive QC\ncredtools qc multi_ancestry_loci.txt qc_results/ --threads 4\n\n# Review heterogeneity\nhead qc_results/cochran_q.txt\n</code></pre>"},{"location":"tutorial/multi-input/#step-3a-cross-ancestry-analysis-with-multisusie","title":"Step 3A: Cross-Ancestry Analysis with MultiSuSiE","text":"<pre><code># Meta-analyze across all populations\ncredtools pipeline multi_ancestry_loci.txt results_cross_ancestry/ \\\n    --meta-method meta_all \\\n    --strategy multi_input \\\n    --tool multisusie \\\n    --max-causal 10 \\\n    --rho 0.8 \\\n    --pop-spec-standardization \\\n    --estimate-prior-variance\n</code></pre>"},{"location":"tutorial/multi-input/#step-3b-population-specific-analysis","title":"Step 3B: Population-Specific Analysis","text":"<pre><code># Meta-analyze within populations, then combine\ncredtools pipeline multi_ancestry_loci.txt results_pop_specific/ \\\n    --meta-method meta_by_population \\\n    --strategy post_hoc_combine \\\n    --tool susie \\\n    --combine-cred cluster \\\n    --combine-pip meta \\\n    --jaccard-threshold 0.2\n</code></pre>"},{"location":"tutorial/multi-input/#step-4-compare-approaches","title":"Step 4: Compare Approaches","text":"<pre><code># Compare results from different strategies\nimport json\nimport pandas as pd\n\n# Load results\nwith open('results_cross_ancestry/creds.json') as f:\n    cross_ancestry = json.load(f)\n\nwith open('results_pop_specific/creds.json') as f:\n    pop_specific = json.load(f)\n\n# Compare credible sets and PIPs\ncross_pips = pd.read_csv('results_cross_ancestry/pips.txt', \n                        sep='\\t', header=None, names=['SNP', 'PIP'])\npop_pips = pd.read_csv('results_pop_specific/pips.txt', \n                      sep='\\t', header=None, names=['SNP', 'PIP'])\n</code></pre>"},{"location":"tutorial/multi-input/#advanced-multi-input-considerations","title":"Advanced Multi-Input Considerations","text":""},{"location":"tutorial/multi-input/#handling-missing-data","title":"Handling Missing Data","text":"<p>When variants are missing in some populations:</p> <p>Missing Data Strategies</p> Complete case analysis Only use variants present in all studies Reduces power but ensures consistency Imputation Impute missing variants using population-specific references Requires careful validation Weighted analysis Weight contributions by data availability Built into CREDTOOLS meta-analysis"},{"location":"tutorial/multi-input/#population-stratification","title":"Population Stratification","text":"<p>When populations have internal structure:</p> <pre><code># Use more conservative thresholds\ncredtools finemap input.txt output/ \\\n    --strategy multi_input \\\n    --tool multisusie \\\n    --rho 0.5 \\\n    --min-abs-corr 0.8\n</code></pre>"},{"location":"tutorial/multi-input/#computational-considerations","title":"Computational Considerations","text":"<p>Large multi-population analyses can be computationally intensive:</p> <p>Performance Optimization</p> <ul> <li>Use <code>--threads</code> for parallelization</li> <li>Consider analyzing loci separately for very large studies</li> <li>Monitor memory usage with many populations</li> <li>Use post-hoc combination for maximum flexibility</li> </ul>"},{"location":"tutorial/multi-input/#interpreting-multi-input-results","title":"Interpreting Multi-Input Results","text":""},{"location":"tutorial/multi-input/#population-specific-effects","title":"Population-Specific Effects","text":"<p>Look for variants with: - High PIPs in some populations but not others - Different credible sets across populations - High Cochran's Q values</p>"},{"location":"tutorial/multi-input/#shared-causal-variants","title":"Shared Causal Variants","text":"<p>Evidence includes: - Consistent PIPs across populations - Overlapping credible sets - Low heterogeneity in meta-analysis</p>"},{"location":"tutorial/multi-input/#meta-analysis-benefits","title":"Meta-Analysis Benefits","text":"<p>Compare single-population vs. meta-analysis: - Increased resolution (smaller credible sets) - Higher PIPs for true causal variants - Discovery of additional signals</p>"},{"location":"tutorial/multi-input/#common-multi-input-issues","title":"Common Multi-Input Issues","text":"<p>Troubleshooting Multi-Input Analysis</p> Excessive heterogeneity Use <code>meta_by_population</code> instead of <code>meta_all</code> Check for technical differences between studies Consider population-specific analyses No shared signals May indicate true population-specific effects Check LD structure differences Verify consistent variant coding Computational issues Reduce number of populations analyzed together Use post-hoc combination strategy Increase available memory/compute resources"},{"location":"tutorial/multi-input/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Topics - Deep dive into parameter optimization and custom workflows</li> <li>Review specific tool documentation for detailed parameter guidance</li> <li>Consider population genetics factors in result interpretation </li> </ul>"},{"location":"tutorial/quick-start/","title":"Quick Start Guide","text":"<p>This guide will get you running your first CREDTOOLS analysis in just a few minutes using the <code>credtools pipeline</code> command - the easiest way to perform end-to-end multi-ancestry fine-mapping.</p>"},{"location":"tutorial/quick-start/#what-is-credtools-pipeline","title":"What is <code>credtools pipeline</code>?","text":"<p>The <code>credtools pipeline</code> command runs the complete CREDTOOLS workflow in a single command:</p> <ol> <li>Quality control</li> <li>Meta-analysis</li> <li>Fine-mapping</li> <li>Results aggregation</li> </ol>"},{"location":"tutorial/quick-start/#input-data-format","title":"Input Data Format","text":"<p>CREDTOOLS requires a tab-separated file describing your loci and studies. Here's the required format:</p> Column Description Example <code>chr</code> Chromosome <code>8</code> <code>start</code> Start position (bp) <code>41242482</code> <code>end</code> End position (bp) <code>42492482</code> <code>popu</code> Population/ancestry <code>EUR</code>, <code>AFR</code>, <code>SAS</code>, <code>HIS</code> <code>sample_size</code> Sample size <code>337465</code> <code>cohort</code> Cohort/study name <code>UKBB</code>, <code>MVP</code> <code>prefix</code> File path prefix <code>/path/to/data/EUR.UKBB.chr8_41242482_42492482</code> <code>locus_id</code> Locus identifier <code>chr8_41242482_42492482</code> <p>File Structure</p> <p>For each <code>prefix</code>, CREDTOOLS expects these files:</p> <ul> <li><code>{prefix}.sumstats</code> - Summary statistics</li> <li><code>{prefix}.ld</code> or <code>{prefix}.ld.npz</code> - LD matrix  </li> <li><code>{prefix}.ldmap</code> - LD matrix variant map</li> </ul>"},{"location":"tutorial/quick-start/#example-input-file","title":"Example Input File","text":"my_loci.txt<pre><code>chr start   end popu    sample_size cohort  prefix  locus_id\n8   41242482    42492482    AFR 89499   MVP data/AFR.MVP.chr8_41242482_42492482 chr8_41242482_42492482\n8   41242482    42492482    EUR 337465  MVP data/EUR.MVP.chr8_41242482_42492482 chr8_41242482_42492482\n8   41242482    42492482    EUR 442817  UKBB    data/EUR.UKBB.chr8_41242482_42492482    chr8_41242482_42492482\n</code></pre>"},{"location":"tutorial/quick-start/#basic-usage","title":"Basic Usage","text":""},{"location":"tutorial/quick-start/#simple-cross-ancestry-analysis","title":"Simple Cross-Ancestry Analysis","text":"<pre><code>credtools pipeline my_loci.txt output_dir \\\n    --tool susie \\\n    --strategy multi_input \\\n    --threads 4\n</code></pre> <p>This command:</p> <ul> <li>Combines all studies across ancestries (<code>meta_all</code>)</li> <li>Uses multi-input strategy with MultiSuSiE</li> <li>Outputs results to <code>output_dir/</code></li> </ul>"},{"location":"tutorial/quick-start/#population-specific-analysis","title":"Population-Specific Analysis","text":"<pre><code>credtools pipeline my_loci.txt output_dir \\\n    --tool susie \\\n    --strategy multi_input \\\n    --threads 4 \\\n    --max-causal 5 \\\n    --credible-level 0.95\n</code></pre> <p>This command:</p> <ul> <li>Meta-analyzes within each ancestry separately (<code>meta_by_population</code>)</li> <li>Runs SuSiE on each population, then combines results (<code>post_hoc_combine</code>)</li> </ul>"},{"location":"tutorial/quick-start/#understanding-the-output","title":"Understanding the Output","text":"<p>After running <code>credtools pipeline</code>, you'll find these files in your output directory:</p>"},{"location":"tutorial/quick-start/#meta-analysis-results","title":"Meta-Analysis Results","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 {locus_id}.{popu}.{cohort}.sumstat    # Meta-analyzed summary stats\n\u251c\u2500\u2500 {locus_id}.{popu}.{cohort}.ld.npz     # Meta-analyzed LD matrix\n\u2514\u2500\u2500 {locus_id}.{popu}.{cohort}.ldmap      # LD variant mapping\n</code></pre>"},{"location":"tutorial/quick-start/#quality-control-reports","title":"Quality Control Reports","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 s_estimate.txt        # Inconsistency parameter estimates\n\u251c\u2500\u2500 kriging_rss.txt       # Allele switch detection\n\u251c\u2500\u2500 maf_comparison.txt    # MAF consistency across studies\n\u251c\u2500\u2500 cochran_q.txt         # Heterogeneity testing\n\u2514\u2500\u2500 ld_structure.txt      # LD matrix eigenanalysis\n</code></pre>"},{"location":"tutorial/quick-start/#fine-mapping-results","title":"Fine-Mapping Results","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 pips.txt             # Posterior inclusion probabilities\n\u2514\u2500\u2500 creds.json           # Credible sets information\n</code></pre>"},{"location":"tutorial/quick-start/#interpreting-results","title":"Interpreting Results","text":""},{"location":"tutorial/quick-start/#posterior-inclusion-probabilities-pips","title":"Posterior Inclusion Probabilities (PIPs)","text":"<p>The <code>pips.txt</code> file contains PIPs for each variant:</p> pips.txt<pre><code>8-41234567-A-G  0.0234\n8-41235678-C-T  0.8765\n8-41236789-G-A  0.0456\n</code></pre> <ul> <li>Values range from 0 to 1</li> <li>Higher values indicate stronger evidence for causality</li> <li>Typically, variants with PIP &gt; 0.1 are considered noteworthy</li> </ul>"},{"location":"tutorial/quick-start/#credible-sets","title":"Credible Sets","text":"<p>The <code>creds.json</code> file contains credible sets - groups of variants that collectively have high probability of containing the causal variant:</p> creds.json<pre><code>{\n  \"credible_sets\": {\n    \"cs1\": {\n      \"variants\": [\"8-41235678-C-T\", \"8-41235680-A-G\"],\n      \"coverage\": 0.95,\n      \"total_pip\": 0.96\n    }\n  }\n}\n</code></pre>"},{"location":"tutorial/quick-start/#common-options","title":"Common Options","text":""},{"location":"tutorial/quick-start/#meta-analysis-methods","title":"Meta-Analysis Methods","text":"<pre><code># Combine all studies regardless of ancestry\n--meta-method meta_all\n\n# Combine studies within each ancestry separately  \n--meta-method meta_by_population\n\n# Keep all studies separate (no meta-analysis)\n--meta-method no_meta\n</code></pre>"},{"location":"tutorial/quick-start/#fine-mapping-tools","title":"Fine-Mapping Tools","text":"<pre><code># General purpose, robust\n--tool susie\n\n# Multi-ancestry designed tools\n--tool multisusie\n--tool susiex\n\n# Bayesian model averaging\n--tool finemap\n\n# Simple Bayes factors  \n--tool abf\n</code></pre>"},{"location":"tutorial/quick-start/#quality-control","title":"Quality Control","text":"<pre><code># Skip QC (faster but not recommended)\n--skip-qc\n\n# Include QC (default, recommended)\n# No flag needed - QC runs by default\n</code></pre>"},{"location":"tutorial/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> File not found errors Check that your file paths in the input table are correct Ensure summary statistics and LD files exist for each prefix Memory errors Large LD matrices can consume significant memory Consider analyzing loci one at a time for very large regions Tool-specific errors Some tools have specific requirements (see tool documentation) Try SuSiE first as it's the most robust default option <p>Performance Tips</p> <ul> <li>Start with smaller regions to test your setup</li> <li>Use <code>--tool susie</code> for initial exploration (fastest, most reliable)</li> <li>Save QC results to identify problematic studies before fine-mapping</li> </ul>"},{"location":"tutorial/quick-start/#next-steps","title":"Next Steps","text":"<p>Once you've run your first analysis:</p> <ul> <li>Single-Input Fine-Mapping - Learn about analyzing individual studies</li> <li>Multi-Input Fine-Mapping - Deep dive into multi-ancestry analysis  </li> <li>Advanced Topics - Customize parameters and understand tool options</li> </ul>"},{"location":"tutorial/quick-start/#example-with-real-data","title":"Example with Real Data","text":"<p>Using the included example data:</p> <pre><code># Navigate to example data directory\ncd exampledata/\n\n# Run pipeline on example locus\ncredtools pipeline test_loci.txt results/ \\\n    --tool susie \\\n    --strategy multi_input \\\n    --threads 4\n</code></pre> <p>This will analyze the multi-ancestry example data and produce results in the <code>results/</code> directory. </p>"},{"location":"tutorial/single-input/","title":"Single-Input Fine-Mapping","text":"<p>Single-input fine-mapping is the traditional approach where you analyze one cohort or ancestry at a time. This strategy is ideal when you have well-powered individual studies or want to understand ancestry-specific genetic architecture.</p>"},{"location":"tutorial/single-input/#when-to-use-single-input-strategy","title":"When to Use Single-Input Strategy","text":"<p>Single-Input Use Cases</p> Well-powered individual studies When each study has sufficient power for fine-mapping alone Ancestry-specific analysis When you want to understand population-specific causal variants Exploratory analysis When investigating individual study characteristics before meta-analysis Heterogeneous effects When effect sizes vary significantly across populations"},{"location":"tutorial/single-input/#step-by-step-workflow","title":"Step-by-Step Workflow","text":""},{"location":"tutorial/single-input/#1-prepare-single-study-input","title":"1. Prepare Single-Study Input","text":"<p>For single-input analysis, your loci file should contain only one row per locus:</p> single_study_loci.txt<pre><code>chr start   end popu    sample_size cohort  prefix  locus_id\n8   41242482    42492482    EUR 442817  UKBB    data/EUR.UKBB.chr8_41242482_42492482    chr8_41242482_42492482\n</code></pre>"},{"location":"tutorial/single-input/#2-quality-control","title":"2. Quality Control","text":"<p>Quality control for single studies focuses on internal consistency:</p> <pre><code>credtools qc single_study_loci.txt qc_output/\n</code></pre>"},{"location":"tutorial/single-input/#qc-metrics-for-single-studies","title":"QC Metrics for Single Studies","text":"Inconsistency Parameter (s) Measures consistency between z-scores and LD matrix Values &gt; 0.2 suggest potential issues Kriging RSS Detects potential allele switches or data quality issues Flags variants with unexpected z-scores given LD structure LD Structure Analysis Eigenvalue decomposition of LD matrix Identifies problematic LD patterns MAF Comparison Compares allele frequencies between summary stats and LD reference Large differences suggest potential strand issues <p>Single-Study QC Limitations</p> <p>Single-study QC cannot detect: - Cross-study heterogeneity - Population-specific biases - Cohort-specific technical artifacts</p> <p>Consider multi-study QC when possible.</p>"},{"location":"tutorial/single-input/#3-fine-mapping","title":"3. Fine-Mapping","text":"<p>Run fine-mapping on your single study:</p> <pre><code>credtools finemap single_study_loci.txt output/ \\\n    --strategy single_input \\\n    --tool susie \\\n    --max-causal 5 \\\n    --coverage 0.95\n</code></pre>"},{"location":"tutorial/single-input/#supported-tools-for-single-input","title":"Supported Tools for Single-Input","text":""},{"location":"tutorial/single-input/#susie-recommended","title":"SuSiE (Recommended)","text":"<p>Best for: General purpose, robust across different scenarios</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool susie \\\n    --max-causal 10 \\\n    --max-iter 100 \\\n    --estimate-residual-variance \\\n    --convergence-tol 1e-3\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--max-causal</code>: Maximum number of causal variants (default: 1)</li> <li><code>--max-iter</code>: Maximum iterations (default: 100)  </li> <li><code>--estimate-residual-variance</code>: Estimate phenotype variance (default: False)</li> <li><code>--convergence-tol</code>: Convergence tolerance (default: 1e-3)</li> </ul>"},{"location":"tutorial/single-input/#finemap","title":"FINEMAP","text":"<p>Best for: Bayesian model averaging, comprehensive uncertainty quantification</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool finemap \\\n    --max-causal 5 \\\n    --n-iter 100000 \\\n    --n-threads 4\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--n-iter</code>: Number of MCMC iterations (default: 100000)</li> <li><code>--n-threads</code>: Number of parallel threads (default: 1)</li> </ul>"},{"location":"tutorial/single-input/#abf-approximate-bayes-factors","title":"ABF (Approximate Bayes Factors)","text":"<p>Best for: Fast, simple analysis with minimal assumptions</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool abf \\\n    --var-prior 0.2\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--var-prior</code>: Prior variance (0.15 for quantitative, 0.2 for binary traits)</li> </ul>"},{"location":"tutorial/single-input/#carma","title":"CARMA","text":"<p>Best for: Model uncertainty quantification, outlier detection  </p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool carma \\\n    --max-causal 10 \\\n    --effect-size-prior \"Spike-slab\" \\\n    --y-var 1.0\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--effect-size-prior</code>: \"Spike-slab\" or \"Cauchy\" (default: \"Spike-slab\")</li> <li><code>--y-var</code>: Phenotype variance (default: 1.0)</li> <li><code>--bf-threshold</code>: Bayes factor threshold (default: 10.0)</li> </ul>"},{"location":"tutorial/single-input/#rsparsepro","title":"RSparsePro","text":"<p>Best for: Sparse regression approach, computational efficiency</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool rsparsepro \\\n    --max-causal 5 \\\n    --eps 1e-5 \\\n    --cthres 0.7\n</code></pre> <p>Key Parameters:</p> <ul> <li><code>--eps</code>: Convergence criterion (default: 1e-5)</li> <li><code>--cthres</code>: Coverage threshold (default: 0.7)</li> <li><code>--minldthres</code>: Minimum LD within effect groups (default: 0.7)</li> </ul>"},{"location":"tutorial/single-input/#comparing-single-input-tools","title":"Comparing Single-Input Tools","text":"Tool Speed Memory Uncertainty Best Use Case SuSiE Fast Low Good General purpose FINEMAP Moderate Moderate Excellent Comprehensive analysis ABF Very Fast Very Low Basic Quick screening CARMA Slow High Excellent Research applications RSparsePro Fast Low Good Large regions"},{"location":"tutorial/single-input/#automatic-parameter-setting","title":"Automatic Parameter Setting","text":"<p>CREDTOOLS can automatically determine the maximum number of causal variants using COJO:</p> <pre><code>credtools finemap input.txt output/ \\\n    --strategy single_input \\\n    --tool susie \\\n    --set-L-by-cojo \\\n    --p-cutoff 5e-8 \\\n    --collinear-cutoff 0.9\n</code></pre> <p>COJO Parameters:</p> <ul> <li><code>--p-cutoff</code>: P-value threshold for conditioning (default: 5e-8)</li> <li><code>--collinear-cutoff</code>: Collinearity threshold (default: 0.9)</li> <li><code>--window-size</code>: Window for conditional analysis (default: 10Mb)</li> <li><code>--maf-cutoff</code>: MAF cutoff (default: 0.01)</li> </ul> <p>COJO Integration</p> <p>COJO (Conditional and Joint analysis) estimates the number of independent signals in a region. CREDTOOLS uses this to set <code>--max-causal</code> automatically, which often works better than arbitrary values.</p>"},{"location":"tutorial/single-input/#output-files","title":"Output Files","text":""},{"location":"tutorial/single-input/#posterior-inclusion-probabilities","title":"Posterior Inclusion Probabilities","text":"output/pips.txt<pre><code># SNPID PIP\n8-41234567-A-G  0.0234\n8-41235678-C-T  0.8765\n8-41236789-G-A  0.0456\n</code></pre>"},{"location":"tutorial/single-input/#credible-sets","title":"Credible Sets","text":"output/creds.json<pre><code>{\n  \"credible_sets\": {\n    \"cs1\": {\n      \"variants\": [\"8-41235678-C-T\", \"8-41235680-A-G\"],\n      \"coverage\": 0.95,\n      \"total_pip\": 0.96,\n      \"min_abs_corr\": 0.34\n    }\n  },\n  \"tool\": \"susie\",\n  \"strategy\": \"single_input\",\n  \"parameters\": {...}\n}\n</code></pre>"},{"location":"tutorial/single-input/#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>Troubleshooting Single-Input Analysis</p> No credible sets found Try increasing <code>--max-causal</code> Check if region has sufficient signal (<code>--p-cutoff</code>) Verify LD matrix quality Very large credible sets May indicate weak signal or LD issues Try more stringent <code>--coverage</code> (e.g., 0.99) Consider region subdivision Tool convergence issues Increase <code>--max-iter</code> for iterative methods Try different <code>--convergence-tol</code> values Switch to more robust tool (SuSiE) Memory issues with large regions Use RSparsePro for efficiency Consider region subdivision Reduce LD matrix precision if possible"},{"location":"tutorial/single-input/#example-complete-single-study-analysis","title":"Example: Complete Single-Study Analysis","text":"<pre><code># 1. Quality control\ncredtools qc my_study.txt qc_results/\n\n# 2. Review QC metrics (check s_estimate.txt, kriging_rss.txt)\n\n# 3. Fine-mapping with automatic L setting\ncredtools finemap my_study.txt results/ \\\n    --strategy single_input \\\n    --tool susie \\\n    --set-L-by-cojo \\\n    --coverage 0.95 \\\n    --max-iter 100\n\n# 4. Review results (pips.txt, creds.json)\n</code></pre>"},{"location":"tutorial/single-input/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-Input Fine-Mapping - Learn how to analyze multiple studies together</li> <li>Advanced Topics - Deep dive into tool-specific parameters and optimization </li> </ul>"},{"location":"tutorial/web-example/","title":"Web Visualization Example","text":"<p>This example demonstrates a complete CREDTOOLS workflow including web visualization.</p>"},{"location":"tutorial/web-example/#sample-dataset","title":"Sample Dataset","text":"<p>For this example, we'll use a hypothetical fine-mapping analysis with three loci across multiple populations.</p>"},{"location":"tutorial/web-example/#directory-structure","title":"Directory Structure","text":"<pre><code>example_analysis/\n\u251c\u2500\u2500 input/\n\u2502   \u2514\u2500\u2500 loci_info.txt\n\u251c\u2500\u2500 results/\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u2514\u2500\u2500 real/\n\u2502   \u2502       \u251c\u2500\u2500 credset/\n\u2502   \u2502       \u2514\u2500\u2500 qc/\n\u2502   \u2514\u2500\u2500 webdata/  # Generated automatically\n\u2514\u2500\u2500 loci_files/\n    \u251c\u2500\u2500 allmeta_loci.txt\n    \u251c\u2500\u2500 popumeta_loci.txt\n    \u2514\u2500\u2500 nometa_loci.txt\n</code></pre>"},{"location":"tutorial/web-example/#step-by-step-workflow","title":"Step-by-Step Workflow","text":""},{"location":"tutorial/web-example/#1-run-fine-mapping-pipeline","title":"1. Run Fine-mapping Pipeline","text":"<p>First, run the complete fine-mapping pipeline:</p> <pre><code># Navigate to your analysis directory\ncd example_analysis/\n\n# Run the complete pipeline\ncredtools pipeline input/loci_info.txt results/ \\\n  --tool susie \\\n  --strategy multi_input \\\n  --meta-method meta_all \\\n  --threads 10\n</code></pre> <p>This generates: - Fine-mapping results in <code>results/data/real/credset/</code> - Quality control metrics in <code>results/data/real/qc/</code> - Meta-analysis results if applicable</p>"},{"location":"tutorial/web-example/#2-prepare-loci-information-files","title":"2. Prepare Loci Information Files","text":"<p>Create loci information files for web visualization:</p> <p>allmeta_loci.txt: <pre><code>locus_id    chr start   end prefix  popu    cohort  sample_size\nlocus1  1   1000000 2000000 /path/to/locus1_data    EUR cohort1 50000\nlocus1  1   1000000 2000000 /path/to/locus1_data    ASN cohort2 30000\nlocus2  2   5000000 6000000 /path/to/locus2_data    EUR cohort1 50000\nlocus3  3   8000000 9000000 /path/to/locus3_data    AFR cohort3 25000\n</code></pre></p>"},{"location":"tutorial/web-example/#3-launch-web-interface","title":"3. Launch Web Interface","text":""},{"location":"tutorial/web-example/#basic-launch","title":"Basic Launch","text":"<pre><code># Simple launch from results directory\ncd results/\ncredtools web\n</code></pre>"},{"location":"tutorial/web-example/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Launch with specific settings\ncredtools web results/ \\\n  --allmeta-loci loci_files/allmeta_loci.txt \\\n  --popumeta-loci loci_files/popumeta_loci.txt \\\n  --nometa-loci loci_files/nometa_loci.txt \\\n  --port 8080 \\\n  --threads 15\n</code></pre>"},{"location":"tutorial/web-example/#4-explore-results","title":"4. Explore Results","text":"<p>Once the web interface starts, open your browser to <code>http://localhost:8080</code>.</p>"},{"location":"tutorial/web-example/#home-page-features","title":"Home Page Features","text":"<ol> <li>Filter by Meta-analysis Method:</li> <li>Select \"allmeta\" to see all-ancestry meta-analysis results</li> <li>Choose \"popumeta\" for population-specific results</li> <li> <p>Pick \"nometa\" for individual cohort results</p> </li> <li> <p>Filter by Fine-mapping Tool:</p> </li> <li>Compare results across SuSiE, FINEMAP, etc.</li> <li> <p>Each tool may show different credible sets</p> </li> <li> <p>View Summary Statistics:</p> </li> <li>Number of credible sets per locus</li> <li>Total credible set sizes</li> <li>SNPs with high posterior probabilities</li> </ol>"},{"location":"tutorial/web-example/#locus-specific-views","title":"Locus-Specific Views","text":"<p>Click on any locus ID to see detailed results:</p> <ol> <li>Association Plots:</li> <li>Manhattan plot with LD coloring</li> <li> <p>Fine-mapping posterior probabilities</p> </li> <li> <p>Quality Control:</p> </li> <li>Lambda inflation values</li> <li>DENTIST-S statistics</li> <li> <p>MAF correlation metrics</p> </li> <li> <p>Credible Sets:</p> </li> <li>Highlighted credible variants</li> <li>Posterior inclusion probabilities</li> <li>Cross-tool comparisons</li> </ol>"},{"location":"tutorial/web-example/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorial/web-example/#programmatic-access","title":"Programmatic Access","text":"<p>You can also process data and launch the web interface programmatically:</p> <pre><code>from credtools.web.export import export_for_web\nfrom credtools.web.app import run_app\n\n# Process data for web visualization\nexport_for_web(\n    data_base_dir=\"results/\",\n    webdata_dir=\"webdata/\",\n    allmeta_loci_file=\"loci_files/allmeta_loci.txt\",\n    popumeta_loci_file=\"loci_files/popumeta_loci.txt\",\n    nometa_loci_file=\"loci_files/nometa_loci.txt\",\n    threads=10\n)\n\n# Launch web application\nrun_app(\n    webdata_dir=\"webdata/\",\n    port=8080,\n    debug=True\n)\n</code></pre>"},{"location":"tutorial/web-example/#batch-processing","title":"Batch Processing","text":"<p>For multiple datasets:</p> <pre><code>#!/bin/bash\n# Process multiple result directories\n\ndatasets=(\"dataset1\" \"dataset2\" \"dataset3\")\nport=8080\n\nfor dataset in \"${datasets[@]}\"; do\n    echo \"Processing $dataset...\"\n\n    # Run fine-mapping if needed\n    credtools pipeline input/${dataset}_loci.txt results/${dataset}/\n\n    # Launch web interface on different ports\n    credtools web results/${dataset}/ \\\n      --port $((port++)) \\\n      --webdata-dir webdata/${dataset} &amp;\ndone\n\necho \"All web interfaces launched. Check ports 8080-8082\"\n</code></pre>"},{"location":"tutorial/web-example/#custom-styling","title":"Custom Styling","text":"<p>The web interface uses Bootstrap themes. You can customize the appearance by modifying the Dash app configuration in your scripts.</p>"},{"location":"tutorial/web-example/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/web-example/#common-issues","title":"Common Issues","text":"<ol> <li>No data appears: Check that loci files have the correct format and paths</li> <li>Slow loading: Reduce the number of threads or use SSD storage</li> <li>Port conflicts: Use a different port with <code>--port</code> option</li> </ol>"},{"location":"tutorial/web-example/#getting-help","title":"Getting Help","text":"<pre><code># Get command help\ncredtools web --help\n\n# Check version\ncredtools --version\n\n# Enable debug mode\ncredtools web results/ --debug\n</code></pre>"},{"location":"tutorial/web-example/#next-steps","title":"Next Steps","text":"<ul> <li>Try filtering by different meta-analysis methods</li> <li>Explore individual loci in detail</li> <li>Export plots for presentations</li> <li>Integrate web visualization into your analysis pipeline</li> <li>Share results with collaborators via the web interface</li> </ul> <p>For more information: - Full Web Tutorial - Advanced Usage - API Documentation </p>"},{"location":"tutorial/web-visualization/","title":"Web Visualization","text":"<p>CREDTOOLS provides an interactive web interface for exploring fine-mapping results. This tutorial covers how to install, configure, and use the web visualization features.</p>"},{"location":"tutorial/web-visualization/#prerequisites","title":"Prerequisites","text":"<ol> <li>Completed fine-mapping analysis with CREDTOOLS</li> <li>Web dependencies installed</li> <li>Results directory with fine-mapping output</li> </ol>"},{"location":"tutorial/web-visualization/#installation","title":"Installation","text":"<p>The web visualization requires additional dependencies that are not included in the base CREDTOOLS installation:</p> <pre><code># Install CREDTOOLS with web dependencies\npip install credtools[web]\n</code></pre>"},{"location":"tutorial/web-visualization/#basic-usage","title":"Basic Usage","text":"<p>Get help on web visualization options:</p> <pre><code>credtools web --help\n</code></pre>"},{"location":"tutorial/web-visualization/#launch-from-results-directory","title":"Launch from Results Directory","text":"<pre><code># Navigate to your CREDTOOLS results directory\ncd /path/to/your/credtools/results\n\n# Start the web interface\ncredtools web\n</code></pre>"},{"location":"tutorial/web-visualization/#launch-with-custom-port","title":"Launch with Custom Port","text":"<pre><code>credtools web /path/to/credtools/results --port 8080\n</code></pre>"},{"location":"tutorial/web-visualization/#data-structure","title":"Data Structure","text":"<p>Your CREDTOOLS results should follow this structure:</p> <pre><code>credtools_results/\n\u251c\u2500\u2500 allmeta_loci.txt\n\u251c\u2500\u2500 popumeta_loci.txt\n\u251c\u2500\u2500 susie/\n\u2502   \u251c\u2500\u2500 susie_allmeta/\n\u2502   \u2514\u2500\u2500 susie_popumeta/\n\u2514\u2500\u2500 finemap/\n    \u251c\u2500\u2500 finemap_allmeta/\n    \u2514\u2500\u2500 finemap_popumeta/\n</code></pre>"},{"location":"tutorial/web-visualization/#advanced-usage","title":"Advanced Usage","text":""},{"location":"tutorial/web-visualization/#launch-from-specific-directory","title":"Launch from Specific Directory","text":"<pre><code>cd /path/to/credtools/results\ncredtools web\n</code></pre>"},{"location":"tutorial/web-visualization/#force-data-regeneration","title":"Force Data Regeneration","text":"<pre><code>credtools web /path/to/data \\\n    --allmeta-loci data/allmeta_loci.txt \\\n    --popumeta-loci data/popumeta_loci.txt \\\n    --force-regenerate\n</code></pre>"},{"location":"tutorial/web-visualization/#debug-mode","title":"Debug Mode","text":"<pre><code>credtools web /path/to/data --debug --port 8081\n</code></pre>"},{"location":"tutorial/web-visualization/#python-api","title":"Python API","text":"<p>When you run <code>credtools web</code>, it automatically:</p> <ol> <li>Processes your fine-mapping results</li> <li>Generates web visualization data</li> <li>Launches a web server</li> </ol> <p>You can also use the Python API directly:</p> <pre><code>from credtools.web.export import export_for_web\nfrom credtools.web.app import run_app\n\n# Process data for web visualization\nexport_for_web(\n    data_base_dir=\"/path/to/credtools/results\",\n    allmeta_loci=\"allmeta_loci.txt\",\n    popumeta_loci=\"popumeta_loci.txt\"\n)\n\n# Launch web server\nrun_app(port=8080)\n</code></pre>"},{"location":"tutorial/web-visualization/#customization","title":"Customization","text":""},{"location":"tutorial/web-visualization/#custom-web-data-directory","title":"Custom Web Data Directory","text":"<pre><code>credtools web /path/to/data --webdata-dir /custom/path/webdata\n</code></pre>"},{"location":"tutorial/web-visualization/#multiple-loci-files","title":"Multiple Loci Files","text":"<pre><code>credtools web /path/to/data \\\n    --allmeta-loci data/allmeta_loci.txt \\\n    --popumeta-loci data/popumeta_loci.txt \\\n    --susie-loci data/susie_loci.txt\n</code></pre>"},{"location":"tutorial/web-visualization/#performance-tuning","title":"Performance Tuning","text":"<pre><code># Use more threads for data processing\ncredtools web /path/to/data --threads 30\n\n# Use fast storage for web data\ncredtools web /path/to/data --webdata-dir /fast/ssd/webdata\n</code></pre>"},{"location":"tutorial/web-visualization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial/web-visualization/#missing-dependencies","title":"Missing Dependencies","text":"<p>If you see import errors:</p> <pre><code>pip install credtools[web]\n</code></pre>"},{"location":"tutorial/web-visualization/#custom-loci-files","title":"Custom Loci Files","text":"<p>If your loci files are in a different location:</p> <pre><code>credtools web /path/to/data --allmeta-loci /path/to/loci.txt\n</code></pre>"},{"location":"tutorial/web-visualization/#port-conflicts","title":"Port Conflicts","text":"<p>Use a different port:</p> <pre><code>credtools web /path/to/data --port 8081\n</code></pre>"},{"location":"tutorial/web-visualization/#slow-processing","title":"Slow Processing","text":"<p>Reduce thread count:</p> <pre><code>credtools web /path/to/data --threads 5\n</code></pre>"},{"location":"tutorial/web-visualization/#debug-mode_1","title":"Debug Mode","text":"<p>Enable debug logging:</p> <pre><code>credtools web /path/to/data --debug\n</code></pre>"},{"location":"tutorial/web-visualization/#logging","title":"Logging","text":"<p>Check CREDTOOLS logs for processing issues:</p> <pre><code>tail -f credtools.log\n</code></pre>"},{"location":"tutorial/web-visualization/#example-workflow","title":"Example Workflow","text":"<ol> <li>Run fine-mapping pipeline:</li> </ol> <pre><code>credtools pipeline input_loci.txt results/\n</code></pre> <ol> <li>Launch web interface:</li> </ol> <pre><code>credtools web results/ --port 8080\n</code></pre>"},{"location":"tutorial/web-visualization/#multiple-datasets","title":"Multiple Datasets","text":"<p>For multiple datasets:</p> <pre><code>for i in {1..3}; do\n    dataset=\"dataset${i}\"\n    credtools web results/${dataset} --webdata-dir webdata/${dataset} --port 808${i}\ndone\n</code></pre>"}]}